<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Install storage in your application cluster using Rook</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/07/13/install-storage-your-application-cluster-using-rook" /><author><name>Praveen Kumar</name></author><id>1438ea43-76b7-4a9b-9951-407c12bb61fc</id><updated>2022-07-13T07:00:00Z</updated><published>2022-07-13T07:00:00Z</published><summary type="html">&lt;p&gt;Developers running applications in the cloud have traditionally separated their applications from storage, but recent services such as the &lt;a href="https://github.com/rook/rook"&gt;Rook&lt;/a&gt; Operator for &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; make it easy to create storage in the cloud. This article shows how to use Rook for object storage. Our example runs a single-node &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; cluster on your laptop or desktop using &lt;a href="https://developers.redhat.com/products/openshift-local/overview"&gt;Red Hat OpenShift Local&lt;/a&gt; (previously known as CodeReady Containers). We'll install Rook, create object storage on OpenShift Local, and perform some &lt;a href="https://ceph.io/en/news/blog/2022/bucket-notifications-with-knative-and-rook-on-minikube2/#bucket-notifications"&gt;bucket notifications&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Rook is an open source offering from the &lt;a href="https://cncf.io"&gt;Cloud Native Computing Foundation&lt;/a&gt; (CNCF). This article creates storage using the &lt;a href="https://ceph.com/en/"&gt;Ceph&lt;/a&gt; object store.&lt;/p&gt; &lt;h2&gt;Install and run Red Hat OpenShift Local&lt;/h2&gt; &lt;p&gt;You can &lt;a href="https://developers.redhat.com/products/openshift-local/overview"&gt;download the latest release of OpenShift Local&lt;/a&gt; and follow the installation instructions to run it.&lt;/p&gt; &lt;h2&gt;Add disk space for Rook&lt;/h2&gt; &lt;p&gt;OpenShift Local doesn't provide additional disk space, so users need to create it manually and attach it to the instance. This process is specific to your operating system. The following steps work on GNU/&lt;a href="https://developers.redhat.com/topics/linux"&gt;Linux&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ qemu-img create -f raw /full/path/to/crc-extra-disk 30G $ sudo virsh attach-disk crc --source /full/path/to/crc-extra-disk --target vdb --cache none&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Log in to the OpenShift cluster&lt;/h2&gt; &lt;p&gt;Log in as the &lt;code&gt;kubeadmin&lt;/code&gt; administrative user:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ crc console --credentials $ oc login -u kubeadmin -p &lt;password&gt; https://api.crc.testing:6443 $ oc whoami kubeadmin&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Install Rook resources&lt;/h2&gt; &lt;p&gt;Rook requires custom resource definitions (CRDs), an &lt;a href="https://developers.redhat.com/topics/kubernetes/operators"&gt;Operator&lt;/a&gt;, and other resources that you can install through OpenShift's &lt;code&gt;oc&lt;/code&gt; administative commmand:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc apply -f https://raw.githubusercontent.com/rook/rook/master/deploy/examples/crds.yaml $ oc apply -f https://raw.githubusercontent.com/rook/rook/master/deploy/examples/common.yaml $ oc apply -f https://raw.githubusercontent.com/rook/rook/master/deploy/examples/operator-openshift.yaml $ oc apply -f https://raw.githubusercontent.com/rook/rook/master/deploy/examples/cluster-test.yaml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Use a secure port for the object store:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ cat &lt;&lt;EOF | oc apply -f - ################################################################################################################# # Create an object store with settings for a test environment. Only a single OSD is required in this example. # kubectl create -f object-test.yaml ################################################################################################################# apiVersion: ceph.rook.io/v1 kind: CephObjectStore metadata: name: my-store namespace: rook-ceph # namespace:cluster spec: metadataPool: replicated: size: 1 dataPool: replicated: size: 1 preservePoolsOnDelete: false gateway: service: annotations: service.beta.openshift.io/serving-cert-secret-name: my-store-tls securePort: 443 instances: 1 EOF&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Wait until all the pods in the &lt;code&gt;rook-ceph&lt;/code&gt; namespace are running. You can check that they are running through the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc get pods -n rook-ceph NAME READY STATUS RESTARTS AGE csi-cephfsplugin-provisioner-6f54f6c477-5sp9k 6/6 Running 0 29m csi-cephfsplugin-z96pz 3/3 Running 0 29m csi-rbdplugin-provisioner-6d765b47d5-pkc8j 6/6 Running 0 29m csi-rbdplugin-ssgc9 3/3 Running 0 29m rook-ceph-mgr-a-5b8f9998c6-vrglx 1/1 Running 0 27m rook-ceph-mon-a-7445f49f8-6tfjj 1/1 Running 0 27m rook-ceph-operator-5df4d596d5-sfrtw 1/1 Running 0 31m rook-ceph-osd-0-5f46f4cb58-498w6 1/1 Running 0 26m rook-ceph-osd-prepare-crc-8rwmc-master-0--1-zjcgc 0/1 Completed 0 26m rook-ceph-rgw-my-store-a-6847bcf96b-cwc9s 1/1 Running 0 17m&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Create an object bucket claim&lt;/h2&gt; &lt;p&gt;An object bucket claim (OBC) is a CRD that creates a storage class. The following commands create your OBC:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc apply -f https://raw.githubusercontent.com/rook/rook/master/deploy/examples/storageclass-bucket-delete.yaml $ oc apply -f https://raw.githubusercontent.com/rook/rook/master/deploy/examples/object-bucket-claim-delete.yaml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;See the &lt;a href="https://rook.io/docs/rook/v1.8/ceph-object-bucket-claim.html"&gt;OBC configuration documentation&lt;/a&gt; for more information.&lt;/p&gt; &lt;h2&gt;Allow external access&lt;/h2&gt; &lt;p&gt;OpenShift has a route resource to expose a service externally, which you can use for the &lt;code&gt;rook-ceph-rgw-my-store&lt;/code&gt; service:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc create route passthrough --service=rook-ceph-rgw-my-store -n rook-ceph $ oc get route -n rook-ceph NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD rook-ceph-rgw-my-store rook-ceph-rgw-my-store-rook-ceph.apps-crc.testing rook-ceph-rgw-my-store https passthrough None&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Upload a file to your object store&lt;/h2&gt; &lt;p&gt;Amazon Web Services (AWS) provides a command-line interface (CLI) to carry out just about anything you need to do on AWS. Download the CLI if you don't already have it; you can find the Linux version &lt;a href="https://docs.aws.amazon.com/cli/v1/userguide/install-linux.html"&gt;here&lt;/a&gt;. Install it as explained in the accompanying documentation.&lt;/p&gt; &lt;p&gt;To make sure you have the AWS CLI installed, enter:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ aws --version aws-cli/1.23.6 Python/3.6.8 Linux/4.18.0-348.el8.x86_64 botocore/1.25.6&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Export the required AWS variables from the cluster:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ export AWS_ACCESS_KEY_ID=$(kubectl -n default get secret ceph-notification-bucket -o jsonpath='{.data.AWS_ACCESS_KEY_ID}' | base64 --decode) $ export AWS_SECRET_ACCESS_KEY=$(kubectl -n default get secret ceph-notification-bucket -o jsonpath='{.data.AWS_SECRET_ACCESS_KEY}' | base64 --decode) $ export AWS_URL=https://rook-ceph-rgw-my-store-rook-ceph.apps-crc.testing&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Get the bucket from the OBC:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ export BUCKET_NAME=$(kubectl get objectbucketclaim ceph-notification-bucket -o jsonpath='{.spec.bucketName}')&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Upload the file and ensure that it's available:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ echo "hello world" &gt; hello.txt $ aws --no-verify-ssl --endpoint-url "$AWS_URL" s3 cp hello.txt s3://"$BUCKET_NAME" upload: ./hello.txt to s3://ceph-bkt-f51b12b7-1240-48a0-b6a5-fa0a15c18e94/hello.txt $ aws --no-verify-ssl --endpoint-url "$AWS_URL" s3 ls s3://"$BUCKET_NAME" 2022-05-04 04:49:57 12 hello.txt&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Object storage is easy to install in the cloud&lt;/h2&gt; &lt;p&gt;This article has shown how to use the Rook Operator to create storage in the same cluster where your application runs. The principles can be extended to many cloud environments and types of storage.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/07/13/install-storage-your-application-cluster-using-rook" title="Install storage in your application cluster using Rook"&gt;Install storage in your application cluster using Rook&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Praveen Kumar</dc:creator><dc:date>2022-07-13T07:00:00Z</dc:date></entry><entry><title type="html">10 IntelliJ Idea Tips to boost your productivity</title><link rel="alternate" href="http://www.mastertheboss.com/eclipse/intellij-idea/10-intellij-idea-tips-to-boost-your-productivity/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/eclipse/intellij-idea/10-intellij-idea-tips-to-boost-your-productivity/</id><updated>2022-07-12T17:52:51Z</updated><content type="html">This article contains a collection of trips and tricks that are available in Jet Brains‘ IntelliJ Idea to take your productivity at the next level. Run anything (Ctrl + Ctrl) Run anything is an universal action that you can use to perform certain tasks much faster. Invoking it is as easy as double-pressing Ctrl. By ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>How to run VS Code with OpenShift Dev Spaces</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/07/12/how-run-vs-code-openshift-dev-spaces" /><author><name>Mario Loriedo</name></author><id>c733c022-3094-4fe1-bba5-74d0bdffc378</id><updated>2022-07-12T07:00:00Z</updated><published>2022-07-12T07:00:00Z</published><summary type="html">&lt;p&gt;Red Hat OpenShift Dev Spaces is an &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;OpenShift&lt;/a&gt;-native developer environment server. Version 3.0 &lt;a href="https://developers.redhat.com/articles/2022/04/01/codeready-workspaces-scales-now-red-hat-openshift-dev-spaces"&gt;has just been released&lt;/a&gt;, and &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_openshift_dev_spaces/3.0/html/user_guide/selecting-an-ide#doc-wrapper"&gt;it allows you to choose the IDE that will be included in the development environment&lt;/a&gt;. Currently, the editors included with OpenShift Dev Spaces are &lt;a href="https://theia-ide.org/"&gt;Eclipse Theia&lt;/a&gt; and &lt;a href="https://www.jetbrains.com/idea/"&gt;JetBrains IntelliJ IDEA&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;What about Visual Studio Code? Although we plan to use it as the default IDE in future versions of OpenShift Dev Spaces, Visual Studio Code is not included in version 3.0. But &lt;a href="https://www.eclipse.org/che/"&gt;Eclipse Che&lt;/a&gt;, which is the upstream project for OpenShift Dev Spaces, already includes it, and we can easily use it in OpenShift Dev Spaces too. Read on to learn how this works.&lt;/p&gt; &lt;h2&gt;Select Visual Studio Code as the editor of a Dev Space environment&lt;/h2&gt; &lt;p&gt;Visual Studio Code is included in the Eclipse Che plugin registry with the identifier &lt;code&gt;che-incubator/che-code/insiders&lt;/code&gt;. OpenShift Dev Spaces has its own internal plugin registry, but it can use external registries too, including the &lt;a href="https://eclipse-che.github.io/che-plugin-registry/main/v3/plugins/"&gt;online Eclipse Che plugin registry&lt;/a&gt;. We can reference the online Che registry using a file in the Git repository or through a URL parameter. We'll take a quick look at each approach.&lt;/p&gt; &lt;h3&gt;Option 1: Adding che-editor.yaml in the Git repository&lt;/h3&gt; &lt;p&gt;The easiest way to use Visual Studio Code for a given project is to reference it from a &lt;code&gt;che-editor.yaml&lt;/code&gt; file in a Git repository.&lt;/p&gt; &lt;p&gt;Create a &lt;code&gt;.che&lt;/code&gt; folder and add a &lt;code&gt;che-editor.yaml&lt;/code&gt; file with the following content in it:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-json"&gt;id: che-incubator/che-code/insiders registryUrl: https://eclipse-che.github.io/che-plugin-registry/main/v3&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The first line specifies the identifier of the editor. The second line contains the URL of the plugin registry where the editor is published. We are using the online Eclipse Che plugin registry in this example.&lt;/p&gt; &lt;p&gt;Just commit and push the change to the project Git repository (Figure 1). Once the file is pushed, any OpenShift Dev Spaces remote development environment started using the Git repository URL (GitHub, GitLab and Bitbucket are supported) will use Visual Studio Code as the editor.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig1_adding-che-editor-in-git-repo.gif"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/fig1_adding-che-editor-in-git-repo.gif" width="960" height="586" alt="Adding the file .che/che-editor.yaml in a git repository" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: Adding file .che/che-editor.yaml in a git repository &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: Adding the file .che/che-editor.yaml in a Git repository&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Visual Studio Code is not the only editor that can be used from the online Che plugin registry. Here is a list of the editor IDs that can be referenced in from &lt;code&gt;.che/che-editor.yaml&lt;/code&gt;:&lt;/p&gt; &lt;div&gt; &lt;table cellspacing="0" width="686"&gt; &lt;thead&gt; &lt;tr&gt; &lt;th scope="col"&gt;Editor&lt;/th&gt; &lt;th scope="col"&gt;Editor ID in the Che plugin registry&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt; &lt;p&gt;Visual Studio Code&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;&lt;code&gt;che-incubator/che-code/insiders&lt;/code&gt;&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;p&gt;JetBrains IntelliJ IDEA&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;&lt;code&gt;che-incubator/che-idea/next&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;che-incubator/che-idea/latest&lt;/code&gt;&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;p&gt;JetBrains PyCharm&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;&lt;code&gt;che-incubator/che-pycharm/next&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;che-incubator/che-pycharm/latest&lt;/code&gt;&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;/div&gt; &lt;h3&gt;Option 2: Using the URL parameter che-editor&lt;/h3&gt; &lt;p&gt;It's not always possible to add the file &lt;code&gt;.che/che-editor.yaml&lt;/code&gt; into a Git repository. In these cases, it's still possible to specify the editor using the &lt;code&gt;che-editor&lt;/code&gt; URL parameter:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-http"&gt;https://&lt;devspaces-hostname&gt;#&lt;git-repository-url&gt;?che-editor=&lt;editor-definition-url&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here's an example of such a URL:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-http"&gt;https://devspaces.apps.mloriedo-devworkspaces.devcluster.openshift.com/#https://github.com/devfile/api?che-editor=https://eclipse-che.github.io/che-plugin-registry/main/v3/plugins/che-incubator/che-code/insiders/devfile.yaml&lt;/code&gt;&lt;/pre&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig2_using-che-editor-url-param.gif"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/fig2_using-che-editor-url-param.gif" width="960" height="586" alt="Using the che-editor URL parameter" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: Using the che-editor URL parameter &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;p&gt;Here is a list of available editors definitions in the Che plugin registry that can be used in the &lt;code&gt;che-editor&lt;/code&gt; URL parameter:&lt;/p&gt; &lt;table border="1" cellpadding="1" cellspacing="1" width="623"&gt; &lt;thead&gt; &lt;tr&gt; &lt;th scope="col"&gt;Editor&lt;/th&gt; &lt;th scope="col"&gt;Link to the Che Plugin Registry definition&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Visual Studio Code&lt;/td&gt; &lt;td&gt;&lt;a href="https://eclipse-che.github.io/che-plugin-registry/main/v3/plugins/che-incubator/che-code/insiders/devfile.yaml" target="_blank"&gt;devfile.yaml&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;JetBrains IntelliJ IDEA&lt;/td&gt; &lt;td&gt; &lt;p&gt;&lt;a href="https://eclipse-che.github.io/che-plugin-registry/main/v3/plugins/che-incubator/che-idea/latest/devfile.yaml"&gt;devfile.yaml (latest version)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://eclipse-che.github.io/che-plugin-registry/main/v3/plugins/che-incubator/che-idea/next/devfile.yaml"&gt;devfile.yaml (next most recent version)&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;JetBrains PyCharm&lt;/td&gt; &lt;td&gt; &lt;p&gt;&lt;a href="https://eclipse-che.github.io/che-plugin-registry/main/v3/plugins/che-incubator/che-pycharm/latest/devfile.yaml"&gt;devfile.yaml (latest version)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://eclipse-che.github.io/che-plugin-registry/main/v3/plugins/che-incubator/che-pycharm/next/devfile.yaml"&gt;devfile.yaml (next most recent version)&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;What Visual Studio Code extensions are available?&lt;/h2&gt; &lt;p&gt;The Visual Studio Code binary that is included in the Che plugin registry is pre-configured to use the online &lt;a href="https://openvsx.org"&gt;Open VSX Registry&lt;/a&gt;, which contains thousands of extensions and is owned by the Eclipse Foundation. Those extensions can be installed from Visual Studio Code itself as usual (Figure 3).&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig3_openvsx-vs-code-extensions.gif"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/fig3_openvsx-vs-code-extensions.gif" width="960" height="586" alt="Browsing and installing Visual Studio Code extensions from Open VSX" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: Browsing and installing Visual Studio Code extensions from Open VSX &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;p&gt;Using Open VSX extensions in air gapped scenarios is currently not supported. We are working on an offline version of Open VSX Registry that can be used with OpenShift Dev Spaces to support such environments, but that won't be available before OpenShift Dev Spaces 3.3.&lt;/p&gt; &lt;h2&gt;Is Visual Studio Code supported for OpenShift Dev Spaces?&lt;/h2&gt; &lt;p&gt;Visual Studio Code is currently only a community project and, as such, it is not included in OpenShift Dev Spaces customer support. (The rest of your OpenShift Dev Spaces environment is fully supported; only the changed editor would not be fully supported if you installed it.) For this change, Red Hat will offer "commercially reasonable" support, which means we will try to help you but cannot make the same guarantees (with SLAs) that we make for the rest of OpenShift Dev Spaces.&lt;/p&gt; &lt;h2&gt;What's the difference between this editor and Microsoft Visual Studio Code?&lt;/h2&gt; &lt;p&gt;The Visual Studio Code-based editor used by Eclipse Che, called &lt;a href="https://github.com/che-incubator/che-code"&gt;che-code&lt;/a&gt;, is a customization of &lt;a href="https://github.com/Microsoft/vscode"&gt;Visual Studio Code Open Source&lt;/a&gt; (Code OSS). It's not a fork of Code OSS (the source code is included as a &lt;a href="https://git-scm.com/book/en/v2/Git-Tools-Advanced-Merging#_subtree_merge"&gt;Git subtree&lt;/a&gt;) and that makes it easy to automatically fetch the latest changes from Code OSS multiple times a day (currently every four hours).&lt;/p&gt; &lt;p&gt;The &lt;code&gt;che-code&lt;/code&gt; customizations include:&lt;/p&gt; &lt;ul&gt; &lt;li aria-level="1"&gt;Eclipse Che icons and labels (branding)&lt;/li&gt; &lt;li aria-level="1"&gt;A few extra built-in extensions (resource monitoring, port plugin, devfile commands, projects, and remote terminal support)&lt;/li&gt; &lt;li aria-level="1"&gt;The configuration to use the online Open VSX Registry&lt;/li&gt; &lt;li aria-level="1"&gt;An opt out from Microsoft telemetry&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Microsoft Visual Studio Code is built on top of Code OSS as well, but includes some closed source built-in extensions, uses the Microsoft Visual Studio Code extensions marketplace, and has telemetry turned on.&lt;/p&gt; &lt;p&gt;We considered using an existing project like &lt;a href="https://github.com/coder/code-server"&gt;coder&lt;/a&gt; or &lt;a href="https://github.com/gitpod-io/openvscode-server"&gt;openvscode-server&lt;/a&gt; to run Visual Studio Code in the browser. But those are forks (making them harder to rebase), and the code to run Visual Studio Code in the browser is, as of now, open source and available in Code OSS, so we don't think there is currently any value to using an intermediate project.&lt;/p&gt; &lt;h2&gt;What's the roadmap for including Visual Studio Code in OpenShift Dev Spaces?&lt;/h2&gt; &lt;p&gt;The default OpenShift Dev Spaces editor is still Eclipse Theia, but the number of Visual Studio Code extensions that can be run properly by Theia is limited. That has been our main issue for the last three years. Developers don't care much about the question of Visual Studio Code vs. Eclipse Theia, but they want to run the myriad of extensions that are available out there.&lt;/p&gt; &lt;p&gt;Our main goal for 2022 Q3 is to include a &lt;a href="https://github.com/Microsoft/vscode"&gt;Code OSS&lt;/a&gt;-based editor using Open VSX Registry in OpenShift Dev Spaces and make it the default editor. To do that, we are currently closing the feature gap with Theia. In particular, we are working on an idling mechanism to stop environments that have been inactive for a given amount of time, as well as working on support for air gapped environments.&lt;/p&gt; &lt;h2&gt;Is it possible to use Visual Studio Code on the Developer Sandbox?&lt;/h2&gt; &lt;p&gt;The &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Red Hat Developer Sandbox&lt;/a&gt; hasn't been updated to OpenShift Dev Spaces yet, and it's not possible to use CodeReady Workspaces 2.15 to provision Visual Studio Code-based development environments. But the DevWorkspace Operator, the new dev environments engine used by OpenShift Dev Spaces, is installed on the Developer Sandbox (because the Web Terminal depends on it), and it supports running a Visual Studio Code-based development environment.&lt;/p&gt; &lt;p&gt;Here are some command line instructions to do that from the OpenShift Web Terminal. You can see the results in Figure 4.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;VSCODE_SAMPLE_DW="https://eclipse-che.github.io/che-devfile-registry/main/devfiles/nodejs/devworkspace-che-code-insiders.yaml" curl -sSL "${VSCODE_SAMPLE_DW}" | sed 's/^[[:space:]]\{8,\}container:/ container:^ env: [{name: CODE_HOST, value: "0.0.0.0"}]/g' | # &lt;-- CHE IS NOT THERE sed 's/template:/template:^ attributes:^ controller.devfile.io\/storage-type: ephemeral/g' | # &lt;-- EPHEMERAL IS FASTER sed 's/volume: {}/volume: {ephemeral: true}/g' | sed '/tkn=eclipse-che/d' | tr '^' '\n' | \ oc apply -f - oc get dw nodejs-web-app --watch &lt;/code&gt;&lt;/pre&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig4_running-vscode-in-developer-sandbox.gif"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/fig4_running-vscode-in-developer-sandbox.gif" width="960" height="586" alt="Running Visual Studio Code in the Red Hat Developer Sandbox using the DevWorkspace Operator" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: Running Visual Studio Code in the Red Hat Developer Sandbox using the DevWorkspace Operator &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;p&gt;We're still working on expanding editor support in OpenShift Dev Spaces. But by following the steps outlined here, users can take advantage of their familiarity with Visual Studio Code and its ecosystem of extensions today.&lt;/p&gt; &lt;p&gt;A special thank you to Rick Wagner and Florent Benoit for their review and suggestions for this article.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/07/12/how-run-vs-code-openshift-dev-spaces" title="How to run VS Code with OpenShift Dev Spaces"&gt;How to run VS Code with OpenShift Dev Spaces&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Mario Loriedo</dc:creator><dc:date>2022-07-12T07:00:00Z</dc:date></entry><entry><title>Deploy an Operator via GitOps using Advanced Cluster Management</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/07/11/deploy-operator-gitops-using-advanced-cluster-management" /><author><name>Sahil Sethi</name></author><id>49f4b632-e0ec-4b59-a317-d7d4093edab3</id><updated>2022-07-11T07:00:00Z</updated><published>2022-07-11T07:00:00Z</published><summary type="html">&lt;p&gt;GitOps is a strict discipline: Everything you code or manage should be specified through configuration files in your Git repositories, and applied automatically through &lt;a href="https://developers.redhat.com/topics/ci-cd"&gt;CI/CD pipelines&lt;/a&gt;. This article shows you how to integrate &lt;em&gt;security policies&lt;/em&gt; into GitOps so that they are applied consistently throughout your clusters. Security policies are part of &lt;a href="https://access.redhat.com/products/red-hat-advanced-cluster-management-for-kubernetes"&gt;Red Hat Advanced Cluster Management for Kubernetes&lt;/a&gt;, a platform that helps developers configure and deploy applications along with other useful services such as metrics. This article also uses &lt;a href="https://www.redhat.com/en/resources/advanced-cluster-security-for-kubernetes-datasheet"&gt;Red Hat Advanced Cluster Security for Kubernetes&lt;/a&gt;. For background on Red Hat Advanced Cluster Management, read &lt;a href="https://cloud.redhat.com/blog/understanding-gitops-with-red-hat-advanced-cluster-management"&gt;Understanding GitOps with Red Hat Advanced Cluster Management&lt;/a&gt; on the Red Hat Hybrid Cloud blog.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;Before beginning the exercise in this article, you'll need to install the following technologies:&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.9/html/installing/index"&gt;Red Hat OpenShift Container Platform 4.9&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.4/html/install/index"&gt;Red Hat Advanced Cluster Management for Kubernetes&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;A &lt;a href="https://github.com/sahilsethi12/acspolicy-gitops-acm"&gt;policies repository&lt;/a&gt; to deploy with Red Hat Advanced Cluster Management and Red Hat Advanced Cluster Security&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Custom labels can be used to select policies in Red Hat Advanced Cluster Security Management. To take advantage of this feature, create two clusters on the Openshift Container Platform. Assign a label with the name &lt;code&gt;env&lt;/code&gt; and the value &lt;code&gt;dev&lt;/code&gt; to one cluster, and a label with the name &lt;code&gt;env&lt;/code&gt; and the value &lt;code&gt;test&lt;/code&gt; to the other.&lt;/p&gt; &lt;h2&gt;Deploy the Subscription-Admin policy&lt;/h2&gt; &lt;p&gt;The policies repository listed in the prerequisites contains a Subscription-Admin policy. To activate it, in the Red Hat Advanced Cluster Management console, navigate to &lt;strong&gt;Governance→Create Policy&lt;/strong&gt;. Copy and paste the &lt;code&gt;policy-configure-subscription-admin-hub.yaml&lt;/code&gt; file from the policies repository into the YAML view. Change the namespace to match the namespace of your cluster, and change the user name (which the file defines as &lt;code&gt;kube:admin&lt;/code&gt;) to the username you use to log into Red Hat Advanced Cluster Management. Once you have created the policy, it will be shown in the &lt;strong&gt;Governance&lt;/strong&gt; page in the console (Figure 1).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/subscription-admin_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/subscription-admin_0.png?itok=utZRE1OM" width="1440" height="900" alt="Screenshow of the Governance page showing a policy after you successfully create it" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: The Governance screen shows a policy after you successfully create it. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: The Governance page shows a policy after you successfully create it.&lt;/figcaption&gt; &lt;/figure&gt; &lt;h2&gt;Install the central policy&lt;/h2&gt; &lt;p&gt;Deploy Red Hat Advanced Cluster Security for Kubernetes by navigating to &lt;strong&gt;Applications→Create Application→Subscription&lt;/strong&gt; and entering the information shown in Figure 2:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The name of your application&lt;/li&gt; &lt;li&gt;The namespace where you want to install the application&lt;/li&gt; &lt;li&gt;The URL of its repository&lt;/li&gt; &lt;li&gt;Your username&lt;/li&gt; &lt;li&gt;Your access token&lt;/li&gt; &lt;/ul&gt; &lt;figure role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/application_1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/application_1.png?itok=BaAnwSss" width="1440" height="900" alt="This picture shows the application details" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: The "Create an application" screen asks for basic information. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Also on the &lt;strong&gt;Create an application&lt;/strong&gt; page, choose &lt;strong&gt;Deploy on local cluster&lt;/strong&gt; (Figure 3).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/application_2.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/application_2.png?itok=SOZXMP2G" width="1440" height="900" alt="This picture shows the application details" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: The "Create an application" screen lets you deploy the application on a local cluster. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3: The 'Create an application' page lets you choose where to deploy the application.&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;The application needs a central policy, which must be located in only one of its environments. The policies repository linked to above defines a central policy with the name &lt;code&gt;policy-advanced-cluster-security-central&lt;/code&gt; and places it in the &lt;code&gt;test&lt;/code&gt; environment using a &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.4/html-single/applications/index#placement-rules"&gt;placement rule&lt;/a&gt; in &lt;a href="https://github.com/sahilsethi12/acspolicy-gitops-acm/blob/main/centraldeploypolicy/policy-acs-operator-central.yaml"&gt;one of the YAML configuration files&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Click &lt;strong&gt;Save Application&lt;/strong&gt;. A successful creation takes you to the &lt;em&gt;resource topology,&lt;/em&gt; a visual representation of the resources in your deployed application, including the Subscription-Application (Figure 4).&lt;/p&gt; &lt;figure role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Application.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/Application.png?itok=P0f31eSW" width="1440" height="900" alt="Shows the Application and all Manifests deployed using the application" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: The application's topology shows the application's resources. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;From the menu on the left, you can choose the &lt;strong&gt;Governance&lt;/strong&gt; tab and see the new Subscription-Admin policy in effect (Figure 5).&lt;/p&gt; &lt;figure role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Governancetab.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/Governancetab.png?itok=gvhNIgei" width="1440" height="900" alt="Shows the policies deployed via Manual as well as via Git " loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5: The Governance screen now shows the Subscription-Admin policy. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;h2&gt;Generate the init bundle for the cluster and deploy it via the application&lt;/h2&gt; &lt;p&gt;From the &lt;code&gt;test&lt;/code&gt; cluster, get the URL of the central endpoint. Enter this URL into your browser and log into the &lt;code&gt;stackrox&lt;/code&gt; namespace. (The policies repository assigned the name &lt;code&gt;stackrox&lt;/code&gt; because &lt;a href="https://www.stackrox.io"&gt;StackRox&lt;/a&gt; is the upstream project from which Red Hat Advanced Cluster Security for Kubernetes evolved, but you can use any name of your choice.) Your password will be picked from the secret named &lt;code&gt;central-htpasswd&lt;/code&gt; in the namespace.&lt;/p&gt; &lt;p&gt;Navigate to &lt;strong&gt;Platform Configuration→Integrations→Cluster Init Bundle→&lt;your_cluster_name&gt;&lt;/strong&gt;. Click &lt;strong&gt;generate→Download kubernetes Secret file&lt;/strong&gt;. Replace the automatically generated file with the &lt;a href="https://github.com/sahilsethi12/acspolicy-gitops-acm/blob/main/centralSecrets/testcluster-cluster-init-secrets-3.yaml"&gt;contents of this file&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Now you have to create an application to deploy the generated secret file in both the &lt;code&gt;dev&lt;/code&gt; and &lt;code&gt;test&lt;/code&gt; clusters. Follow these steps for each cluster:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Navigate to &lt;strong&gt;Applications→Create Application→Subscription&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Enter the name of the application and the namespace where you want to install it. Choose &lt;code&gt;centralSecrets&lt;/code&gt; as the Git path.&lt;/li&gt; &lt;li&gt;Instead of choosing a local cluster, choose the labels for which the cluster needs to be deployed. As shown in Figure 6, one label has the name &lt;code&gt;env&lt;/code&gt; with the value &lt;code&gt;test&lt;/code&gt;, and the other has the name &lt;code&gt;env&lt;/code&gt; with the value &lt;code&gt;dev&lt;/code&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;figure role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/InitBundle_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/InitBundle_0.png?itok=Zn1DTKnl" width="1440" height="900" alt="Shows the Application Creation Fields for Secret Manifest " loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 6: The "Create an application" screen shows the test and dev labels. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;h2&gt;Installing the secured cluster&lt;/h2&gt; &lt;p&gt;Each cluster runs an application. For simplicity, in this section you'll create two identical applications, one for the &lt;code&gt;dev&lt;/code&gt; cluster and one for the &lt;code&gt;test&lt;/code&gt; cluster. (Another approach would be to create one application and use templates to get the value for the cluster from the secrets, instead of hardcoding the value.)&lt;/p&gt; &lt;p&gt;For the simple approach we'll use here, go through these steps on each cluster:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Navigate to &lt;strong&gt;Applications→Create Application→Subscription&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Enter the name of the application and the namespace where you want to install it. For the &lt;code&gt;dev&lt;/code&gt; cluster, choose &lt;a href="https://github.com/sahilsethi12/acspolicy-gitops-acm/tree/main/secureclusterdeploypolicydevcluster"&gt;secureclusterdeploypolicy_devcluster&lt;/a&gt; as the Git path, and for the &lt;code&gt;test&lt;/code&gt; cluster, choose &lt;a href="https://github.com/sahilsethi12/acspolicy-gitops-acm/tree/main/secureclusterdeploypolicytestcluster"&gt;secureclusterdeploypolicy_testcluster&lt;/a&gt; as the Git path.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Figure 7 shows the values to enter.&lt;/p&gt; &lt;figure role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/securedcluster.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/securedcluster.png?itok=vM7xKE9p" width="1440" height="900" alt="Shows the Application Creation Fields for Secured Cluster Installation" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 7: The "Create an application" screen shows options for the dev cluster. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Once everything is deployed, you can see the status of your clusters, along with some metrics, in the central server (Figure 8).&lt;/p&gt; &lt;figure role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Central.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/Central.png?itok=T1E_yCKc" width="1440" height="900" alt="Shows the RHACS Central UI" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 8: The metrics dashboard shows the status and activites on your clusters. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article has shown how configuration files and CI/CD pipelines can be used to manage security policies. The general principles can apply to other processes that you need to automate with GitOps.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/07/11/deploy-operator-gitops-using-advanced-cluster-management" title="Deploy an Operator via GitOps using Advanced Cluster Management"&gt;Deploy an Operator via GitOps using Advanced Cluster Management&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Sahil Sethi</dc:creator><dc:date>2022-07-11T07:00:00Z</dc:date></entry><entry><title type="html">How to export and import Realms in Keycloak</title><link rel="alternate" href="http://www.mastertheboss.com/keycloak/how-to-export-and-import-realms-in-keycloak/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/keycloak/how-to-export-and-import-realms-in-keycloak/</id><updated>2022-07-10T09:30:00Z</updated><content type="html">This article discusses about Importing and Exporting Keycloak Realms using the latest product distribution which runs on a Quarkus runtime. Realm Set up Firstly, if you are new to Keycloak and Quarkus, we recommend checking this article which covers the basics: Getting started with Keycloak powered by Quarkus A Keycloak Realm is a space where ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">Explaining Drools with TrustyAI</title><link rel="alternate" href="https://blog.kie.org/2022/07/explaining-drools-trustyai.html" /><author><name>Rob Geada</name></author><id>https://blog.kie.org/2022/07/explaining-drools-trustyai.html</id><updated>2022-07-08T13:55:30Z</updated><content type="html">INTRODUCTION TO TRUSTYAI AND DROOLS Explainability is a crucial aspect in modern AI and decision services work; recent laws entitle any person subject to automated decisions to explanations of the intuition behind said decisions. Moreover, people are more likely to trust the decisions of explained models compared to unexplained models (Kim et al., 2022). Furthermore, explainability is very useful in introspecting models during the engineering process, to validate that the model is working according to design specifications and making ethical, legal, and fair decisions. However, providing intuitive explanations of a model’s workings can be difficult for large, complex models, especially so for "blackbox" models like deep neural networks or random forests. To address this issue, TrustyAI provides a suite of explainability algorithms such as , , and to explain any blackbox model. Meanwhile, business rules engines like Drools provide a powerful toolset to define rules, individual pieces of decision logic that outline some larger process. Drools in this regard has immense power and flexibility to produce complex and nuanced decision processes. While each individual rule should be fairly interpretable, in that the majority tend to follow if-then-else style logic, the composition of many rules en masse can make the entire ruleset quite hard to parse, especially for those that didn’t have a role in building the ruleset. In these circumstances, providing an explanation of a ruleset’s behavior is an intensive task, involving reading all the individual rules and attempting to manually understand how they interlink. Rulesets can be dynamic which makes this even harder, in that the particular rationale for one decision may involve ruleflows that are completely irrelevant or entirely unused for other decisions. A natural conclusion therefore is to try and explain Drools with TrustyAI, since TrustyAI’s algorithms can explain complex blackbox models and should therefore be well suited to explaining complex rulesets. MAPPING A RULESET INTO A MODEL The model paradigm; given some fixed input, produce some fixed output The term "model" in an explainability algorithm context refers to some function f that, when given some input x, produces some output y, i.e, something in the form y = f(x). What f, x, and y actually consist of is dependent on the particular usecase. For example, in a credit card application process, the input x could be a potential applicant’s credit details, the model f be some mechanism that evaluates the applicant, and the output y a binary accept/reject. An explainability algorithm would try to identify and explain how the specific facets or "features" within the input x affected the output y. However, the concepts of input, model, and output can be poorly defined within a rules engine like Drools. This is because Drools rulesets in implementation are simply a collection of Java classes, where the rules define how these classes interact and evolve throughout the execution of a ruleset. Rulesets can therefore take the form of basically any arbitrary Java program, in which case, how should input and output be defined in such a way that is most applicable to the majority of existing rulesets? Furthermore, can these defined inputs and outputs be automatically identified and extracted from existing rulesets without requiring any redesign of the ruleset, such that rulesets can be explained "out-of-the-box"? To satisfy these criteria, I’ve created the following definitions of input and output within Drools: INPUTS To initialize a rule evaluation, a series of objects are inserted into Drools. These inserted objects are logical choice for the inputs, specifically, any settable attribute within these objects or their recursive nested objects is defined as the model input. These attributes need to be settable due to the way that most explainability algorithms operate in a modify-and-observe fashion; they modify characteristics of the input and observe their effects on the output to understand the model’s internal workings. Therefore, the explanability algorithms need to be able to modify the inputs, meaning anything used as an input needs to be settable. OUTPUTS While the object insertion allows for a fairly simple definition of input, outputs are more amorphous as Drools does not specifically return anything from a ruleset evaluation. Instead, the evaluation of a ruleset modifies or deletes the inserted objects, or even creates new ones. These are the consequences of a ruleset evaluation, and are the most plausible candidates as the outputs of the system. Specifically, any of the following are considered a possible output: * Any gettable attribute of any object or recursive nested object that was created, modified, or deleted during ruleset evaluation * Any object that was created or deleted during ruleset evaluation The motivation to restrict attributes to be gettable is fairly obvious; in order to actually extract a created, modified, or deleted attribute it has to be retrievable in the first place. IMPLEMENTING THESE DEFINITIONS INPUT TRACKER Automatically identifying potential inputs is fairly straightforward; the user needs to pass a Supplier of all the objects that need to be inserted into Drools before ruleset evaluation. This initial input marks the initial values of all extracted features, and it is these values that will be explained by the TrustyAI algorithms. After the Object Supplier is created, all attributes of these objects are parsed recursively (such as to identify the settable attributes of all nested objects) for settable attributes. All found settable attributes are identified as candidate input features. This list can be narrowed down by a set of user-configurable filters that include or exclude specific rules, field names, or objects from consideration. Then a mapping between a TrustyAI PredictionInput and the attribute setters is generated, such that the set of objects-to-be-inserted can be generated, the relevant attributes set to desired values as per the PredictionInput (i.e., the particular values desired by the explainability algorithm as it tweaks the original input), and then passed into Drools. OUTPUT TRACKER Again, identifying potential outputs proves slightly harder. Since the goal is to track the creation, modification, or deletion of objects or gettable attributes during ruleset evaluation, a RuleFireListener is placed into the Drools engine. This tracks whenever any rule activates within the ruleset evaluation, and allows for the insertion of hooks before and after the activation. This functionality is exploited to monitor for potential outputs: before a rule fires, all objects and attribute values in the Drools engine are tracked. If any of these items have not been previously seen during evaluation, they are marked as novel items and thus potential outputs. After the rule fires, all objects and attribute values within the engine are again recorded. Any differences (in either item presence/absence or attribute value) between the before and after sets are marked as further potential outputs. After the full ruleset evaluation is complete, this process will have created a set of all objects and attributes that meet our output criteria. Again, this set can be narrowed down by a set of user-configurable filters that include or exclude specific rules, field names, or objects from consideration. Practically, this requires a single evaluation of the ruleset ahead of the actual explanation work, to track the various consequences of the specific initial input passed into Drools. This does limit the available output candidates to just those that were identified during this initial input. Novel outputs (i.e., consequences unseen during the evaluation of the initial input) are not valid output candidates. However, the system is robust to the absence of desired outputs, (that is, an output that was recorded during the evaluation of the initial input, but does not necessarily appear for other inputs) and as such a workaround to the novel output issue is to find an input that produces the desired output, and use its absence as the tracked output signal. USAGE With the input trackers and output trackers, we now have a schema by which to automatically input novel feature values into the rule engine and then extract our desired outputs. This lets a Drools ruleset evaluation be viewed as our model, which in turn lets Drools be explained via the TrustyAI explainers. In general, the workflow involved in producing the explanations looks as follows: 1. Define a Supplier&lt;List&lt;Object&gt;&gt;, a function that produces an initial set of objects to be inserted into Drools, thus defining the initial feature values. 2. Define a DroolsWrapper by specifying a Drools rule set and the Object Supplier. 3. Identify the available features within the supplied objects. 1. Narrow these features down by specifying filters, if desired. 2. If a counterfactual explanation is desired: * Specify feature boundaries to constrain the possible values of these features. 3. If a SHAP explanation is desired: * Specify background feature values. 4. Identify the available outputs. 1. Narrow these outputs down by specifying filters, if desired. 2. Choose a set of specific outputs to be marked as model outputs during explanation. 5. Wrap the DroolsWrapper into a TrustyAI PredictionProvider. 6. Use the PredictionProvider within any TrustyAI explanation algorithm, just like any other PredictionProvider model. EXPLAINING DROOLS WITH TRUSTYAI: EXAMPLES The Shipping Cost Calculation Ruleset We’ll use the example from Nicolas Héron’s gitbook, . In this ruleset, an Order is created consisting of a variety of products as well as a Trip which details the shipping route and modalities that the order must undergo. The evaluation of the ruleset then computes the various associated costs with the order shipment, like the tax, handling, and transportation costs. SETUP First, let’s define the Object Supplier: Supplier&lt;List&lt;Object&gt;&gt; objectSupplier = () -&gt; { // define Trip City cityOfShangai = new City(City.ShangaiCityName); City cityOfRotterdam = new City(City.RotterdamCityName); City cityOfTournai = new City(City.TournaiCityName); City cityOfLille = new City(City.LilleCityName); Step step1 = new Step(cityOfShangai, cityOfRotterdam, 22000, Step.Ship_TransportType); Step step2 = new Step(cityOfRotterdam, cityOfTournai, 300, Step.train_TransportType); Step step3 = new Step(cityOfTournai, cityOfLille, 20, Step.truck_TransportType); Trip trip = new Trip("trip1"); trip.getSteps().add(step1); trip.getSteps().add(step2); trip.getSteps().add(step3); // define Order Order order = new Order("toExplain"); Product drillProduct = new Product("Drill", 0.2, 0.4, 0.3, 2, Product.transportType_pallet); Product screwDriverProduct = new Product("Screwdriver", 0.03, 0.02, 0.2, 0.2, Product.transportType_pallet); Product sandProduct = new Product("Sand", 0.0, 0.0, 0.0, 0.0, Product.transportType_bulkt); Product gravelProduct = new Product("Gravel", 0.0, 0.0, 0.0, 0.0, Product.transportType_bulkt); Product furnitureProduct = new Product("Furniture", 0.0, 0.0, 0.0, 0.0, Product.transportType_individual); order.getOrderLines().add(new OrderLine(1000, drillProduct)); order.getOrderLines().add(new OrderLine(35000.0, sandProduct)); order.getOrderLines().add(new OrderLine(14000.0, gravelProduct)); order.getOrderLines().add(new OrderLine(500, furnitureProduct)); // combine Trip and Order into CostCalculationRequest CostCalculationRequest request = new CostCalculationRequest(); request.setTrip(trip); request.setOrder(order); return List.of(request) } While that was a little clunky, it is an implicit necessity of this specific ruleset; the only difference required by the TrustyAI-Drools integration is popping all of that inside the Supplier () -&gt; {etc} lambda. Next, we can initialize the DroolsWrapper and investigate possible features. // initialize the wrapper DroolsWrapper droolsWrapper = new DroolsWrapper(kieContainer,"CostRulesKS", objectSupplier, "P1"); FEATURE SELECTION With a DroolsWrapper created, we can investigate possible features: // setup Feature extraction droolsWrapper.displayFeatureCandidates(); This produces a massive list of possible features, a sample of which are shown below: === FEATURE CANDIDATES ====================================================== Feature | Value | Type | Domain ----------------------------------------------------------------------------- trip.steps[0].transportType | 1 | number | Empty trip.steps[2].stepStart.name | Tournai | categorical | Empty order.orderLines[0].product.height | 0.2 | number | Empty order.orderLines[1].product.depth | 0.0 | number | Empty order.orderLines[2].product.transportType | 3 | number | Empty order.orderLines[3].numberItems | 500 | number | Empty order.orderLines[1].product.width | 0.0 | number | Empty order.orderLines[1].product.name | Sand | categorical | Empty totalCost | 0.0 | number | Empty order.orderLines[3].product.transportType | 2 | number | Empty totalHandlingCost | 0.0 | number | Empty ... ... ... ============================================================================= The interesting choices among these for possible features are the variables, things that we would have the power to change. In the case of our shipment, it’s the shipping modality and product quantities, which we can isolate by setting up the following feature filters: // add filters via regex droolsWrapper.setFeatureExtractorFilters( List.of( "(orderLines\\[\\d+\\].weight)", "(orderLines\\[\\d+\\].numberItems)", "(trip.steps\\[\\d+\\].transportType)" ) ); // display candidates droolsWrapper.displayFeatureCandidates(); === FEATURE CANDIDATES ===================================== Feature | Value | Type | Domain ------------------------------------------------------------ order.orderLines[0].numberItems | 1000 | number | Empty trip.steps[0].transportType | 1 | number | Empty order.orderLines[3].numberItems | 500 | number | Empty order.orderLines[1].weight | 35000.0 | number | Empty order.orderLines[2].weight | 14000.0 | number | Empty trip.steps[1].transportType | 2 | number | Empty trip.steps[2].transportType | 3 | number | Empty ============================================================ These seem like good choices for the input, but one thing we notice here is that the trip.steps[*].transportType was automatically categorized as a numeric feature by the DroolsWrapper, likely due to however the transportType attribute is handled in the ruleset. This should really be a categorical feature, as there are only three possible values (1=Ship, 2=Train, 3=Truck). We’ll override the automatic type inference, as well as specifiy some feature domains (valid value ranges) for each of our features: // set feature type overrides, anything matching this regex will be categorical HashMap&lt;String, Type&gt; featureTypeOverrides = new HashMap&lt;&gt;(); featureTypeOverrides.put("trip.steps\\[\\d+\\].transportType", Type.CATEGORICAL); droolsWrapper.setFeatureTypeOverrides(featureTypeOverrides); // set feature domains for (Feature f: droolsWrapper.featureExtractor(objectSupplier.get()).keySet()) { if (f.getName().contains("transportType")){ // transport type can be truck, train, ship FeatureDomain&lt;Object&gt; fd = ObjectFeatureDomain.create(List.of(Step.truck_TransportType, Step.train_TransportType, Step.Ship_TransportType)); droolsWrapper.addFeatureDomain(f.getName(), fd); } else { // let numeric features range from 0 to original value FeatureDomain nfd = NumericalFeatureDomain.create(0., ((Number) f.getValue().getUnderlyingObject()).doubleValue()); droolsWrapper.addFeatureDomain(f.getName(), nfd); } } droolsWrapper.displayFeatureCandidates(); === FEATURE CANDIDATES ================================================ Feature | Value | Type | Domain ----------------------------------------------------------------------- trip.steps[0].transportType | 1 | categorical | [1, 2, 3] order.orderLines[3].numberItems | 500 | number | 0.0-&gt;500.0 order.orderLines[2].weight | 14000.0 | number | 0.0-&gt;14000.0 trip.steps[2].transportType | 3 | categorical | [1, 2, 3] order.orderLines[1].weight | 35000.0 | number | 0.0-&gt;35000.0 trip.steps[1].transportType | 2 | categorical | [1, 2, 3] order.orderLines[0].numberItems | 1000 | number | 0.0-&gt;1000.0 ======================================================================= Our features seem correctly configured, so let’s move onto output selection. OUTPUT SELECTION We’ll immediately apply some output filters to to remove irrelevant rules, objects, and attributes and focus on the interesting output candidates: // exclude the following objects droolsWrapper.setExcludedOutputObjects(List.of( "pallets", "LeftToDistribute", "cost.Product", "cost.OrderLine", "java.lang.Double", "costElements", "Pallet", "City", "Step", "org.drools.core.reteoo.InitialFactImpl", "java.util.ArrayList")); // exclude the following field names droolsWrapper.setExcludedOutputFields(List.of("pallets", "order", "trip", "step", "distance", "transportType", "city", "Step")); // only look at consequences of the following rules droolsWrapper.setIncludedOutputRules(List.of("CalculateTotal")); droolsWrapper.generateOutputCandidates(true); Which produces the following results: === OUTPUT CANDIDATES ============================================================ Index | Rule | Field Name | Final Value ---------------------------------------------------------------------------------- 0 | CalculateTotal | totalHandlingCost | 5004.0 1 | CalculateTotal | rulebases.cost.CostCalculationRequest_1 | Created 2 | CalculateTotal | totalTransportCost | 2499790.0 3 | CalculateTotal | totalCost | 2505075.8 4 | CalculateTotal | numPallets | 547 5 | CalculateTotal | totalTaxCost | 281.8 ================================================================================== A SIMPLE SHAP EXPLANATION From these candidate outputs, let’s investigate how each of our items affected the Total Cost and Tax Cost of the shipment using SHAP. First, we’ll set these two as our desired outputs, using their indeces shown in table above: // select the 2nd and 5th options from the generated candidates droolsWrapper.selectOutputIndicesFromCandidates(List.of(2,5)); Next, we need to specify a background input to compare against in order to use SHAP. In our case, we’ll use a shipment containing 0 items/kilos of each item, all shipped by Truck as the comparison baseline: List&lt;Feature&gt; backgroundFeatures = new ArrayList&lt;&gt;(); for (int j = 0; j &lt; samplePI.getFeatures().size(); j++) { Feature f = samplePI.getFeatures().get(j); if (f.getName().contains("transportType")) { backgroundFeatures.add(FeatureFactory.copyOf(f, new Value(Step.truck_TransportType))); } else { backgroundFeatures.add(FeatureFactory.copyOf(f, new Value(0.))); } } List&lt;PredictionInput&gt; background = List.of(new PredictionInput(backgroundFeatures)); We can then run SHAP: Explainers.runSHAP(droolsWrapper, background) ----------------- OUTPUT CALCULATETOTAL: TOTALTRANSPORTCOST ----------------- Feature : SHAP Value FNull : 0.000 order.orderLines[1].weight = 35000.0 : 336125.000 +/- 1587245.191 order.orderLines[2].weight = 14000.0 : 134450.000 +/- 1587245.191 trip.steps[2].transportType = 3 : 0.000 +/- 0.000 order.orderLines[3].numberItems = 500 : 6722500.000 +/- 1587245.191 order.orderLines[0].numberItems = 1000 : 161340.000 +/- 1587245.191 trip.steps[1].transportType = 2 : -41025.000 +/- 1587245.191 trip.steps[0].transportType = 1 : -4813600.000 +/- 3549188.144 --------------------------------------------------------------------------- Prediction : 2499790.000 ----------------- OUTPUT CALCULATETOTAL TOTALTAXCOST ----------------- Feature : SHAP Value FNull : 33.000 order.orderLines[1].weight = 35000.0 : 98.000 +/- 0.000 order.orderLines[2].weight = 14000.0 : 98.000 +/- 0.000 trip.steps[2].transportType = 3 : 0.000 +/- 0.000 order.orderLines[3].numberItems = 500 : 32.000 +/- 0.000 order.orderLines[0].numberItems = 1000 : 20.800 +/- 0.000 trip.steps[1].transportType = 2 : 0.000 +/- 0.000 trip.steps[0].transportType = 1 : -0.000 +/- 0.000 --------------------------------------------------------------------- Prediction : 281.800 From these explanations, we can see a few interesting things. Namely, shipping by truck is the most expensive option as compared to shipping by train or ship; for example, the trip.steps[0].transportType= 1 : -4813600.000 shows that shipping the first leg of the journey via ship (transportType=1 in this particular example) saved $4,813,600 from the total cost as compared to shipping via truck. Additionally, we can see that the choice of shipping method has no effect on the tax cost: Feature : SHAP Value trip.steps[1].transportType = 2 : 0.000 +/- 0.000 trip.steps[0].transportType = 1 : -0.000 +/- 0.000 trip.steps[2].transportType = 3 : 0.000 +/- 0.000 A SIMPLE COUNTERFACTUAL EXPLANATION Another cool thing we can do is produce a counterfactual explanation of the model. Say for example we only had a shipping budget of $2,000,000, we can use the counterfactual explainer to find the closest shipment to our original one that meets our constraint. To do this, we need to specify our Counterfactual Goal: List&lt;Output&gt; goal = List.of( new Output( "rulebases.cost.CostCalculationRequest.totalCost", Type.NUMBER, new Value(2_000_000), // this is where we set our goal to 2 million 0.0d) ); Then we can run the counterfactual explainer: Explainers.runCounterfactualSearch( droolsWrapper, goal, .01, // want to get within 1% of goal 60L // allow 60 seconds of search time ); which, after a minute, outputs: === COUNTERFACTUAL INPUTS ====================================== Feature | Original Value → Found Value ---------------------------------------------------------------- order.orderLines[1].weight | 35000.0 → 35000.0 order.orderLines[2].weight | 14000.0 → 14000.0 order.orderLines[3].numberItems | 500 → 386 trip.steps[2].transportType | 3 → 3 trip.steps[1].transportType | 2 → 2 trip.steps[0].transportType | 1 → 1 order.orderLines[0].numberItems | 1000 → 1000 ================================================================ === COUNTERFACTUAL OUTPUT ====================================== Output | Original Value → Found Value ---------------------------------------------------------------- CalculateTotal: totalCost | 2505075.8 → 1983049.8 ================================================================ Meets Criteria? true The solution the counterfactual explainer has found makes just one change to the order (reducing order.orderLines[3].numberItems from 500 to 386) which results in a new totalCost of 1.98 million, which meets the criteria we set out originally. A MORE COMPLEX COUNTERFACTUAL We can try for even more complex outcomes too, for example, what if we wanted to keep our total shipped pallet count as close to unchanged as possible, while reducing our tax cost to $200? To do this, we can set our output targets in the DroolsWrapper and create a new Counterfactual search goal. Here, we’ll set the numPallets goal to 547 (our original pallet count) and the totalTaxCost goal to 200. droolsWrapper.selectOutputIndicesFromCandidates(List.of(4,5)); List&lt;Output&gt; goal = List.of( new Output("rulebases.cost.CostCalculationRequest.numPallets", Type.NUMBER, new Value(540), 0.0d), new Output("rulebases.cost.CostCalculationRequest.totalTaxCost", Type.NUMBER, new Value(200), 0.0d) ); Explainers.runCounterfactualSearch(droolsWrapper, goal, .005, //aim for within 0.5% of goals 300L //allow for 5 minutes of search time ); The counterfactual explainer will now try and find an input configuration that meets both criteria, and does so: === COUNTERFACTUAL INPUTS ============================================= Feature | Original Value → Found Value ----------------------------------------------------------------------- order.orderLines[0].numberItems | 1000 → 1000 trip.steps[0].transportType | 1 → 1 order.orderLines[3].numberItems | 500 → 498 trip.steps[1].transportType | 2 → 2 order.orderLines[1].weight | 35000.0 → 35000.0 trip.steps[2].transportType | 3 → 3 order.orderLines[2].weight | 14000.0 → 12841.676 ======================================================================= === COUNTERFACTUAL OUTPUT ============================================= Output | Original Value -&gt; Found Value ----------------------------------------------------------------------- numPallets | 547.0 -&gt; 545.0 totalTaxCost | 281.80 -&gt; 200.717 ======================================================================= Meets Criteria? true The counterfactual explainer has found a solution that removes just 2 pallets from the shipment, while reducing the tax cost from $281.80 down to $200.72. GET THE CODE The full code for the demo above can be seen . The entire TrustyAI-Drools repo is at . CONCLUSION In this blogpost we’ve taken a look at how to integrate TrustyAI’s explainers into Drools, allowing for the explanation of Drools ruleset evaluations via TrustyAI’s explanability algorithms. We’ve also taken a look at an example use-case, exploring how the explainers can give us insight about the functionality of rulesets as well as provide a new set of interesting features to Drools itself. The post appeared first on .</content><dc:creator>Rob Geada</dc:creator></entry><entry><title type="html">Kogito 1.24.0 released!</title><link rel="alternate" href="https://blog.kie.org/2022/07/kogito-1-24-0-released.html" /><author><name>Cristiano Nicolai</name></author><id>https://blog.kie.org/2022/07/kogito-1-24-0-released.html</id><updated>2022-07-08T06:38:35Z</updated><content type="html">We are glad to announce that the Kogito 1.24.0 release is now available! This goes hand in hand with , release. From a feature point of view, we have included a series of new features and bug fixes, including: * Persistence support for Process versions * Allow users to set up the configuration key for open API properties by setting  kogito.sw.operationIdStrategy. If none of the predefined approaches are suitable, users can define their own config key by using UriDefinitions extension. Possible values for the new property are: * FILE_NAME:  use the last element of the spec uri path. * FULL_URI:  use the full uri path to generate the key. * FUNCTION_NAME:  use the workflow id and the function name that references the spec uri * SPEC_TITLE: use specification title * Added the new kogito-quarkus-serverless-workflow-devui for Serverless Workflow testing * PostgreSQL persistence addon to support correlation BREAKING CHANGES * The swf-quarkus-extension module had its artifactId changed to kogito-quarkus-serverless-workflow-devui For more details head to the complete . All artifacts are available now: * Kogito runtime artifacts are available on Maven Central. * Kogito examples can be found . * Kogito images are available on . * Kogito operator is available in the in OpenShift and Kubernetes. * Kogito tooling 0.20.0 artifacts are available at the . A detailed changelog for 1.24.0 can be found in . New to Kogito? Check out our website . Click the "Get Started" button. The post appeared first on .</content><dc:creator>Cristiano Nicolai</dc:creator></entry><entry><title type="html">Saying goodbye to Red Hat</title><link rel="alternate" href="http://www.schabell.org/2022/07/saying-goodbye-to-red-hat.html" /><author><name>Eric D. Schabell</name></author><id>http://www.schabell.org/2022/07/saying-goodbye-to-red-hat.html</id><updated>2022-07-08T05:00:00Z</updated><content type="html">My tenure: 2009-2022 The time has come... the end of my tenure at Red Hat after +13 years.   How about a short summary of some of the highlights while at Red Hat before I move on to a new adventure?  I'll try to capture the big milestones, but there are just so many that I'm sure to miss a few.  It's a moment of reflection on more than a decade spent in the world of enterprise open source technologies and riding a wave that was Red Hat in the prime of its evolution in the industry. I have been involved in open source since my introduction late in life to programming, operating systems, and Linux in 1996. Before joining Red Hat I was involved in the community around a business process management project called . We were using this heavily in the financial institution I was working at and I became active as you do when you submerge yourself into an open source technology. In those days it was still jBPM 3 and we were having the time of our lives learning to model and implement processes. We even went crazy with a , to just give you an example of what we thought was possible back in those days.  It all led to Red Hat knocking on my door in 2009 to join the Netherlands team as their first middleware (JBoss) solution architect.  At the time I joined I remember the SA team was about 15 associates across Europe, Red Hat had around 2000 associates world wide, and the stock price was between $22-$24 (later sold to IBM for $190).  It was incredible fun and there was a very active start up feel to what we were doing. My experiences with JBoss technologies and specifically the Business Rules Management System (BRMS) led to me spending more time across Europe helping other sales teams than in my own region. I was also still active in the community around those products, meaning Drools, jBPM, and other JBoss projects upstream.  This role I held for approximately three years before the product orgnanization came knocking. My contacts in the product teams led to my being asked to join the Middleware Business Unit as one of their first MW Technical Marketing Managers or TMM. At this time I think we had like 4-5 products so one person could manage the role. I ended up spending around four years in this role and we grew not only our MW product portfolio, but the team eventually grew out to five TMM's before I moved on.  We worked on products like JBoss SOA-P, Switchyard, JBoss Virtualization, JBoss BPM Suite, the first versions of OpenShift (remember gears?), and so much more. This role required enablement sessions being delivered to the field and we visited all our regional offices; APAC (Singapore, Tokyo, Beijing), EMEA (Munich, London, Stockholm, Madrid, Amsterdam, Brussels, Rome, Warsaw, etc), NA (Boston, Dallas, Raleigh, Mountain View, Portland, St. Louis, Tampa, etc), and LATAM (Mexico City, San Paulo, etc).  Right at the end of that role I got the chance to move to the US, out on an island off the coast of North Carolina for a year. Quite the adventure and my family loved every minute of our island time. The next role to come along was in a brand new business unit, called the Integrated Solutions Business Unit, setup to try and pull together the first multi-product products for our field to sell. I spent the next two years working with some serious rock stars as we pulled together Red Hat Cloud Infrastructure and Red Hat Cloud Suite products. I was the TMM for both of these, and due to our PMM leaving before the launch of the Red Hat Cloud Suite I picked up the PMM work for the product launch.  Part of this period also involved the setting up of the internal Red Hat TMM Practice, where all existing and new TMM's could find a baseline on how to be effective in their roles. We also worked with Red Hat HR and developed a well defined ladder for the creation of the official TMM role in Red Hat.  My final role was to help setup a new concept, that of the Portfolio Architecture team. We have spent the last four years defining, researching, receiving field feedback, and creating 25 published architectures that you can explore on the Red Hat Portfolio Architecture Center. This team started with just three of us, but has since grown out to seven and I had the honour of being the technical director of this developing product.  All along this journey through Red Hat I've spent many hours mentoring all manner of TMM's (and other associates) as I am a true believer that we all have something to share to make others better. I never suspected how long this would last when I walked into that first interview and they asked me to draw a SOA architecture on a whiteboard. I look around now 13 yrs, 106 days later and I'm still engaged at Red Hat.  Now this brings me to the part were we say goodbye and thank you all for the fun we've had together. There are so many colleagues that have become both friends and family, so it's not even possible to name them (but you know who you are). My last day is next week Friday, 15 July. What's next you ask?  Stay tuned for more on that after I take a break between the old and the new. </content><dc:creator>Eric D. Schabell</dc:creator></entry><entry><title type="html">Refactoring the Drools Compiler</title><link rel="alternate" href="https://blog.kie.org/2022/07/refactoring-the-drools-compiler.html" /><author><name>Edoardo Vacchi</name></author><id>https://blog.kie.org/2022/07/refactoring-the-drools-compiler.html</id><updated>2022-07-07T14:00:00Z</updated><content type="html">In the past few weeks we have been working hard on redesigning the architecture of Drools the rules engine and the rest of our ecosystem of runtime engines. In this blog post I want to focus a bit on the refactoring of the KnowledgeBuilder, a core component of the build infrastructure of the v6-v7 APIs. For the ongoing work on Drools 8, we are rethinking the entire design of this and other components. On the latest stable version of the 7 series, that contains logic for processing resources of different types (such as DRL, DMN, BPMN, PMML, XLS etc…) On the main branch, , where most of the fat is really public methods that we kept for backwards compatibility, that are now delegating to new self-contained classes. The main culprit with the KnowledgeBuilderImpl was that it was both the class holding the logic for building assets, and both a sort of "context" object that was passed around to collect pieces of information. The main goals of the refactoring were 1. Refactoring most of the state inside the KnowledgeBuilderImpl into smaller objects with well-defined boundaries 2. Moving the building logic related to the DRL family (plain DRL, XLS, DSLs etc.) to a series smaller, composable 3. Ensuring that each CompilationPhase never referred directly the KnowledgeBuilderImpl The same work involved the CompositeKnowledgeBuilderImpl (which decorates KnowledgeBuilderImpl) and for the ModelBuilderImpl (which subclasses the KnowledgeBuilderImpl). As you can imagine the work was a bit long and iterative, but the good news is that it is now possible to put the CompositePhases in sequence, instantiating them without requiring the entire KnowledgeBuilder, but just its constituent. The KnowledgeBuilderImpl itself now implements by delegating to self-contained objects (e.g. , , ). The phases always refer to such interfaces, e.g., a only refers to a . The result is that now it is possible to put in sequence such phases to produce a: List&lt;CompilationPhase&gt; phases = asList( new ImportCompilationPhase(packageRegistry, packageDescr), new TypeDeclarationAnnotationNormalizer(annotationNormalizer, packageDescr), new EntryPointDeclarationCompilationPhase(packageRegistry, packageDescr), new AccumulateFunctionCompilationPhase(packageRegistry, packageDescr), new TypeDeclarationCompilationPhase(packageDescr, typeBuilder, packageRegistry, null), new WindowDeclarationCompilationPhase(packageRegistry, packageDescr, typeDeclarationContext), new FunctionCompilationPhase(packageRegistry, packageDescr, configuration), new ImmutableGlobalCompilationPhase(packageRegistry, packageDescr, globalVariableContext), new RuleAnnotationNormalizer(annotationNormalizer, packageDescr), new RuleValidator(packageRegistry, packageDescr, configuration), new ImmutableFunctionCompiler(packageRegistry, packageDescr, rootClassLoader), new ImmutableRuleCompilationPhase(packageRegistry, packageDescr, parallelRulesBuildThreshold, attributesForPackage, resource, typeDeclarationContext), new ConsequenceCompilationPhase(packageRegistryManager) ); The same is true both for the traditional in-memory compiler, and . This huge refactoring makes it possible to reuse most of the logic in the traditional compilation flow in a new compiler architecture that is currently being worked on. Stay tuned for more details! The post appeared first on .</content><dc:creator>Edoardo Vacchi</dc:creator></entry><entry><title>Add an Infinispan cache to your ASP.NET application</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/07/07/add-infinispan-cache-your-aspnet-application" /><author><name>Vittorio Rigamonti</name></author><id>290f5459-e17a-43b6-ae71-20d8acf9484e</id><updated>2022-07-07T07:00:00Z</updated><published>2022-07-07T07:00:00Z</published><summary type="html">&lt;p&gt;The open source &lt;a&gt;Infinispan&lt;/a&gt; data store is popular for in-memory operations. A &lt;a href="https://developers.redhat.com/topics/dotnet"&gt;.NET Core&lt;/a&gt; application can now easily integrate Infinispan as a caching service or session provider. This article provides basic information on how to do that in &lt;a href="https://developers.redhat.com/topics/c"&gt;C#&lt;/a&gt; on &lt;a href="https://developers.redhat.com/topics/linux"&gt;Linux&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;What you need:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;.NET 6.0+ installed on your system&lt;/li&gt; &lt;li&gt;Access to get packages from &lt;a href="https://www.nuget.org"&gt;NuGet&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Access to an Infinispan server&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;On the Infinispan server, create a cache named &lt;code&gt;default&lt;/code&gt; listening on the loopback host and port 127.0.0.1:11222, without authentication.&lt;/p&gt; &lt;h2&gt;Create the application&lt;/h2&gt; &lt;p&gt;You are going to work on an &lt;a href="https://docs.microsoft.com/en-us/aspnet/core/?view=aspnetcore-6.0"&gt;ASP.NET Core&lt;/a&gt; application, so run the following command to generate a new application scaffold:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;dotnet new webapp -lang 'C#' -n Infinispan.Example.Caching -f net6.0&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This command creates a new folder with an empty but working ASP.NET Core application. Run the application as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cd Infinispan.Example.Caching dotnet run&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Check the application's log message for its HTTP or HTTPS URL and enter it into your browser to see the application's display.&lt;/p&gt; &lt;h2&gt;Add Infinispan as a cache&lt;/h2&gt; &lt;p&gt;The application requires the Infinispan caching package, so add it as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;dotnet add package Infinispan.Hotrod.Caching --version 0.0.1-alpha3&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This package provides an IDistibutedCache implementation based on the Infinispan C# client, imported as a dependency.&lt;/p&gt; &lt;h2&gt;Set up Infinispan as a cache provider&lt;/h2&gt; &lt;p&gt;An ASP.NET Core application (6.0) provides all the services and pipeline setup in the &lt;code&gt;Program.cs&lt;/code&gt; file. To this file, you need to add the Infinispan client configuration in the service setup, as well as session management in the process pipeline. Set the cache entries to expire after 10 seconds of idle time. &lt;code&gt;Program.cs&lt;/code&gt; should now look like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-diff"&gt;using Infinispan.Hotrod.Core; using Infinispan.Hotrod.Caching.Distributed; var builder = WebApplication.CreateBuilder(args); // Add services to the container. // Infinispan default setup is used: // 127.0.0.1:11222 cacheName: default builder.Services.AddInfinispanCache(); builder.Services.AddSession(options =&gt; { options.IdleTimeout = TimeSpan.FromSeconds(10); options.Cookie.HttpOnly = true; options.Cookie.IsEssential = true; }); builder.Services.AddRazorPages(); var app = builder.Build(); // Configure the HTTP request pipeline. if (!app.Environment.IsDevelopment()) { app.UseExceptionHandler("/Error"); // The default HSTS value is 30 days. You may want to change this for production scenarios, see https://aka.ms/aspnetcore-hsts. app.UseHsts(); } app.UseHttpsRedirection(); app.UseStaticFiles(); app.UseRouting(); app.UseAuthorization(); app.UseSession(); app.MapRazorPages(); app.Run();&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Add business code&lt;/h2&gt; &lt;p&gt;Our business code is very simple: It presents some information (the user name, the age of the session, and the first access time) to the user as a result of a request. The data is fetched from the session cache if available there and computed by the program otherwise. This logic is implemented in the model file &lt;code&gt;Pages/Index.cshtml.cs&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cs"&gt;using Microsoft.AspNetCore.Mvc; using Microsoft.AspNetCore.Mvc.RazorPages; namespace Infinispan.Example.Caching.Pages { public class IndexModel : PageModel { private readonly ILogger&lt;IndexModel&gt; _logger; public IndexModel(ILogger&lt;IndexModel&gt; logger) { _logger = logger; } public const string SessionKeyName = "_Name"; public const string SessionKeyAge = "_Age"; public const string SessionKeyFirstAccess = "_FirstAccess"; public static Random RndSource = new Random(); public string DataSource { get; private set; } = ""; public void OnGet() { // Requires: using Microsoft.AspNetCore.Http; if (string.IsNullOrEmpty(HttpContext.Session.GetString(SessionKeyName))) { DataSource = "Computed"; HttpContext.Session.SetString(SessionKeyName, "Mickey"); HttpContext.Session.SetInt32(SessionKeyAge, RndSource.Next(100)); HttpContext.Session.SetString(SessionKeyFirstAccess, DateTime.Now.ToString()); return; } DataSource = "Cache"; } } }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The output is generated from a file named &lt;code&gt;Pages/Index.cshtml&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-html"&gt;@page @using Infinispan.Example.Caching.Pages @using Microsoft.AspNetCore.Http @model IndexModel @{ ViewData["Title"] = "Home page"; } &lt;div class="text-center"&gt; &lt;h1 class="display-4"&gt;Welcome&lt;/h1&gt; &lt;h2&gt;The time on the server is @DateTime.Now&lt;/h2&gt; &lt;h2&gt;Name (@Model.DataSource): @HttpContext.Session.GetString(IndexModel.SessionKeyName)&lt;/h2&gt; &lt;h2&gt;Age (@Model.DataSource): @HttpContext.Session.GetInt32(IndexModel.SessionKeyAge)&lt;/h2&gt; &lt;h2&gt;First Access Date (@Model.DataSource): @HttpContext.Session.GetString(IndexModel.SessionKeyFirstAccess)&lt;/h2&gt; &lt;p&gt;Learn about &lt;a href="https://docs.microsoft.com/aspnet/core"&gt;building Web apps with ASP.NET Core&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Easy ASP.NET access to Infinispan&lt;/h2&gt; &lt;p&gt;Everything is now in place for the show. Run your application again and check the output in the browser. The &lt;code&gt;Age &lt;/code&gt;and the &lt;code&gt;First Access Date&lt;/code&gt; field are computed the first time and cached for 10 seconds. The session itself expires after 10 seconds of idle time, as configured in the services configuration. Therefore, the cached values are reused by the application if requests come in quick succession.&lt;/p&gt; &lt;p&gt;This article has demonstrated how easily you can integrate Infinispan as a distributed cache and session provider for ASP.Net Core applications.&lt;/p&gt; &lt;p&gt;Explore more .NET tutorials on Red Hat Developer:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/03/21/hello-podman-using-net"&gt;Hello Podman using .NET&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/02/22/debug-net-applications-running-local-containers-vs-code"&gt;Debug .NET applications running in local containers with VS Code&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/07/07/deploy-net-applications-red-hat-openshift-using-helm"&gt;Deploy .NET applications on Red Hat OpenShift using Helm&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/07/07/add-infinispan-cache-your-aspnet-application" title="Add an Infinispan cache to your ASP.NET application"&gt;Add an Infinispan cache to your ASP.NET application&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Vittorio Rigamonti</dc:creator><dc:date>2022-07-07T07:00:00Z</dc:date></entry></feed>
