<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title type="html">SOAP Web Services with Quarkus made easy</title><link rel="alternate" href="https://www.mastertheboss.com/soa-cloud/quarkus/soap-web-services-with-quarkus-made-easy/" /><author><name>F.Marchioni</name></author><id>https://www.mastertheboss.com/soa-cloud/quarkus/soap-web-services-with-quarkus-made-easy/</id><updated>2023-06-06T12:38:41Z</updated><content type="html">In this step-by-step guide we will learn how to code, run and test SOAP web services using the Quarkus CXF Extensions. We will first learn how to deploy a simple SOAP Web services and then we will consume it with different Clients such as SOAP UI, Java Client or a Camel CXF Consumer. Prerequisites Before ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>How to create an EC2 instance in AWS using Ansible CLI</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/06/05/how-create-ec2-instance-aws-using-ansible-cli" /><author><name>Deepankar Jain</name></author><id>5ddcb81a-a6ef-4bb8-8049-2af1a4f347d5</id><updated>2023-06-05T07:00:00Z</updated><published>2023-06-05T07:00:00Z</published><summary type="html">&lt;p&gt;This is the first article in a series that covers the end-to-end process of creating an Elastic Compute Cloud (EC2) instance on Amazon Web Services (AWS) using &lt;a href="https://developers.redhat.com/products/ansible/"&gt;Red Hat Ansible Automation Platform&lt;/a&gt;. This tutorial demonstrates the steps required to set up the necessary resources to create an EC2 instance using the AWS module in Ansible Automation Platform.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Follow our 3-part series:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Part 1: How to create an EC2 instance in AWS using Ansible CLI&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Part 2: &lt;a href="https://developers.redhat.com/articles/2023/06/05/how-create-ec2-instance-aws-using-ansible-automation"&gt;How to create an EC2 instance in AWS using Ansible Automation&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Part 3: &lt;a href="https://developers.redhat.com/articles/2023/06/05/how-create-ec2-instance-aws-using-ansible-workflow"&gt;How to create an EC2 instance in AWS using Ansible workflow&lt;/a&gt;&lt;/p&gt; &lt;p&gt;By the end of this article, you will have a better understanding of how to use the Ansible Automation Platform CLI to manage an EC2 instance and how it can help to streamline your infrastructure management workflows.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;You must have an active &lt;a href="https://portal.aws.amazon.com/billing/signup#/start/email"&gt;AWS account&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;How to create an EC2 instance&lt;/h2&gt; &lt;p&gt;The following steps demonstrate how to use the Ansible Automation Platform CLI to create an EC2 instance on AWS.&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Step 1: Generate the access key and secret key from the &lt;a href="https://docs.aws.amazon.com/powershell/latest/userguide/pstools-appendix-sign-up.html"&gt;AWS documentation&lt;/a&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Step 2: Open any editor on your local machine and enter the following .yml:&lt;/li&gt; &lt;/ul&gt;&lt;pre&gt; &lt;code&gt;--- # Creating an EC2 Instance  - name: Creating an EC2 Instance with ansible cli   hosts: localhost   vars:     region: ap-south-1     instance_type: t3.micro     ami: ami-0f9d9a251c1a44858     key_name: ansible-demo     vpc_name: ansible-vpc-test     cidr_block: "10.10.0.0/16"     cidr: "10.10.0.0/24"     aws_access_key:&lt;YOUR ACCESS KEY&gt;     aws_secret_key: &lt;YOUR SECRET KEY&gt;   tasks:     - name: Create a new VPC        amazon.aws.ec2_vpc_net:         name: "Ansible-Test"         cidr_block: "{{ cidr_block }}"         aws_access_key: "{{ aws_access_key }}"         aws_secret_key: "{{ aws_secret_key }}"         region: "{{ region }}"       register: vpc     - name: Create a new Subnet       amazon.aws.ec2_vpc_subnet:         aws_access_key: "{{ aws_access_key }}"         aws_secret_key: "{{ aws_secret_key }}"         cidr: "{{ cidr }}"         region: "{{ region }}"         vpc_id: "{{ vpc.vpc.id }}"       register: subnet          - name: Create a Security Group         amazon.aws.ec2_security_group:         name: "Ansible-Test-Security-Group"         description: "Ansible-Testing"         aws_access_key: "{{ aws_access_key }}"         aws_secret_key: "{{ aws_secret_key }}"         vpc_id: "{{ vpc.vpc.id }}"         region: "{{ region }}"         rules:           - proto: tcp             ports:             - 80             cidr_ip: 0.0.0.0/0             rule_desc: "allow all on port 80"       register: security_group         - name: Launch an EC2 Instance       amazon.aws.ec2_instance:         name: "Test-Ansible"         key_name: "{{ key_name }}"         aws_access_key: "{{ aws_access_key }}"         aws_secret_key: "{{ aws_secret_key }}"         vpc_subnet_id: "{{ subnet.subnet.id }}"         instance_type: "{{ instance_type }}"         security_group: "{{ security_group.group_id  }}"         count: 1         wait: yes         aws_region: "ap-south-1"         network:           assign_public_ip: true         image_id: "{{ ami }}"&lt;/code&gt;&lt;/pre&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Step 3: Save and close the file. &lt;/li&gt; &lt;li aria-level="1"&gt;Step 4: Open the terminal in the directory where the file is located on your local machine.&lt;/li&gt; &lt;li aria-level="1"&gt;Step 5: Run the following command: &lt;pre&gt; &lt;code class="language-bash"&gt;ansible-playbook &lt;filename&gt;.yml&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The output is as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;ansible-playbook create_ec2_cli.yml [WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match 'all' PLAY [Creating an EC2 Instance with ansible cli] ****************************************************************************************************************************************************************** TASK [Gathering Facts] ******************************************************************************************************************************************************************************************** ok: [localhost] TASK [Create a new VPC] ******************************************************************************************************************************************************************************************* changed: [localhost] TASK [Create a new Subnet] **************************************************************************************************************************************************************************************** changed: [localhost] TASK [Create a Security Group] ************************************************************************************************************************************************************************************ changed: [localhost] TASK [Launch an EC2 Instance] ************************************************************************************************************************************************************************************* changed: [localhost] PLAY RECAP ******************************************************************************************************************************************************************************************************** localhost : ok=5 changed=4 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Figure 1 illustrates the EC2 instance in AWS.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_2023-04-28_101134.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_2023-04-28_101134.png?itok=u3OP0BGQ" width="600" height="321" alt="A screenshot of the EC2 instance in AWS." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: The EC2 instance in AWS.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2&gt;Stay tuned for more learning opportunities&lt;/h2&gt; &lt;p&gt;In this article, we have demonstrated how to create an EC2 instance using Ansible Automation Platform. By following this step-by-step guide, you should now have a good understanding of how to use Ansible Automation Platform to create an EC2 instance on AWS. To learn more about Ansible Automation Platform and additional resources and guides, including examples and use cases, we recommend visiting the &lt;a href="https://developers.redhat.com/learn/ansible"&gt;Red Hat Ansible Automation Platform&lt;/a&gt; learning portal.&lt;/p&gt; &lt;p&gt;If you are interested in exploring how to use Ansible Automation Platform on Azure, you try hands-on interactive &lt;a href="https://developers.redhat.com/content-gateway/link/3872066"&gt;labs&lt;/a&gt;. These labs show how to automate infrastructure deployment. To understand automation more in-depth, you can refer to the ebook &lt;a href="https://developers.redhat.com/e-books/it-executives-guide-automation"&gt;An IT executive's guide to automation&lt;/a&gt;, which provides a comprehensive overview of automation's impact on businesses. If you are new to Ansible Automation Platform, you can &lt;a href="https://developers.redhat.com/products/ansible/download"&gt;download it&lt;/a&gt; and get started by exploring interactive labs at no cost. Keep exploring and stay up to date with the latest trends and techniques in cloud infrastructure management on &lt;a href="https://developers.redhat.com/"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;In our &lt;a href="https://developers.redhat.com/articles/2023/06/05/how-create-ec2-instance-aws-using-ansible-automation"&gt;next article&lt;/a&gt;, we will explore how &lt;a href="https://developers.redhat.com/products/ansible/overview"&gt;Ansible Automation Platform&lt;/a&gt; further eases the process of creating EC2 instances, by enabling you to define infrastructure as code, track infrastructure changes, and enforce compliance policies.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/06/05/how-create-ec2-instance-aws-using-ansible-cli" title="How to create an EC2 instance in AWS using Ansible CLI"&gt;How to create an EC2 instance in AWS using Ansible CLI&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Deepankar Jain</dc:creator><dc:date>2023-06-05T07:00:00Z</dc:date></entry><entry><title>JBoss Tools for Eclipse 2023-06M2</title><link rel="alternate" type="text/html" href="https://tools.jboss.org/blog/4.28.0.am1.html" /><category term="release" /><category term="jbosstools" /><category term="jbosscentral" /><author><name>sbouchet</name></author><id>https://tools.jboss.org/blog/4.28.0.am1.html</id><updated>2023-06-05T16:14:54Z</updated><published>2023-06-05T00:00:00Z</published><content type="html">&lt;div&gt;&lt;div id="preamble"&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Happy to announce 4.28.0.AM1 (Developer Milestone 1) build for Eclipse 2023-06M2.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Downloads available at &lt;a href="https://tools.jboss.org/downloads/jbosstools/2023-06/4.28.0.AM1.html"&gt;JBoss Tools 4.28.0 AM1&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="what-is-new"&gt;&lt;a class="anchor" href="#what-is-new"&gt;&lt;/a&gt;What is New?&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Full info is at &lt;a href="https://tools.jboss.org/documentation/whatsnew/jbosstools/4.28.0.AM1.html"&gt;this page&lt;/a&gt;. Some highlights are below.&lt;/p&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="general"&gt;&lt;a class="anchor" href="#general"&gt;&lt;/a&gt;General&lt;/h3&gt; &lt;div class="sect3"&gt; &lt;h4 id="components-depreciation"&gt;&lt;a class="anchor" href="#components-depreciation"&gt;&lt;/a&gt;Components Depreciation&lt;/h4&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Fuse Tooling is now deprecated. &lt;a href="https://issues.redhat.com/browse/FUSETOOLS-3685"&gt;More information here.&lt;/a&gt;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect3"&gt; &lt;h4 id="components-removal"&gt;&lt;a class="anchor" href="#components-removal"&gt;&lt;/a&gt;Components Removal&lt;/h4&gt; &lt;div class="paragraph"&gt; &lt;p&gt;As previously announced &lt;a href="https://issues.redhat.com/browse/JBIDE-28678"&gt;here&lt;/a&gt;, we’ve removed the Central / update tab, SEAM and JSF support from JBossTools.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;For central update tab, all the existing central extensions can be installed with the marketplace client, using the entries listed &lt;a href="https://issues.redhat.com/browse/JBIDE-28853"&gt;here.&lt;/a&gt;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="future-releases-cadences"&gt;&lt;a class="anchor" href="#future-releases-cadences"&gt;&lt;/a&gt;Future releases cadences&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Starting from 4.28.0.Final, there will be no more .AM1 releases. The .Final releases can be now scheduled close to the Eclipse releases.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="hibernate-tools"&gt;&lt;a class="anchor" href="#hibernate-tools"&gt;&lt;/a&gt;Hibernate Tools&lt;/h3&gt; &lt;div class="sect3"&gt; &lt;h4 id="runtime-provider-updates"&gt;&lt;a class="anchor" href="#runtime-provider-updates"&gt;&lt;/a&gt;Runtime Provider Updates&lt;/h4&gt; &lt;div class="paragraph"&gt; &lt;p&gt;A new Hibernate 6.2 runtime provider incorporates Hibernate Core version 6.2.3.Final, Hibernate Ant version 6.2.3.Final and Hibernate Tools version 6.2.3.Final.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="and-more"&gt;&lt;a class="anchor" href="#and-more"&gt;&lt;/a&gt;And more…​&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;You can find more noteworthy updates in on &lt;a href="https://tools.jboss.org/documentation/whatsnew/jbosstools/4.28.0.AM1.html"&gt;this page&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Enjoy!&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Stéphane Bouchet&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt;</content><summary>Happy to announce 4.28.0.AM1 (Developer Milestone 1) build for Eclipse 2023-06M2. Downloads available at JBoss Tools 4.28.0 AM1. What is New? Full info is at this page. Some highlights are below. General Components Depreciation Fuse Tooling is now deprecated. More information here. Components Removal As previously announced here, we’ve removed the Central / update tab, SEAM and JSF support from JBossTools. For central update tab, all the existing central extensions can be installed with the marketplace client, using the entries listed here. Future releases cadences Starting from 4.28.0.Final, there will be no more .AM1 releases. The .Final releases can be now scheduled close to the Eclipse releases. Hibernate Tools Runtime Provider Updates A new...</summary><dc:creator>sbouchet</dc:creator><dc:date>2023-06-05T00:00:00Z</dc:date></entry><entry><title>Automate your Quarkus deployment using Ansible</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/06/01/automate-your-quarkus-deployment-using-ansible" /><author><name>Romain Pelisse</name></author><id>0e1a45d5-12ee-44f5-9e84-7181a36393b2</id><updated>2023-06-01T07:00:00Z</updated><published>2023-06-01T07:00:00Z</published><summary type="html">&lt;p&gt;In this article, we’ll explain how to use Ansible to build and deploy a &lt;a href="https://developers.redhat.com/products/quarkus/overview"&gt;Quarkus application&lt;/a&gt;. &lt;a href="https://www.redhat.com/en/topics/cloud-native-apps/what-is-quarkus"&gt;Quarkus&lt;/a&gt; is an exciting, lightweight &lt;a href="https://developers.redhat.com/java"&gt;Java&lt;/a&gt; development framework designed for cloud and &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; deployments, and &lt;a href="https://developers.redhat.com/products/ansible/"&gt;Red Hat Ansible Automation Platform&lt;/a&gt;  is one of the most popular &lt;a href="https://developers.redhat.com/topics/automation"&gt;automation&lt;/a&gt; tools and a star product from Red Hat.&lt;/p&gt; &lt;h2&gt;Set up your Ansible environment&lt;/h2&gt; &lt;p&gt;Before discussing how to automate a Quarkus application deployment using Ansible, we need to ensure the prerequisites are in place. First, you have to install Ansible on your development environment. On a Fedora or a &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; machine, this is achieved easily utilizing the &lt;a href="https://docs.fedoraproject.org/en-US/quick-docs/dnf/"&gt;dnf package manager&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ dnf install ansible-core&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The only other requirement is to install the Ansible collection dedicated to Quarkus:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ansible-galaxy collection install middleware_automation.quarkus&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This is all you need to prepare the Ansible control machine (the name given to the machine executing Ansible).&lt;/p&gt; &lt;p&gt;Generally, the &lt;a href="https://docs.ansible.com/ansible/latest/network/getting_started/basic_concepts.html#control-node"&gt;control node&lt;/a&gt; is used to set up other systems that are designated under the name &lt;strong&gt;targets&lt;/strong&gt;. For the purpose of this tutorial, and for simplicity's sake, we are going to utilize the same system for both the control node and our (only) target. This will make it easier to reproduce the content of this article on a single development machine.&lt;/p&gt; &lt;p&gt;Note that you don’t need to set up any kind of Java development environment, because the Ansible collection will take care of that.&lt;/p&gt; &lt;p&gt;The Ansible collection dedicated to Quarkus is a community project, and it’s not supported by Red Hat. However, both Quarkus and Ansible are Red Hat products and thus fully supported. The Quarkus collection might be supported at some point in the future, but is not as the time of the writing of this article.&lt;/p&gt; &lt;h3&gt;Inventory file&lt;/h3&gt; &lt;p&gt;Before we can execute Ansible, we need to provide to the tool an &lt;a href="https://docs.ansible.com/ansible/latest/inventory_guide/intro_inventory.html"&gt;inventory&lt;/a&gt; of the targets. There are many ways to achieve that, but the simplest solution for a tutorial such as this one is to write up an inventory file of our own.&lt;/p&gt; &lt;p&gt;As mentioned above, we are going to use the same host for both the controller and the target, so the inventory file has only one host. Here again, for simplicity's sake, this machine is going to be the localhost: &lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ cat inventory [all] localhost ansible_connection=local&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Refer to the Ansible documentation for more information on &lt;a href="https://docs.ansible.com/ansible/latest/inventory_guide/intro_inventory.html"&gt;Ansible inventory&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Build and deploy the app with Ansible&lt;/h2&gt; &lt;p&gt;For this demonstration, we are going to utilize one of the sample applications provided as part of the &lt;a href="https://github.com/quarkusio/quarkus-quickstarts/tree/main/getting-started"&gt;Quarkus quick starts project&lt;/a&gt;. We will use Ansible to build and deploy the &lt;strong&gt;getting started&lt;/strong&gt; application.&lt;/p&gt; &lt;p&gt;All we need to provide to Ansible is the application name, repository URL, and the destination folder, where to deploy the application on the target. Because of the directory structure of the Quarkus quick start, containing several projects, we'll also need to specify the directory containing the source code:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ansible-playbook -i inventory middleware_automation.quarkus.playbook \ -e app_name='optaplanner-quickstart' \ -e quarkus_app_source_folder='optaplanner-quickstart' \ -e quarkus_path_to_folder_to_deploy=/opt/optplanner \ -e quarkus_app_repo_url='https://github.com/quarkusio/quarkus-quickstarts.git'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Below is the output of this command :&lt;/p&gt; &lt;pre&gt; PLAY [Build and deploy a Quarkus app using Ansible] **************************** TASK [Gathering Facts] ********************************************************* ok: [localhost] TASK [Build the Quarkus from https://github.com/quarkusio/quarkus-quickstarts.git.] *** TASK [middleware_automation.quarkus.quarkus : Ensure required parameters are provided.] *** ok: [localhost] TASK [middleware_automation.quarkus.quarkus : Define path to mvnw script.] ***** ok: [localhost] TASK [middleware_automation.quarkus.quarkus : Ensure that builder host localhost has appropriate JDK installed: java-17-openjdk] *** changed: [localhost] TASK [middleware_automation.quarkus.quarkus : Delete previous workdir (if requested).] *** ok: [localhost] TASK [middleware_automation.quarkus.quarkus : Ensure app workdir exists: /tmp/workdir] *** changed: [localhost] TASK [middleware_automation.quarkus.quarkus : Checkout the application source code.] *** changed: [localhost] TASK [middleware_automation.quarkus.quarkus : Build the App using Maven] ******* ok: [localhost] TASK [middleware_automation.quarkus.quarkus : Display build application log] *** skipping: [localhost] TASK [Deploy Quarkus app on target.] ******************************************* TASK [middleware_automation.quarkus.quarkus : Ensure required parameters are provided.] *** ok: [localhost] TASK [middleware_automation.quarkus.quarkus : Ensure requirements on target system are fullfilled.] *** included: /root/.ansible/collections/ansible_collections/middleware_automation/quarkus/roles/quarkus/tasks/deploy/prereqs.yml for localhost TASK [middleware_automation.quarkus.quarkus : Ensure required OpenJDK is installed on target.] *** skipping: [localhost] TASK [middleware_automation.quarkus.quarkus : Ensure Quarkus system group exists on target system] *** changed: [localhost] TASK [middleware_automation.quarkus.quarkus : Ensure Quarkus user exists on target system.] *** changed: [localhost] TASK [middleware_automation.quarkus.quarkus : Ensure deployement directory exits: /opt/optplanner.] *** changed: [localhost] TASK [middleware_automation.quarkus.quarkus : Set Quarkus app source dir (if not defined).] *** ok: [localhost] TASK [middleware_automation.quarkus.quarkus : Deploy application as a systemd service on target system.] *** included: /root/.ansible/collections/ansible_collections/middleware_automation/quarkus/roles/quarkus/tasks/deploy/service.yml for localhost TASK [middleware_automation.quarkus.quarkus : Deploy application from to target system] *** ok: [localhost] TASK [middleware_automation.quarkus.quarkus : Deploy Systemd configuration for Quarkus app] *** changed: [localhost] TASK [middleware_automation.quarkus.quarkus : Perform daemon-reload to ensure the changes are picked up] *** ok: [localhost] TASK [middleware_automation.quarkus.quarkus : Ensure Quarkus app service is running.] *** changed: [localhost] TASK [middleware_automation.quarkus.quarkus : Ensure firewalld configuration is appropriate (if requested).] *** skipping: [localhost] PLAY RECAP ********************************************************************* localhost : ok=19 changed=8 unreachable=0 failed=0 skipped=3 rescued=0 ignored=0 &lt;/pre&gt; &lt;p&gt;As you can see, the Ansible collection for Quarkus does all the heavy lifting for us: its content takes care of checking out the source code from GitHub and builds the application. It also ensures the system used for this step has the required OpenJDK installed on the target machine.&lt;/p&gt; &lt;p&gt;Once the application is successfully built, the collection takes care of the deployment. Here again, it checks that the appropriate OpenJDK is available on the target system. Then, it verifies that the required user and group exist on the target and if not, creates them. This is recommended mostly to be able to run the Quarkus application with a regular user, rather than with the root account.&lt;/p&gt; &lt;p&gt;With those requirements in place, the jars produced during the build phase are copied over to the target, along with the required configuration for the application integration into &lt;a href="https://developers.redhat.com/cheat-sheets/systemd-commands-cheat-sheet"&gt;systemd&lt;/a&gt; as a service. Any change to the systemd configuration requires reloading its daemon, which the collection ensures will happen whenever it is needed. With all of that in place, the collection starts the service itself.&lt;/p&gt; &lt;h2&gt;Validate the execution results &lt;/h2&gt; &lt;p&gt;Let’s take a minute to verify that all went well and that the service is indeed running:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# systemctl status optaplanner-quickstart.service ● optaplanner-quickstart.service - A Quarkus service named optaplanner-quickstart Loaded: loaded (/usr/lib/systemd/system/optaplanner-quickstart.service; enabled; vendor preset: disabled) Active: active (running) since Wed 2023-04-26 09:40:13 UTC; 3h 19min ago Main PID: 934 (java) CGroup: /system.slice/optaplanner-quickstart.service └─934 /usr/bin/java -jar /opt/optplanner/quarkus-run.jar Apr 26 09:40:13 be44b3acb1f3 systemd[1]: Started A Quarkus service named optaplanner-quickstart. Apr 26 09:40:14 be44b3acb1f3 java[934]: __ ____ __ _____ ___ __ ____ ______ Apr 26 09:40:14 be44b3acb1f3 java[934]: --/ __ \/ / / / _ | / _ \/ //_/ / / / __/ Apr 26 09:40:14 be44b3acb1f3 java[934]: -/ /_/ / /_/ / __ |/ , _/ ,&lt; / /_/ /\ \ Apr 26 09:40:14 be44b3acb1f3 java[934]: --\___\_\____/_/ |_/_/|_/_/|_|\____/___/ Apr 26 09:40:14 be44b3acb1f3 java[934]: 2023-04-26 09:40:14,843 INFO [io.quarkus] (main) optaplanner-quickstart 1.0.0-SNAPSHOT on JVM (powered by Quarkus 2.16.6.Final) started in 1.468s. Listening on: http://0.0.0.0:8080 Apr 26 09:40:14 be44b3acb1f3 java[934]: 2023-04-26 09:40:14,848 INFO [io.quarkus] (main) Profile prod activated. Apr 26 09:40:14 be44b3acb1f3 java[934]: 2023-04-26 09:40:14,848 INFO [io.quarkus] (main) Installed features: [agroal, cdi, hibernate-orm, hibernate-orm-panache, hibernate-orm-rest-data-panache, jdbc-h2, narayana-jta, optaplanner, optaplanner-jackson, resteasy-reactive, resteasy-reactive-jackson, resteasy-reactive-links, smallrye-context-propagation, vertx, webjars-locator] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Having the service running is certainly good, but it does not guarantee by itself that the application is available. To double-check, we can simply confirm the accessibility of the application by connecting to it:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# curl -I http://localhost:8080/ HTTP/1.1 200 OK accept-ranges: bytes content-length: 8533 cache-control: public, immutable, max-age=86400 last-modified: Wed, 26 Apr 2023 10:00:18 GMT date: Wed, 26 Apr 2023 13:00:19 GMT&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Writing up a playbook&lt;/h2&gt; &lt;p&gt;The default playbook provided with the Ansible collection for Quarkus is quite handy and allows you to bootstrap your automation with a single command. However, most likely, you’ll need to write your own playbook so you can add automation required around the deployment of your Quarkus app.&lt;/p&gt; &lt;p&gt;Here is the content of the playbook provided with the collection that you can simply use as a base for your own:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;--- - name: "Build and deploy a Quarkus app using Ansible" hosts: all gather_facts: false vars: quarkus_app_repo_url: 'https://github.com/quarkusio/quarkus-quickstarts.git' app_name: optaplanner-quickstart' quarkus_app_source_folder: 'optaplanner-quickstart' quarkus_path_to_folder_to_deploy: '/opt/optaplanner' pre_tasks: - name: "Build the Quarkus from {{ quarkus_app_repo_url }}." ansible.builtin.include_role: name: quarkus tasks_from: build.yml tasks: - name: "Deploy Quarkus app on target." ansible.builtin.include_role: name: quarkus tasks_from: deploy.yml &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To run this playbook, you again use the &lt;code&gt;ansible-playbook &lt;/code&gt;command, but providing the path to the playbook:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ansible-playbook -i inventory playbook.yml&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Thanks to the Ansible collection for Quarkus, the work needed to automate the deployment of a Quarkus application is minimal. The collection takes care of most of the heavy lifting and allows its user to focus on the automation needs specific to their application and business needs.&lt;/p&gt; &lt;p&gt;Explore other Ansible tutorials on Red Hat Developer:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2023/04/04/update-and-upgrade-jboss-eap-ansible"&gt;Update and upgrade JBoss EAP with Ansible&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2023/02/20/automate-your-sso-ansible-and-keycloak"&gt;Automate your SSO with Ansible and Keycloak&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/12/22/automate-jboss-web-server-deployment-red-hat-certified-content-collection-jws"&gt;Automate JBoss Web Server deployment with the Red Hat Certified Content Collection for JWS&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/04/20/deploy-keycloak-single-sign-ansible"&gt;Deploy Keycloak single sign-on with Ansible&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/03/21/deploy-infinispan-automatically-ansible"&gt;Deploy Infinispan automatically with Ansible&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/02/08/automate-and-deploy-jboss-eap-cluster-ansible"&gt;Automate and deploy a JBoss EAP cluster with Ansible&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/09/28/set-modcluster-red-hat-jboss-web-server-ansible"&gt;Set up mod_cluster for Red Hat JBoss Web Server with Ansible&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/06/01/automate-your-quarkus-deployment-using-ansible" title="Automate your Quarkus deployment using Ansible"&gt;Automate your Quarkus deployment using Ansible&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Romain Pelisse</dc:creator><dc:date>2023-06-01T07:00:00Z</dc:date></entry><entry><title type="html">This Week in JBoss - June, 1st 2023</title><link rel="alternate" href="https://www.jboss.org/posts/weekly-2023-06-01.html" /><category term="quarkus" /><category term="java" /><category term="jee" /><category term="micro-profile" /><category term="wildfly" /><category term="ansible" /><category term="kogito" /><category term="keycloak" /><author><name>Romain Pelisse 2023-06-01</name><uri>https://www.jboss.org/people/romain-pelisse 2023-06-01</uri><email>do-not-reply@jboss.com</email></author><id>https://www.jboss.org/posts/weekly-2023-06-01.html</id><updated>2023-06-01T00:00:00Z</updated><content type="html">&lt;article class="" data-tags="quarkus, java, jee, micro-profile, wildfly, ansible, kogito, keycloak"&gt; &lt;h1&gt;This Week in JBoss - June, 1st 2023&lt;/h1&gt; &lt;p class="preamble"&gt;&lt;/p&gt;&lt;p&gt;First and foremost! Dear readers, we want our opinion about the content of the editorial. We know filling up poll can be annoying, so you made as simple as possible…​ Please answer our &lt;strong&gt;one question only&lt;/strong&gt; &lt;a href="https://framadate.org/XbAltuQw4kQDY9At"&gt;poll on our editorial&lt;/a&gt;! Thanks!&lt;/p&gt; &lt;p&gt;Two of the biggest stars of the JBoss ecosystem are for sure Quarkus and, of course, Wildfly, the application server that used to be called, well, JBoss! The last weeks have seen a lot of interesting content and news around those two projects, so we are going to focus this editorial on them. Buckle up, there is a ton of passionating stuff coming your way!&lt;/p&gt;&lt;p&gt;&lt;/p&gt; &lt;div class="sect1"&gt; &lt;h2 id="_quarkus"&gt;Quarkus&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;The last two weeks have seen again quite a few interesting news, and content released, for the "Supersonic Subatomic Java" framework Quarkus! On top on the &lt;a href="https://quarkus.io/blog/quarkus-3-1-0-final-released/"&gt;3.1.0.Final release&lt;/a&gt; and the &lt;a href="https://quarkus.io/blog/quarkus-3-0-4-final-released/"&gt;3.0.4.Final release&lt;/a&gt; a new guide to migrate to those versions have been published: &lt;a href="https://quarkus.io/blog/quarkus-3-upgrade/"&gt;Migration to Quarkus 3.0 is a breeze&lt;/a&gt;! No reason to stay behind now, jump on board of Quarkus 3!&lt;/p&gt; &lt;p&gt;I’m also happy to mention that my guide on &lt;a href="https://quarkus.io/guides/ansible"&gt;Automate Quarkus deployment with Ansible&lt;/a&gt; has been added to the website. Hope it helps Quarkus user who wants to automate their deployment!&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_be_the_boss_of_wildfly"&gt;Be The Boss of Wildfly&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;In the unlikely case, you have never heard of this website, Master The Boss, that has been around since forever, you have a chance to discover today. It has been publishing content on JBoss for over a decade and the articles published in the last week are pretty example of the site’s richness.&lt;/p&gt; &lt;p&gt;First one we wanted to mention is &lt;a href="https://www.mastertheboss.com/keycloak/google-social-login-with-keycloak/"&gt;KeyCloak Social Login Step-by-Step guide&lt;/a&gt;. Keycloak is popular SSO software and social login is certainly worth a mention. Then come two articles more focused on Wildfly. The first one is &lt;a href="https://www.mastertheboss.com/eclipse/eclipse-microservices/microprofile-lra-a-comprehensive-guide/"&gt;MicroProfile LRA: A Comprehensive Guide&lt;/a&gt; and the second is &lt;a href="https://www.mastertheboss.com/eclipse/jboss-tools/using-visual-studio-to-develop-and-manage-wildfly/"&gt;Using Visual Studio to develop and manage WildFly&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_release_roundup"&gt;Release roundup&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;As always the JBoss world is a thriving place thus the last got her fair share of releases:&lt;/p&gt; &lt;div class="ulist square"&gt; &lt;ul class="square"&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/quarkus-3-1-0-final-released/"&gt;Quarkus 3.1.0.Final released&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/quarkus-3-0-4-final-released/"&gt;Quarkus 3.0.4.Final&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.wildfly.org//news/2023/05/23/WildFly2801-Released/"&gt;WildFly 28.0.1 is released!&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://blog.kie.org/2023/05/kogito-1-38-0-released.html"&gt;KOGITO 1.38.0 is released!&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_decaf"&gt;Decaf&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Enough java? Too much jitters? Wants to peek outside the coffee cup for a sec? The decaf’s section is here for you!&lt;/p&gt; &lt;p&gt;To stay on today’s theme, we wanted to mention that my guide on &lt;a href="https://quarkus.io/guides/ansible"&gt;Automate Quarkus deployment with Ansible&lt;/a&gt; has been added to the project’s website. I hope it helps Quarkus user who wants to automate their deployment! And note that there is also a &lt;a href="https://www.wildfly.org/news/2023/01/10/ansible-wildfly/"&gt;Ansible collection (extension) dedicated to Wildfly&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;A last note on Ansible, that may interest people of the community. There is a very cool new project called &lt;a href="https://www.ansible.com/blog/getting-started-with-event-driven-ansible"&gt;Event Driven Ansible&lt;/a&gt; using, behind the curtains, another cool project of the JBoss community: &lt;a href="https://www.drools.org/"&gt;Drools&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;&lt;em&gt;That’s all folks! Please join us again in two weeks for another round of our JBoss editorial!&lt;/em&gt;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="author"&gt; &lt;pfe-avatar pfe-shape="circle" pfe-pattern="squares" pfe-src="/img/people/romain-pelisse 2023-06-01.png"&gt;&lt;/pfe-avatar&gt; &lt;span&gt;Romain Pelisse 2023-06-01&lt;/span&gt; &lt;/div&gt;&lt;/article&gt;</content><dc:creator>Romain Pelisse 2023-06-01</dc:creator></entry><entry><title>Improvements to static analysis in the GCC 13 compiler</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/05/31/improvements-static-analysis-gcc-13-compiler" /><author><name>David Malcolm</name></author><id>9fcab65e-5911-46ef-862f-4e4d1c3b4504</id><updated>2023-05-31T07:00:00Z</updated><published>2023-05-31T07:00:00Z</published><summary type="html">&lt;p&gt;I work at Red Hat on &lt;a href="https://gcc.gnu.org/"&gt;GCC, the GNU Compiler Collection&lt;/a&gt;. For the last four releases of GCC, I've been working on &lt;code&gt;-fanalyzer&lt;/code&gt;, a static analysis pass that tries to identify various problems at compile-time, rather than at runtime. It performs "symbolic execution" of &lt;a href="https://developers.redhat.com/topics/c"&gt;C&lt;/a&gt; source code—effectively simulating the behavior of the code along the various possible paths of execution through it (with some caveats that we'll discuss).&lt;/p&gt; &lt;p&gt;This article summarizes what's new with &lt;code&gt;-fanalyzer&lt;/code&gt; in &lt;a href="https://gcc.gnu.org/gcc-13/changes.html"&gt;GCC 13&lt;/a&gt;, which has just been released.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;[ Learn more: &lt;a href="https://developers.redhat.com/articles/2023/05/04/new-c-features-gcc-13" target="_blank"&gt;New C features in GCC 13&lt;/a&gt; ] &lt;/strong&gt;&lt;/p&gt; &lt;h2&gt;New warnings&lt;/h2&gt; &lt;p&gt;I first added the analyzer to GCC in &lt;a href="https://developers.redhat.com/blog/2020/03/26/static-analysis-in-gcc-10/"&gt;GCC 10&lt;/a&gt;, with 15 new warnings for the compiler, and we've added more in each subsequent release (Table 1).&lt;/p&gt; &lt;table border="1" cellpadding="1" cellspacing="1" width="500"&gt;&lt;caption&gt; &lt;p class="text-align-left"&gt;Table 1: GCC warnings controlled by &lt;code&gt;-fanalyzer&lt;/code&gt; by release&lt;/p&gt; &lt;/caption&gt; &lt;thead&gt;&lt;tr&gt;&lt;th class="text-align-center" scope="row"&gt;Release&lt;/th&gt; &lt;th class="text-align-center" scope="col"&gt;New warnings&lt;/th&gt; &lt;th class="text-align-center" scope="col"&gt;Cumulative warnings&lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;th class="text-align-center" scope="row"&gt;&lt;a href="https://developers.redhat.com/blog/2020/03/26/static-analysis-in-gcc-10"&gt;GCC 10&lt;/a&gt;&lt;/th&gt; &lt;td class="text-align-center"&gt;15&lt;/td&gt; &lt;td class="text-align-center"&gt;15&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;th class="text-align-center" scope="row"&gt;&lt;a href="https://developers.redhat.com/blog/2021/01/28/static-analysis-updates-in-gcc-11"&gt;GCC 11&lt;/a&gt;&lt;/th&gt; &lt;td class="text-align-center"&gt;7&lt;/td&gt; &lt;td class="text-align-center"&gt;22&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;th class="text-align-center" scope="row"&gt;&lt;a href="https://developers.redhat.com/articles/2022/04/12/state-static-analysis-gcc-12-compiler#"&gt;GCC 12&lt;/a&gt;&lt;/th&gt; &lt;td class="text-align-center"&gt;5&lt;/td&gt; &lt;td class="text-align-center"&gt;27&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;th class="text-align-center" scope="row"&gt;&lt;a href="https://gcc.gnu.org/gcc-13/changes.html"&gt;GCC 13&lt;/a&gt;&lt;/th&gt; &lt;td class="text-align-center"&gt;20&lt;/td&gt; &lt;td class="text-align-center"&gt;47&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;As you can see in Table 1, GCC 13 is a big release for &lt;code&gt;-fanalyzer&lt;/code&gt;, adding 20 new warnings. Let's take a look at some of them.&lt;/p&gt; &lt;h3&gt;Track dynamic buffer size&lt;/h3&gt; &lt;p&gt;Can you spot the bug in the following C code?&lt;/p&gt; &lt;pre&gt; &lt;code class="language-c"&gt;#include &lt;stdlib.h&gt; #include &lt;string.h&gt; struct str { size_t len; char data[]; }; struct str * make_str_badly (const char *src) { size_t len = strlen(src); struct str *str = malloc(sizeof(str) + len); if (!str) return NULL; str-&gt;len = len; memcpy(str-&gt;data, src, len); str-&gt;data[len] = '\0'; return str; } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;span&gt;The above example &lt;/span&gt;makes the common mistake with C-style strings of forgetting the null terminator when computing how much space to allocate for &lt;code&gt;str&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;GCC 13's &lt;code&gt;-fanalyzer&lt;/code&gt; option now keeps track of the sizes of dynamically allocated buffers, and for many cases it checks the simulated memory reads and writes against the sizes of the relevant buffers. With this new work it detects the above problem by &lt;a href="https://godbolt.org/z/Y3v3c35zY"&gt;emitting this new warning&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;&lt;source&gt;: In function 'make_str_badly': &lt;source&gt;:18:18: warning: heap-based buffer overflow [CWE-122] [-Wanalyzer-out-of-bounds] 18 | str-&gt;data[len] = '\0'; | ~~~~~~~~~~~~~~~^~~~~~ 'make_str_badly': events 1-4 | | 13 | struct str *str = malloc(sizeof(str) + len); | | ^~~~~~~~~~~~~~~~~~~~~~~~~ | | | | | (1) capacity: 'len + 8' bytes | 14 | if (!str) | | ~ | | | | | (2) following 'false' branch (when 'str' is non-NULL)... | 15 | return NULL; | 16 | str-&gt;len = len; | | ~~~~~~~~~~~~~~ | | | | | (3) ...to here | 17 | memcpy(str-&gt;data, src, len); | 18 | str-&gt;data[len] = '\0'; | | ~~~~~~~~~~~~~~~~~~~~~ | | | | | (4) write of 1 byte at offset 'len + 8' exceeds the buffer |&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I want to thank &lt;a href="https://tim-lange.me/gsoc/"&gt;Tim Lange&lt;/a&gt; who implemented this warning as part of Google's Summer of Code program last year (along with two other new warnings: &lt;a href="https://gcc.gnu.org/onlinedocs/gcc-13.1.0/gcc/Static-Analyzer-Options.html#index-Wanalyzer-allocation-size"&gt;&lt;code&gt;-Wanalyzer-allocation-size&lt;/code&gt;&lt;/a&gt; and &lt;a href="https://gcc.gnu.org/onlinedocs/gcc-13.1.0/gcc/Static-Analyzer-Options.html#index-Wanalyzer-imprecise-fp-arithmetic"&gt;&lt;code&gt;-Wanalyzer-imprecise-fp-arithmetic&lt;/code&gt;&lt;/a&gt;).&lt;/p&gt; &lt;h3&gt;Check if NULL is dereferenced&lt;/h3&gt; &lt;p&gt;Here's an example of another new warning—what's wrong with the following C code?&lt;/p&gt; &lt;pre&gt; &lt;code class="language-c"&gt;#include &lt;assert.h&gt; #include &lt;stdio.h&gt; extern FILE *logfile; struct obj { const char *name; int x; int y; }; int is_within_boundary (struct obj *p, int radius_squared) { fprintf (logfile, "%s: (%i, %i)\n", p-&gt;name, p-&gt;x, p-&gt;y); if (!p) return 0; return (p-&gt;x * p-&gt;x) + (p-&gt;y * p-&gt;y) &lt; radius_squared; } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The issue is that the code is unclear about whether &lt;code&gt;p&lt;/code&gt; can be &lt;code&gt;NULL&lt;/code&gt;: it's dereferenced unconditionally at the &lt;code&gt;fprintf&lt;/code&gt; call, but then checked for &lt;code&gt;NULL&lt;/code&gt; later on. A pointer that's unconditionally dereferenced can be assumed by a compiler to be non-&lt;code&gt;NULL&lt;/code&gt;, and thus the check against &lt;code&gt;NULL&lt;/code&gt; can potentially be optimized away, which is probably not want you want—but the compiler has no way to know what you meant.&lt;/p&gt; &lt;p&gt;As of GCC 13, the &lt;code&gt;-fanalyzer&lt;/code&gt; option now detects the above by &lt;a href="https://godbolt.org/z/7G35YxoP3"&gt;emitting this warning&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;&lt;source&gt;: In function 'is_within_boundary': &lt;source&gt;:16:6: warning: check of 'p' for NULL after already dereferencing it [-Wanalyzer-deref-before-check] 16 | if (!p) | ^ 'is_within_boundary': events 1-2 | | 15 | fprintf (logfile, "%s: (%i, %i)\n", p-&gt;name, p-&gt;x, p-&gt;y); | | ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ | | | | | (1) pointer 'p' is dereferenced here | 16 | if (!p) | | ~ | | | | | (2) pointer 'p' is checked for NULL here but it was already dereferenced at (1) | &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Other new warnings&lt;/h3&gt; &lt;p&gt;I don't have space in this article to give examples of every new warning added in GCC 13, but here's a round-up of the others.&lt;/p&gt; &lt;p&gt;I added support to &lt;code&gt;-fanalyzer&lt;/code&gt; for tracking the state of &lt;code&gt;&lt;stdarg.h&gt;&lt;/code&gt;:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://gcc.gnu.org/onlinedocs/gcc-13.1.0/gcc/Static-Analyzer-Options.html#index-Wanalyzer-va-list-leak"&gt;&lt;code&gt;-Wanalyzer-va-list-leak&lt;/code&gt;&lt;/a&gt; for complaining about missing &lt;code&gt;va_end&lt;/code&gt; after a &lt;code&gt;va_start&lt;/code&gt; or &lt;code&gt;va_copy&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://gcc.gnu.org/onlinedocs/gcc-13.1.0/gcc/Static-Analyzer-Options.html#index-Wanalyzer-va-list-use-after-va-end"&gt;&lt;code&gt;-Wanalyzer-va-list-use-after-va-end&lt;/code&gt;&lt;/a&gt; for complaining about &lt;code&gt;va_arg&lt;/code&gt; or &lt;code&gt;va_copy&lt;/code&gt; used on a &lt;code&gt;va_list&lt;/code&gt; that's had &lt;code&gt;va_end&lt;/code&gt; called on it&lt;/li&gt; &lt;li&gt;&lt;a href="https://gcc.gnu.org/onlinedocs/gcc-13.1.0/gcc/Static-Analyzer-Options.html#index-Wanalyzer-va-arg-type-mismatch"&gt;&lt;code&gt;-Wanalyzer-va-arg-type-mismatch&lt;/code&gt;&lt;/a&gt; for type-checking of &lt;code&gt;va_arg&lt;/code&gt; usage in interprocedural execution paths against the types of the parameters that were actually passed to the variadic call&lt;/li&gt; &lt;li&gt;&lt;a href="https://gcc.gnu.org/onlinedocs/gcc-13.1.0/gcc/Static-Analyzer-Options.html#index-Wanalyzer-va-list-exhausted"&gt;&lt;code&gt;-Wanalyzer-va-list-exhausted&lt;/code&gt;&lt;/a&gt; for complaining in interprocedural execution paths if &lt;code&gt;va_arg&lt;/code&gt; is used too many times on a &lt;code&gt;va_list&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;&lt;span dir="ltr"&gt;&lt;a href="https://gist.github.com/mirimmad/9524fa6ada8dda5436447dcc4cfc86f0"&gt;Immad Mir&lt;/a&gt; implemented tracking of file descriptors within the analyzer as part of Google Summer of Code 2022. We added seven new warnings relating to this in GCC 13:&lt;/span&gt;&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://gcc.gnu.org/onlinedocs/gcc-13.1.0/gcc/Static-Analyzer-Options.html#index-Wanalyzer-fd-access-mode-mismatch"&gt;&lt;code&gt;-Wanalyzer-fd-access-mode-mismatch&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://gcc.gnu.org/onlinedocs/gcc-13.1.0/gcc/Static-Analyzer-Options.html#index-Wanalyzer-fd-double-close"&gt;&lt;code&gt;-Wanalyzer-fd-double-close&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://gcc.gnu.org/onlinedocs/gcc-13.1.0/gcc/Static-Analyzer-Options.html#index-Wanalyzer-fd-leak"&gt;&lt;code&gt;-Wanalyzer-fd-leak&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://gcc.gnu.org/onlinedocs/gcc-13.1.0/gcc/Static-Analyzer-Options.html#index-Wanalyzer-fd-phase-mismatch"&gt;&lt;code&gt;-Wanalyzer-fd-phase-mismatch&lt;/code&gt;&lt;/a&gt; (e.g. calling &lt;code&gt;accept&lt;/code&gt; on a socket before calling &lt;code&gt;listen&lt;/code&gt; on it)&lt;/li&gt; &lt;li&gt;&lt;a href="https://gcc.gnu.org/onlinedocs/gcc-13.1.0/gcc/Static-Analyzer-Options.html#index-Wanalyzer-fd-type-mismatch"&gt;&lt;code&gt;-Wanalyzer-fd-type-mismatch&lt;/code&gt;&lt;/a&gt; (e.g. using a stream socket operation on a datagram socket)&lt;/li&gt; &lt;li&gt;&lt;a href="https://gcc.gnu.org/onlinedocs/gcc-13.1.0/gcc/Static-Analyzer-Options.html#index-Wanalyzer-fd-use-after-close"&gt;&lt;code&gt;-Wanalyzer-fd-use-after-close&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://gcc.gnu.org/onlinedocs/gcc-13.1.0/gcc/Static-Analyzer-Options.html#index-Wanalyzer-fd-use-without-check"&gt;&lt;code&gt;-Wanalyzer-fd-use-without-check&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;along with &lt;a href="https://gcc.gnu.org/onlinedocs/gcc-13.1.0/gcc/Common-Function-Attributes.html#index-fd_005farg-function-attribute"&gt;attributes for marking &lt;code&gt;int&lt;/code&gt; function arguments as being file descriptors&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Finally, I implemented various other warnings:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://gcc.gnu.org/onlinedocs/gcc-13.1.0/gcc/Static-Analyzer-Options.html#index-Wanalyzer-exposure-through-uninit-copy"&gt;&lt;code&gt;-Wanalyzer-exposure-through-uninit-copy&lt;/code&gt;&lt;/a&gt; (for detecting "infoleaks" in the Linux kernel)&lt;/li&gt; &lt;li&gt;&lt;a href="https://gcc.gnu.org/onlinedocs/gcc-13.1.0/gcc/Static-Analyzer-Options.html#index-Wanalyzer-infinite-recursion"&gt;&lt;code&gt;-Wanalyzer-infinite-recursion&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://gcc.gnu.org/onlinedocs/gcc-13.1.0/gcc/Static-Analyzer-Options.html#index-Wanalyzer-jump-through-null"&gt;&lt;code&gt;-Wanalyzer-jump-through-null&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://gcc.gnu.org/onlinedocs/gcc-13.1.0/gcc/Static-Analyzer-Options.html#index-Wanalyzer-putenv-of-auto-var"&gt;&lt;code&gt;-Wanalyzer-putenv-of-auto-var&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://gcc.gnu.org/onlinedocs/gcc-13.1.0/gcc/Static-Analyzer-Options.html#index-Wanalyzer-tainted-assertion"&gt;&lt;code&gt;-Wanalyzer-tainted-assertion&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;SARIF output&lt;/h2&gt; &lt;p&gt;In GCC 9 I added an option &lt;code&gt;-fdiagnostics-format=json&lt;/code&gt; &lt;span dir="ltr"&gt;to provide &lt;a href="https://developers.redhat.com/blog/2019/03/08/usability-improvements-in-gcc-9#not_just_for_humans"&gt;machine-readable output for GCC's diagnostics&lt;/a&gt;. This is a custom JSON-based format that closely follows GCC's own internal representation.&lt;/span&gt;&lt;/p&gt; &lt;p&gt;In the meantime, another JSON-based format has emerged as the standard in this space: &lt;a href="https://sarifweb.azurewebsites.net/"&gt;SARIF (the Static Analysis Results Interchange Format)&lt;/a&gt;. This file format is suited for capturing the results of static analysis tools (like GCC's &lt;code&gt;-fanalyzer&lt;/code&gt;), but it can also be used for plain GCC warnings and errors.&lt;/p&gt; &lt;p&gt;So for GCC 13 I've extended &lt;code&gt;-fdiagnostics-format=&lt;/code&gt; to add two new options implementing SARIF support: &lt;code&gt;-fdiagnostics-format=sarif-stderr&lt;/code&gt; and &lt;code&gt;-fdiagnostics-format=sarif-file&lt;/code&gt;. I've also joined the &lt;a href="https://www.oasis-open.org/committees/tc_home.php?wg_abbrev=sarif"&gt;technical committee&lt;/a&gt; overseeing the standard.&lt;/p&gt; &lt;p&gt;By producing data in an industry standard format we benefit from interoperability with existing consumers of SARIF data. Figure 1 is a simple example, showing VS Code (with a SARIF plugin) viewing a SARIF file generated by GCC. The IDE is able to annotate the source code, adding squiggly lines under code where GCC finds problems. Here I've clicked on a line where &lt;code&gt;-fanalyzer&lt;/code&gt; reported a double-free bug, and the IDE is showing the path of execution through the code that GCC predicted will trigger the problem.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/2022-06-06-vscode-showing-gcc-sarif-output.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/2022-06-06-vscode-showing-gcc-sarif-output.png?itok=Rg5L9WZ5" width="881" height="689" alt="Screenshot of VS Code showing GCC SARIF output" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: GCC SARIF output in VS Code.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Fixing false positives&lt;/h2&gt; &lt;p&gt;Static analyzers are not perfect—it's impossible to reason perfectly about the most interesting properties of source code. The GCC analyzer performs a crude simulation of the state of the inside of the program, and I've made many tradeoffs to try to make it fast enough to use when working on code. I receive anecdotal reports that people are using it and it's finding bugs for them earlier than they would have found them otherwise, but there will be false positives and false negatives. The analyzer is a bug-finding tool, rather than a tool for proving program correctness (and, alas, sometimes bugs lead to it being too slow). In technical terms, it's neither "sound" nor "complete." &lt;/p&gt; &lt;p&gt;I've spent the first few months of this year trying to reduce "spam" from the analyzer for GCC 13. I created an &lt;a href="https://github.com/davidmalcolm/gcc-analyzer-integration-tests"&gt;integration testing suite&lt;/a&gt;: I picked various real-world C projects, including Doom, the &lt;a href="https://developers.redhat.com/topics/linux/"&gt;Linux&lt;/a&gt; kernel, and qemu. I've been building them with their standard options, but with &lt;code&gt;-fanalyzer&lt;/code&gt; added to the build flags, examining the warnings emitted, and trying to fix the false positives.&lt;/p&gt; &lt;p&gt;I made a lot of fixes to the analyzer; Table 2 shows some before and after numbers for the warnings that were most improved by this work, where FP means a "false positive" (a bogus warning about a non-problem) and TP means a "true positive" (a valid warning about a real problem in the source code).&lt;/p&gt; &lt;table border="1" cellpadding="1" cellspacing="1" width="645"&gt;&lt;caption&gt; &lt;p class="text-align-left"&gt;Table 2: Improved warnings.&lt;/p&gt; &lt;/caption&gt; &lt;thead&gt;&lt;tr&gt;&lt;th class="text-align-center"&gt;Warning&lt;/th&gt; &lt;th class="text-align-center"&gt; &lt;p&gt;FP&lt;/p&gt; &lt;p&gt;before&lt;/p&gt; &lt;/th&gt; &lt;th class="text-align-center"&gt; &lt;p&gt;FP&lt;/p&gt; &lt;p&gt;after&lt;/p&gt; &lt;/th&gt; &lt;th class="text-align-center"&gt; &lt;p&gt;TP&lt;/p&gt; &lt;p&gt;before&lt;/p&gt; &lt;/th&gt; &lt;th class="text-align-center"&gt; &lt;p&gt;TP&lt;/p&gt; &lt;p&gt;after&lt;/p&gt; &lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;code&gt;-Wanalyzer-deref-before-check&lt;/code&gt;&lt;/td&gt; &lt;td class="text-align-right"&gt;63&lt;/td&gt; &lt;td class="text-align-right"&gt;12&lt;/td&gt; &lt;td class="text-align-right"&gt;1&lt;/td&gt; &lt;td class="text-align-right"&gt;1&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;code&gt;-Wanalyzer-malloc-leak&lt;/code&gt;&lt;/td&gt; &lt;td class="text-align-right"&gt;78&lt;/td&gt; &lt;td class="text-align-right"&gt;50&lt;/td&gt; &lt;td class="text-align-right"&gt;0&lt;/td&gt; &lt;td class="text-align-right"&gt;61&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;code&gt;-Wanalyzer-use-of-uninitialized-value&lt;/code&gt;&lt;/td&gt; &lt;td class="text-align-right"&gt;998&lt;/td&gt; &lt;td class="text-align-right"&gt;125&lt;/td&gt; &lt;td class="text-align-right"&gt;0&lt;/td&gt; &lt;td class="text-align-right"&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;You can see that I eliminated most (but not all) of the false positives from &lt;code&gt;-Wanalyzer-deref-before-check&lt;/code&gt; , and that I reduced the number of FPs from &lt;code&gt;-Wanalyzer-malloc-leak&lt;/code&gt; whilst fixing it so that it correctly detected a bunch of real memory leaks that it had previously missed (in Doom's initialization logic, as it happens). Unfortunately, &lt;code&gt;-Wanalyzer-use-of-uninitialized-value&lt;/code&gt; is still the "spammiest" warning, despite me making a big dent in its number of FPs; it seems to be most prone to exploring paths through the code that can't happen in practice, where the analyzer doesn't have enough high-level information about invariants in the code to figure that out.&lt;/p&gt; &lt;h2&gt;Trying it out&lt;/h2&gt; &lt;p&gt;GCC 13 has been released upstream, and is the system compiler in the &lt;a href="https://fedoramagazine.org/announcing-fedora-38/"&gt;recently-released Fedora 38&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;For simple C examples, you can play around with the new GCC online at the &lt;a href="https://godbolt.org/"&gt;Compiler Explorer site&lt;/a&gt;. Select GCC 13.1 and add &lt;code&gt;-fanalyzer&lt;/code&gt; to the compiler options to run static analysis.&lt;/p&gt; &lt;p&gt;As noted above, the analyzer isn't perfect, but I hope it's helpful. Given that every compiler and analyzer finds a slightly different subset of bugs it's usually a good idea to run your code through more than one toolchain to see what shakes out.&lt;/p&gt; &lt;p&gt;Finally, if you're interested in getting involved in compiler development, I've written a &lt;a href="https://gcc-newbies-guide.readthedocs.io/en/latest/"&gt;guide to getting started as a GCC contributor&lt;/a&gt;. It includes lots of ideas for new warnings and features in GCC's Bugzilla.&lt;/p&gt; &lt;p&gt;Have fun!&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/05/31/improvements-static-analysis-gcc-13-compiler" title="Improvements to static analysis in the GCC 13 compiler"&gt;Improvements to static analysis in the GCC 13 compiler&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>David Malcolm</dc:creator><dc:date>2023-05-31T07:00:00Z</dc:date></entry><entry><title type="html">KeyCloak Social Login Step-by-Step guide</title><link rel="alternate" href="https://www.mastertheboss.com/keycloak/google-social-login-with-keycloak/" /><author><name>F.Marchioni</name></author><id>https://www.mastertheboss.com/keycloak/google-social-login-with-keycloak/</id><updated>2023-05-31T06:25:35Z</updated><content type="html">Are you looking to enhance your application’s user authentication process? Configuring social login in Keycloak can provide a seamless and convenient login experience for your users. In this comprehensive tutorial, we will guide you through the step-by-step process of setting up social login in Keycloak, leveraging Google Identity Provider as an example. So let’s get ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">Quarkus 3.1.0.Final released - Programmatic creation of Reactive REST Clients, Kotlin 1.8.21 and more</title><link rel="alternate" href="https://quarkus.io/blog/quarkus-3-1-0-final-released/" /><author><name>Guillaume Smet</name></author><id>https://quarkus.io/blog/quarkus-3-1-0-final-released/</id><updated>2023-05-31T00:00:00Z</updated><content type="html">It has been a month since we released Quarkus 3.0 and it is our pleasure to announce Quarkus 3.1.0.Final. As usual, it comes with a lot of improvements all over the place. Major changes are: Provide new API to programmatically create Reactive REST Clients Introduce a way to set headers...</content><dc:creator>Guillaume Smet</dc:creator></entry><entry><title>Why use RHEL for SAP Solutions?</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/05/30/why-use-rhel-sap-solutions" /><author><name>Nikhil Mungale</name></author><id>8ef5986f-2a99-48a5-afe1-33e4654381e5</id><updated>2023-05-30T13:30:00Z</updated><published>2023-05-30T13:30:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/topics/linux/"&gt;Red Hat Enterprise Linux&lt;/a&gt; &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;(RHEL) for SAP Solutions is a secure and scalable platform tailored to the needs of SAP workloads, such as SAP NetWeaver, SAP S/4HANA, and the SAP HANA platform. As SAP Business Suite will be out of standard support by 2027, it is essential for developers and administrators to plan for a smooth migration to the SAP S/4 HANA platform. &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;In this article, we'll explore what RHEL for SAP Solutions is and the features and functionalities it provides to developers and administrators.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;h2&gt;What is Red Hat Enterprise Linux for SAP Solutions?&lt;/h2&gt; &lt;p&gt;Red Hat Enterprise Linux for SAP Solutions is tailored for SAP workloads such as the SAP HANA platform and S/4HANA. The SAP environment can be standardized on Red Hat Enterprise Linux. &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;By standardizing the SAP environment on RHEL, businesses can streamline operations, reduce costs, and benefit from integrated smart management and high-availability solutions included in the offering&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. &lt;/p&gt; &lt;h2&gt;What's included in the RHEL for SAP Solutions subscription?&lt;/h2&gt; &lt;p&gt;Built on the foundation of Red Hat Enterprise Linux (RHEL) for SAP Applications along with its packages and components,  a RHEL for SAP Solutions subscription includes the following:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;SAP-specific technical components to support SAP S/4HANA, SAP HANA, and SAP Business Applications.&lt;/li&gt; &lt;li aria-level="1"&gt;SAP-specific high availability solutions for SAP S/4HANA, SAP HANA, and SAP Business Applications.&lt;/li&gt; &lt;li aria-level="1"&gt;RHEL System Roles for SAP for automation of operating system configuration to run SAP workloads.&lt;/li&gt; &lt;li aria-level="1"&gt;SAP-tailored Red Hat Insights Dashboard and Smart Management help streamline operations and reduce costs&lt;/li&gt; &lt;li aria-level="1"&gt;Update Services for SAP Solutions or Extended Update Support (EUS), providing support for specific minor RHEL releases for up to four years from General Availability. See the &lt;a href="https://access.redhat.com/support/policy/updates/errata/"&gt;RHEL Life Cycle&lt;/a&gt; web page for more information about EUS and Update Services for SAP Solutions.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;All the packages provided by RHEL for SAP Applications are also provided by RHEL for SAP Solutions. &lt;/p&gt; &lt;p&gt;Learn more about the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_for_sap_solutions/9/html/overview_of_red_hat_enterprise_linux_for_sap_solutions_subscription/index"&gt;RHEL for SAP solutions subscription&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;What are the benefits of RHEL for SAP Solutions?&lt;/h2&gt; &lt;p&gt;Because the SAP Business Suite is going out of standard support by 2027, existing applications on SAP Business Suite need to migrate to S/4HANA, which will be available only on x86/Power and &lt;a href="https://developers.redhat.com/topics/linux/"&gt;Linux&lt;/a&gt;. RHEL provides migration support for such cases, ensuring a smooth and hassle-free transition. This benefit for businesses is that they can seamlessly migrate their SAP solutions to RHEL without disrupting business operations. &lt;/p&gt; &lt;p&gt;With Red Hat Insights included in the subscription, RHEL for SAP Solutions provides technologies that prioritize recommendations based on deep analysis with targeted expertise. This allows organizations to evaluate all of their ecosystems, including hybrid cloud environments, at no extra cost.&lt;/p&gt; &lt;p&gt;RHEL also provides powerful analytical tools for monitoring SAP applications and solutions, ensuring optimal performance and stability. With these tools, administrators can quickly identify and troubleshoot issues, reducing the risk of downtime and improving the system's overall performance. &lt;a href="https://developers.redhat.com/products/rhel/download#rhel3ways"&gt;Download RHEL for SAP solutions packages&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Migrate to SAP S/4 HANA and SAP HANA with RHEL&lt;/h2&gt; &lt;p&gt;Migration to SAP S/4 HANA can be complex and costly. Administrators must deal with outdated, legacy hardware, and cases where migration may be challenging. They need to have a flexible and secure platform that provides evolving software technologies and has the ability to port applications seamlessly.  &lt;/p&gt; &lt;p&gt;Red Hat solutions for SAP workloads unify administration and management, &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;enabling SAP customers to avoid using disparate toolsets from different vendors to manage their virtual infrastructure, high availability cluster, and operating system&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;.&lt;/p&gt; &lt;p&gt;Red Hat’s solution for lifecycle management can be used to operate all infrastructure layers and technologies with a single user experience. &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_for_sap_solutions/9/html/how_to_in-place_upgrade_sap_environments_from_rhel_8_to_rhel_9/index"&gt;Learn how to subscribe to Update Services for SAP Solutions on RHEL 8 and RHEL 9.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;RHEL for SAP Solutions includes high availability and proactive monitoring capabilities, remote management options, and extended support subscriptions, so developers and administrators can focus on achieving business objectives.&lt;/p&gt; &lt;h2&gt;Run and manage SAP applications on hyperscalers with RHEL&lt;/h2&gt; &lt;p&gt;SAP customers are looking for the ideal foundation for the hybrid cloud that allows for native application development and modernization to keep the digital core clean for deploying and extending SAP into a true hybrid cloud. &lt;/p&gt; &lt;p&gt;Running Red Hat Enterprise Linux for SAP Solutions on the public cloud can provide a single agnostic and consistent set of infrastructure tooling that can be used on-prem and on any cloud. Organizations can choose the cloud provider best for their workloads and be free to move those workloads as needed. Red Hat Enterprise Linux is certified on all the major cloud providers and has an extensive ecosystem of certified hardware platforms, value-add software integrations, and ISV software.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;In summary, running RHEL for SAP Solutions provides enterprises with a reliable, secure, high-performance platform optimized for running critical business systems. With Red Hat Enterprise Linux's migration support, high availability capabilities, live kernel patching, and powerful analytical tools, businesses can ensure that their SAP applications and solutions are always available, reliable, and efficient. &lt;a href="https://developers.redhat.com/products/rhel/download#rhel3ways"&gt;Download RHEL for SAP Solutions and Applications packages&lt;/a&gt; to get started.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/05/30/why-use-rhel-sap-solutions" title="Why use RHEL for SAP Solutions?"&gt;Why use RHEL for SAP Solutions?&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Nikhil Mungale</dc:creator><dc:date>2023-05-30T13:30:00Z</dc:date></entry><entry><title>Build an all-in-one edge manager with single-node OpenShift</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/05/30/build-edge-manager-single-node-openshift" /><author><name>Benjamin Schmaus, Josh Swanson</name></author><id>8727de2f-875d-4381-ae3d-638c2c1a1da1</id><updated>2023-05-30T07:00:00Z</updated><published>2023-05-30T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; includes an abundance of technologies out of the box that are necessary for effectively managing a fleet of devices at the &lt;a href="https://developers.redhat.com/topics/edge-computing"&gt;edge&lt;/a&gt;. One of those components, the scheduler, enables these services to be efficiently co-located onto a single platform.&lt;/p&gt; &lt;p&gt;In addition, OpenShift manages many of these services via an &lt;a href="https://developers.redhat.com/topics/kubernetes/operators/"&gt;Operator&lt;/a&gt;, meaning a non-technical team doesn’t need to understand all the specific details about the service. OpenShift, in a sense, helps make managing devices at the edge simpler and cost-effective.&lt;/p&gt; &lt;p&gt;This article details how to go about building this configuration on a single-node OpenShift cluster. Keep in mind that you can apply these same concepts to a 3-node compact and full OpenShift cluster as well.&lt;/p&gt; &lt;h2&gt;Why use OpenShift on a single node?&lt;/h2&gt; &lt;p&gt;Before we begin, let us step back and ask: Why do this? Well, there are a variety of reasons:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Having all the components in a single-node OpenShift (SNO) cluster makes it a great way to have a one-stop experience.&lt;/li&gt; &lt;li&gt;Having all the components in a single OpenShift node provides a quick and easy way to prove out a concept.&lt;/li&gt; &lt;li&gt;Since it's OpenShift, the SNO concept can be graduated to a large cluster to meet the capacity needs of a production environment.&lt;/li&gt; &lt;li&gt;Device Edge images are really just YAML files that should be maintained in Git, which gives us a clear path to infrastructure as code (IaC) and proper continuous integration/continuous deployment (&lt;a href="https://developers.redhat.com/topics/ci-cd/"&gt;CI/CD&lt;/a&gt;) all within OpenShift Container Platform.&lt;/li&gt; &lt;li&gt;Operators are genuinely a great way to reduce the barrier of entry when it comes to installing services and components in OpenShift.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Components&lt;/h2&gt; &lt;p&gt;Now that we understand some of the why, let's move forward and lay out what components we will be using in this single-node OpenShift "edge manager in a box." The core set of services we’ll be consuming are:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;An image registry to store our edge images as we compose them.&lt;/li&gt; &lt;li&gt;Management of local storage for retaining our composed images, databases, etc.&lt;/li&gt; &lt;li&gt;An instance of Ansible automation controller to drive our automation and leverage existing automation.&lt;/li&gt; &lt;li&gt;A pipeline technology; we’ll be using Red Hat OpenShift Pipelines.&lt;/li&gt; &lt;li&gt;A virtualization platform such as Red Hat OpenShift Virtualization (formerly container-native virtualization).&lt;/li&gt; &lt;li&gt;A virtual machine template to deploy virtual machines from which we can build our images.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;These core services, when integrated together, offer the functionality necessary for managing our fleet of device edge devices.&lt;/p&gt; &lt;p&gt;There are different ways to deploy workloads on OpenShift. However, because we’ll be consuming a handful of Operators, we find it's useful to leverage &lt;a href="https://developers.redhat.com/topics/automation"&gt;automation&lt;/a&gt; to get everything deployed. Ansible has a module in the kubernetes.core collection (k8s) that can be leveraged to talk directly to the Kubernetes API. We’ll use it here to push k8s objects related to installing Operators and creating instances from those Operators.&lt;/p&gt; &lt;h2&gt;The wrapper playbook&lt;/h2&gt; &lt;p&gt;The first playbook we need to create on our quest for edge device management is a wrapper playbook that will ultimately call all the playbooks to build out our environment. The playbook will look like the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;--- - name: import playbook to configure the local registry ansible.builtin.import_playbook: configure-registry.yml - name: import playbook to setup local storage ansible.builtin.import_playbook: configure-storage.yml - name: import playbook to setup controller ansible.builtin.import_playbook: install-ansible.yml - name: import playbook to setup pipelines ansible.builtin.import_playbook: configure-pipelines.yml - name: import playbook to setup virtualization ansible.builtin.import_playbook: configure-virtualization.yml - name: import playbook to setup image builder virtual machine template ansible.builtin.import_playbook: setup-image-builder-vm-template.yml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This playbook simply imports other playbooks that contain the actual steps necessary to get an Operator installed: create an OperatorGroup, deploy an instance, and more. We won’t go through all of these playbooks, but let’s take a deep dive on the playbook to set up &lt;a href="https://developers.redhat.com/products/ansible/overview"&gt;Red Hat Ansible Automation Platform&lt;/a&gt;:&lt;/p&gt; &lt;div class="highlight highlight-source-shell notranslate overflow-auto position-relative"&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;--- - name: install controller hosts: - sno_clusters gather_facts: false module_defaults: kubernetes.core.k8s: kubeconfig: "{{ tmpdir.path }}/ocp/auth/kubeconfig" tasks: - name: configure storage delegate_to: localhost block: - name: create namespace kubernetes.core.k8s: definition: "{{ lookup('file', 'files/namespaces/ansible-automation-platform.yaml') | from_yaml }}" - name: create operator group kubernetes.core.k8s: definition: "{{ lookup('file', 'files/operator-groups/ansible-automation-platform.yaml') | from_yaml }}" - name: install operator kubernetes.core.k8s: definition: "{{ lookup('file', 'files/operators/ansible-automation-platform.yaml') | from_yaml }}" register: operator_install until: - operator_install.result.status.state is defined - operator_install.result.status.state == 'AtLatestKnown' retries: 100 delay: 10 - name: create instance of controller kubernetes.core.k8s: definition: "{{ lookup('file', 'files/instances/controller.yaml') | from_yaml }}" &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;p&gt;The playbook above is grabbing files that contain k8s objects and pushing them into the Kubernetes API. For Ansible Automation Platform specifically, we have a namespace, an Operator group, a subscription, and then an instance of Controller. First, the namespace custom resource YAML:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;--- apiVersion: v1 kind: Namespace metadata: labels: openshift.io/cluster-monitoring: "true" name: ansible-automation-platform&lt;/code&gt;&lt;/pre&gt; &lt;div class="highlight highlight-source-shell notranslate overflow-auto position-relative"&gt; &lt;p&gt;Next, we have the Operator group custom resource YAML:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;--- apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: ansible-automation-platform-operator namespace: ansible-automation-platform spec: targetNamespaces: - ansible-automation-platform&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;p&gt;Then comes the subscription custom resource YAML:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;--- apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: ansible-automation-platform namespace: ansible-automation-platform spec: channel: 'stable-2.3' installPlanApproval: Automatic name: ansible-automation-platform-operator source: redhat-operators sourceNamespace: openshift-marketplace&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Finally, once the Operator finishes deploying, an instance of the controller custom resource YAML:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;--- apiVersion: automationcontroller.ansible.com/v1beta1 kind: AutomationController metadata: name: controller namespace: ansible-automation-platform spec: replicas: 1&lt;/code&gt;&lt;/pre&gt; &lt;div class="highlight highlight-source-shell notranslate overflow-auto position-relative"&gt; &lt;p&gt;After a few minutes, we’ll have a running instance of the Ansible automation controller on our OpenShift cluster.&lt;/p&gt; &lt;/div&gt; &lt;h2&gt;Configuring Ansible automation controller&lt;/h2&gt; &lt;p&gt;If we already have an instance of Controller set up and configured, then this part isn’t necessary. However, if we're starting from a completely empty instance of Controller, then we need to apply some base configuration to it so it can start driving automation.&lt;/p&gt; &lt;p&gt;Note: A best practice with automation controller is to store the configuration in code, then leverage automation to deploy the configuration to Controller. Here, we’ll leverage the redhat_cop.controller_configuration collection.  First, we’ll need some specific credential types:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;OpenShift kubeconfig:&lt;/strong&gt; A credential type to inject a kubeconfig into the execution environment of our automation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Red Hat Subscription Management credentials:&lt;/strong&gt; A credential type for storing authentication details for Red Hat Customer Portal.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Image credentials:&lt;/strong&gt; A credential type for securely storing the user account credentials we want in our composed images, as opposed to storing these in plain text.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ansible controller API credentials:&lt;/strong&gt; A set of credentials to authenticate to automation controller’s API.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Kube credentials:&lt;/strong&gt; A set of credentials that will be used to authenticate to our registry. I’m using OpenShift’s internal registry and the kubeadmin account, but you can substitute a properly scoped account and use a registry of your choosing.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The YAML definitions of these custom credential types:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;controller_credential_types: - name: Openshift Kubeconfig kind: cloud inputs: fields: - id: kubeconfig type: string label: Kubeconfig #secret: true multiline: true injectors: env: K8S_AUTH_KUBECONFIG: "{ { tower.filename.kubeconfig }}" KUBECONFIG: "{ { tower.filename.kubeconfig }}" file: template.kubeconfig: "{ { kubeconfig }}" - name: RHSM Credentials kind: cloud inputs: fields: - id: rhsm_username type: string label: RHSM Hostname - id: rhsm_password type: string label: RHSM Username secret: true injectors: extra_vars: rhsm_username: "{ { rhsm_username }}" rhsm_password: "{ { rhsm_password }}" - name: Image Credentials kind: cloud inputs: fields: - id: image_username type: string label: Image Hostname - id: image_password type: string label: Image Username secret: true injectors: extra_vars: image_username: "{ { image_username }}" image_password: "{ { image_password }}" - name: Ansible Controller API Credentials kind: cloud inputs: fields: - id: controller_hostname type: string label: Controller Hostname - id: controller_username type: string label: Controller Username - id: controller_password type: string label: Controller Password secret: yes injectors: extra_vars: controller_hostname: "{ { controller_hostname }}" controller_username: "{ { controller_username }}" controller_password: "{ { controller_password }}" controller_validate_certs: "no" - name: Kubeadmin Credentials kind: cloud inputs: fields: - id: kubeadmin_username type: string label: Kubeadmin username - id: kubeadmin_password type: string label: Kubeadmin password secret: true injectors: extra_vars: kubeadmin_username: "{ { kubeadmin_username }}" kubeadmin_password: "{ { kubeadmin_password }}"&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Most of these credential types are straightforward; however, the kubeconfig credential type has some additional injectors in the form of a file and an environment variable of the path to that file. In addition, the leading two brackets in the injector configurations are how we tell the collection to send an “unsafe” string to the API without attempting to render it locally. Leveraging our new credential types, we can create the set of credentials we’ll need for our automation:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;controller_credentials: - name: kubeconfig organization: Default credential_type: Openshift Kubeconfig inputs: kubeconfig: "{{ lookup('file', (tmpdir.path + '/ocp/auth/kubeconfig')) | from_yaml | string }}" - name: Machine Credentials organization: Default credential_type: Machine inputs: username: cloud-user password: "{{ vm_template_password }}" become_password: "{{ vm_template_password }}" - name: Ansible Controller API Credentials credential_type: Ansible Controller API Credentials organization: Default inputs: controller_hostname: "{{ controller_hostname }}" controller_username: admin controller_password: "{{ controller_password }}" - name: RHSM Credentials credential_type: RHSM Credentials organization: Default inputs: rhsm_username: "{{ rhsm_username }}" rhsm_password: "{{ rhsm_password }}" - name: Image Credentials credential_type: Image Credentials organization: Default inputs: image_username: "{{ image_username }}" image_password: "{{ image_password }}" - name: Kubeadmin Credentials credential_type: Kubeadmin Credentials organization: Default inputs: kubeadmin_username: kubeadmin kubeadmin_password: "{{ lookup('file', (tmpdir.path + '/ocp/auth/kubeadmin-password')) }}"&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, we’ll need an execution environment that contains the appropriate collections and &lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt; libraries. We’ll discuss the building of this execution environment later, but for now, this is the definition:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;controller_execution_environments: - name: Image Builder Execution Environment image: quay.io/device-edge-workshops/helper-ee:latest pull: always&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;After our execution environment, we’ll set up two inventories: one scoped for performing “local actions,” where the execution node performs the work without needing to connect to a remote system, and another to contain our image builder system:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;controller_inventories: - name: Image Builder Servers organization: Default variables: k8s_api_address: "api.{{ inventory_hostname }}" k8s_api_int_address: "api-int.{{ inventory_hostname }}:6443" ocp_namespace: image-builder image_registry: 'image-registry.openshift-image-registry.svc.cluster.local:5000' - name: Local Actions organization: Default variables: k8s_api_address: "api.{{ inventory_hostname }}" k8s_api_int_address: "api-int.{{ inventory_hostname }}:6443" ocp_namespace: image-builder image_registry: 'image-registry.openshift-image-registry.svc.cluster.local:5000'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Be sure to define the inventory variables to correspond to your OpenShift cluster environment. Next, a simple host to use for local actions:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;controller_hosts: - name: localhost inventory: Local Actions variables: ansible_connection: local ansible_python_interpreter: "{ { ansible_playbook_python }}"&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note: This has the same double spacing as above, meaning we’re sending a variable that will be resolved by Controller when it runs the automation, and not by the playbook configuring Controller right now. After that, a project containing our code:&lt;/p&gt; &lt;div class="highlight highlight-source-shell notranslate overflow-auto position-relative"&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;controller_projects: - name: Image Builder Codebase organization: Default scm_type: git scm_url: https://github.com/redhat-manufacturing/device-edge-demos.git&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;p&gt;Finally, we define our job templates:&lt;/p&gt; &lt;div class="highlight highlight-source-shell notranslate overflow-auto position-relative"&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;controller_templates: - name: Manage Virtual Machine Connectivity organization: Default inventory: Local Actions project: Image Builder Codebase playbook: demos/rhde-pipeline/playbooks/manage-vm-connection.yml execution_environment: Image Builder Execution Environment ask_variables_on_launch: true credentials: - kubeconfig - name: Manage Host in Controller organization: Default inventory: Local Actions project: Image Builder Codebase playbook: demos/rhde-pipeline/playbooks/manage-host-in-controller.yml execution_environment: Image Builder Execution Environment ask_variables_on_launch: true credentials: - kubeconfig - Ansible Controller API Credentials - name: Preconfigure Virtual Machine organization: Default inventory: Image Builder Servers project: Image Builder Codebase playbook: demos/rhde-pipeline/playbooks/preconfigure-virtual-machine.yml execution_environment: Image Builder Execution Environment ask_variables_on_launch: true become_enabled: true credentials: - Machine Credentials - RHSM Credentials - name: Install Image Builder organization: Default inventory: Image Builder Servers project: Image Builder Codebase playbook: demos/rhde-pipeline/playbooks/install-image-builder.yml execution_environment: Image Builder Execution Environment ask_variables_on_launch: true become_enabled: true credentials: - Machine Credentials - name: Manage Image Builder Connectivity organization: Default inventory: Local Actions project: Image Builder Codebase playbook: demos/rhde-pipeline/playbooks/manage-ib-connection.yml execution_environment: Image Builder Execution Environment ask_variables_on_launch: true credentials: - kubeconfig - name: Compose Image organization: Default inventory: Image Builder Servers project: Image Builder Codebase playbook: demos/rhde-pipeline/playbooks/compose-image.yml execution_environment: Image Builder Execution Environment ask_variables_on_launch: true become_enabled: true credentials: - Machine Credentials - Image Credentials - name: Push Image to Registry organization: Default inventory: Image Builder Servers project: Image Builder Codebase playbook: demos/rhde-pipeline/playbooks/push-image-to-registry.yml execution_environment: Image Builder Execution Environment ask_variables_on_launch: true become_enabled: true credentials: - Machine Credentials - Kubeadmin Credentials - name: Deploy Edge Container organization: Default inventory: Local Actions project: Image Builder Codebase playbook: demos/rhde-pipeline/playbooks/deploy-edge-container.yml execution_environment: Image Builder Execution Environment ask_variables_on_launch: true credentials: - kubeconfig&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;p&gt;A few things to note here: We’re consuming the credentials, inventories, project, and execution environment we created earlier. We’re also allowing some of these job templates to take additional variables when launched, a feature we’ll leverage later when building out our pipeline. Also, all of the referenced playbooks are available on GitHub as a starting point for building your own edge automation.&lt;/p&gt; &lt;h2&gt;Interfacing with automation controller&lt;/h2&gt; &lt;p&gt;Automation controller has a fully featured RESTful API that can be leveraged to perform basically every controller function, making it very easy to integrate with. However, we will do something a bit more custom, which will simplify our pipeline tasks and allow individual tasks to wait for the corresponding automation to complete.&lt;/p&gt; &lt;p&gt;A quick refresher: Execution environments are &lt;a href="https://developers.redhat.com/topics/containers"&gt;container&lt;/a&gt; images with roles, collections, Python libraries, and the Ansible bits pre-installed and ready to roll. Since we’re already operating within a container platform, we can reuse those execution environments within our pipeline tasks.&lt;/p&gt; &lt;p&gt;Because we’re building an execution environment, our collections and Python libraries will be included, meaning if we start the container, we can directly call Ansible. To extend the functionality a bit further, we’ll add a few steps to the build process and insert a playbook directly that we can leverage during our pipeline run.&lt;/p&gt; &lt;p&gt;Here’s an example Containerfile for our execution environment:&lt;/p&gt; &lt;div class="highlight highlight-source-shell notranslate overflow-auto position-relative"&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;ARG EE_BASE_IMAGE=registry.redhat.io/ansible-automation-platform-23/ee-minimal-rhel8:latest ARG EE_BUILDER_IMAGE=registry.redhat.io/ansible-automation-platform-23/ansible-builder-rhel8 FROM $EE_BASE_IMAGE as galaxy ARG ANSIBLE_GALAXY_CLI_COLLECTION_OPTS= ARG ANSIBLE_GALAXY_CLI_ROLE_OPTS= USER root ADD _build /build WORKDIR /build RUN ansible-galaxy role install $ANSIBLE_GALAXY_CLI_ROLE_OPTS -r requirements.yml --roles-path "/usr/share/ansible/roles" RUN ANSIBLE_GALAXY_DISABLE_GPG_VERIFY=1 ansible-galaxy collection install $ANSIBLE_GALAXY_CLI_COLLECTION_OPTS -r requirements.yml --collections-path "/usr/share/ansible/collections" FROM $EE_BUILDER_IMAGE as builder COPY --from=galaxy /usr/share/ansible /usr/share/ansible ADD _build/requirements.txt requirements.txt RUN ansible-builder introspect --sanitize --user-pip=requirements.txt --write-bindep=/tmp/src/bindep.txt --write-pip=/tmp/src/requirements.txt RUN assemble FROM $EE_BASE_IMAGE USER root # Add our customizations here RUN mkdir /helper-playbooks COPY run-job-template.yml /helper-playbooks/ COPY --from=galaxy /usr/share/ansible /usr/share/ansible COPY --from=builder /output/ /output/ RUN /output/install-from-bindep &amp;&amp; rm -rf /output/wheels LABEL ansible-execution-environment=true&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;p&gt;We’ve added two steps: creating a directory and placing a playbook into it. This playbook is very simple and only acts as a “go-between” our pipeline and the Controller API, yet allows us to wait for jobs to complete and do a bit of validation of inputs:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;--- - name: trigger job template run hosts: localhost gather_facts: false pre_tasks: - name: assert that vars are defined ansible.builtin.assert: that: - controller_hostname is defined - controller_username is defined - controller_password is defined - controller_validate_certs is defined - job_template is defined - name: set vars for role ansible.builtin.set_fact: controller_launch_jobs: - name: "{{ job_template }}" wait: true timeout: 14400 extra_vars: virtual_machine_name: "{{ virtual_machine_name | default('rhel9-vm') }}" resource_state: "{{ resource_state | default('present') }}" roles: - redhat_cop.controller_configuration.job_launch&lt;/code&gt;&lt;/pre&gt; &lt;div class="highlight highlight-source-shell notranslate overflow-auto position-relative"&gt; &lt;/div&gt; &lt;p&gt;Once the build is complete, this execution environment will also be consumable for our Device Edge build pipeline.&lt;/p&gt; &lt;h2&gt;Creating a pipeline to build Device Edge images&lt;/h2&gt; &lt;p&gt;With the automation pieces in place and an execution environment (container image) we can leverage as a simple interface between a pipeline and automation controller, we can start to build out a pipeline that will let us achieve our best practices for Device Edge images—defining them as code (IaC) and testing them before rolling them out to our fleet of devices (CI/CD).&lt;/p&gt; &lt;p&gt;From this point forward, we’re going to treat automation controller as what it is: a platform we can consume to run automation in the proper context and securely, all via the API.&lt;/p&gt; &lt;p&gt;The goal of our pipeline is to kick off a compose of a Device Edge image anytime we update or change our image definition. We’ll need to take some additional steps to set up for and capture our composed image, which the pipeline will also handle. Once those steps are completed, our pipeline will clean up all of the lingering pieces configured to ensure our compose works.&lt;/p&gt; &lt;p&gt;First, &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift 4.12&lt;/a&gt; includes a tech preview feature to manage virtual machines with OpenShift Pipelines, which allows us to easily spin up and spin down virtual machines as part of our pipeline.&lt;/p&gt; &lt;p&gt;Leveraging our customized execution environment from before, we’ll set up some tasks that will be strung together to form our pipeline. In addition, I’ve created a secret in the namespace of my virtual machine and pipeline that contains the details of my instance of Automation Controller; however, feel free to replace that with a proper secret storage system.&lt;/p&gt; &lt;p&gt;First, a task to expose the SSH port of the created virtual machine:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;--- apiVersion: tekton.dev/v1beta1 kind: Task metadata: name: manage-virtual-machine-connectivity namespace: image-builder spec: params: - name: virtualMachineName type: string description: The name of the virtual machine to expose default: rhel9-vm - name: resourceState type: string description: Creating or cleaning up default: present steps: - name: expose-virtual-machine image: quay.io/device-edge-workshops/helper-ee:latest env: - name: CONTROLLER_HOSTNAME valueFrom: secretKeyRef: name: controller-auth-account key: controller_hostname - name: CONTROLLER_USERNAME valueFrom: secretKeyRef: name: controller-auth-account key: controller_username - name: CONTROLLER_PASSWORD valueFrom: secretKeyRef: name: controller-auth-account key: controller_password - name: CONTROLLER_VALIDATE_CERTS valueFrom: secretKeyRef: name: controller-auth-account key: controller_validate_certs script: | ansible-playbook /helper-playbooks/run-job-template.yml \ --extra-vars "controller_hostname=$CONTROLLER_HOSTNAME" \ --extra-vars "controller_username=$CONTROLLER_USERNAME" \ --extra-vars "controller_password=$CONTROLLER_PASSWORD" \ --extra-vars "controller_validate_certs=$CONTROLLER_VALIDATE_CERTS" \ --extra-vars "job_template='Manage Virtual Machine Connectivity'" \ --extra-vars "virtual_machine_name=$(params.virtualMachineName)" \ --extra-vars "resource_state=$(params.resourceState)"&lt;/code&gt;&lt;/pre&gt; &lt;div class="highlight highlight-source-shell notranslate overflow-auto position-relative"&gt; &lt;/div&gt; &lt;p&gt;A good number of our tasks will look similar, so we can go through this task in detail and then simply make tweaks for later tasks.&lt;/p&gt; &lt;p&gt;From top to bottom, we’ve defined the following:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;A name and namespace for the task.&lt;/li&gt; &lt;li&gt;Some parameters the task will take, and default values for them. Note that we’ve defined a parameter of &lt;code&gt;resourceState&lt;/code&gt;—this allows us to reuse this same task to both create and destroy resources, simply by feeding in a different value from the pipeline.&lt;/li&gt; &lt;li&gt;Inserting the values of our Kubernetes secret into the container environment.&lt;/li&gt; &lt;li&gt;Our execution environment we built earlier.&lt;/li&gt; &lt;li&gt;A simple script block that calls our helper playbook and feeds in the appropriate variables.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;When this task runs, the execution environment is started, ansible-playbook is invoked, and our corresponding variables are fed to the playbook, which communicates with the Controller API.&lt;/p&gt; &lt;p&gt;Our other tasks are similar, with minor tweaks to the &lt;code&gt;job_template&lt;/code&gt; variable so a different job template is called and executed by controller. As an added perk, the collection leveraged within our playbook will wait for controller to complete the job, then return success or failure accordingly, giving our pipeline the necessary visibility.&lt;/p&gt; &lt;p&gt;To view all the tasks, check out the &lt;code&gt;tasks&lt;/code&gt; directory on GitHub. You can create tasks using Ansible (similar to above, where we were configuring OpenShift) or the &lt;code&gt;oc&lt;/code&gt; CLI tool.&lt;/p&gt; &lt;p&gt;With our tasks created, we can build our pipeline:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;--- apiVersion: tekton.dev/v1beta1 kind: Pipeline metadata: name: build-and-host-device-edge-image namespace: image-builder spec: tasks: - name: create-vm-from-template params: - name: templateName value: rhel9-image-builder-template - name: runStrategy value: RerunOnFailure - name: startVM value: 'true' taskRef: kind: ClusterTask name: create-vm-from-template - name: expose-virtual-machine-ssh params: - name: virtualMachineName value: $(tasks.create-vm-from-template.results.name) runAfter: - create-vm-from-template taskRef: kind: Task name: manage-virtual-machine-connectivity - name: create-host-in-controller params: - name: virtualMachineName value: $(tasks.create-vm-from-template.results.name) runAfter: - expose-virtual-machine-ssh taskRef: kind: Task name: manage-host-in-controller - name: preconfigure-virtual-machine params: - name: virtualMachineName value: $(tasks.create-vm-from-template.results.name) runAfter: - create-host-in-controller taskRef: kind: Task name: preconfigure-virtual-machine - name: install-image-builder params: - name: virtualMachineName value: $(tasks.create-vm-from-template.results.name) runAfter: - preconfigure-virtual-machine taskRef: kind: Task name: install-image-builder - name: expose-image-builder params: - name: virtualMachineName value: $(tasks.create-vm-from-template.results.name) runAfter: - install-image-builder taskRef: kind: Task name: manage-image-builder-connectivity - name: compose-image params: - name: virtualMachineName value: $(tasks.create-vm-from-template.results.name) runAfter: - install-image-builder - expose-image-builder taskRef: kind: Task name: compose-image - name: push-image-to-registry params: - name: virtualMachineName value: $(tasks.create-vm-from-template.results.name) runAfter: - compose-image taskRef: kind: Task name: push-image-to-registry - name: deploy-composed-image params: - name: virtualMachineName value: $(tasks.create-vm-from-template.results.name) runAfter: - push-image-to-registry taskRef: kind: Task name: push-image-to-registry finally: - name: cleanup-virtual-machine params: - name: vmName value: $(tasks.create-vm-from-template.results.name) - name: stop value: 'true' - name: delete value: 'true' taskRef: kind: ClusterTask name: cleanup-vm - name: cleanup-vm-connectivity params: - name: virtualMachineName value: $(tasks.create-vm-from-template.results.name) - name: resourceState value: absent taskRef: kind: Task name: manage-virtual-machine-connectivity - name: cleanup-image-builder-connectivity params: - name: virtualMachineName value: $(tasks.create-vm-from-template.results.name) - name: resourceState value: absent taskRef: kind: Task name: manage-image-builder-connectivity - name: cleanup-host-in-controller params: - name: virtualMachineName value: $(tasks.create-vm-from-template.results.name) - name: resourceState value: absent taskRef: kind: Task name: manage-host-in-controller&lt;/code&gt;&lt;/pre&gt; &lt;div class="highlight highlight-source-shell notranslate overflow-auto position-relative"&gt; &lt;/div&gt; &lt;p&gt;Let’s walk through the pipeline step-by-step:&lt;/p&gt; &lt;ol&gt;&lt;li dir="ltr"&gt;Create a virtual machine on OpenShift and pass the name to later tasks.&lt;/li&gt; &lt;li dir="ltr"&gt;Expose SSH to the virtual machine externally (this isn’t necessary, but it was useful while building and testing this process out).&lt;/li&gt; &lt;li dir="ltr"&gt;Create a corresponding host entry in automation controller.&lt;/li&gt; &lt;li dir="ltr"&gt;Run some preconfiguration steps on the virtual machine, such as registering to Red Hat Subscription Management.&lt;/li&gt; &lt;li dir="ltr"&gt;Install image builder.&lt;/li&gt; &lt;li dir="ltr"&gt;Compose a Device Edge image.&lt;/li&gt; &lt;li dir="ltr"&gt;Push the composed image to an image registry.&lt;/li&gt; &lt;li dir="ltr"&gt;Deploy the composed image to OpenShift.&lt;/li&gt; &lt;li dir="ltr"&gt;Clean up after ourselves.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;With this pipeline in place, we remove the burden of having to constantly run and manage a &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; image just to run image builder. Instead, all the infrastructure we need is spun up and down on demand, only existing while being consumed, then being destroyed after the work concludes.&lt;/p&gt; &lt;h2&gt;Expanding the concepts further&lt;/h2&gt; &lt;p&gt;This article is meant to serve as a foundation for building out an "edge manager in a box" capable of best practices for edge device management. As such, there are a few additional things we'd recommend adding to the above, but are out of scope for this specific tutorial:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;Use a legitimate secret store:&lt;/strong&gt; There are a few places above where simple secret storage is used, and while functional, it is not at all recommended for production use cases.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Extending the pipeline:&lt;/strong&gt; Currently, the pipeline really only tests if the image will successfully build. Ideally, this would be extended to provision a "test" system using the new image, and test deploying edge applications onto it before declaring the whole process a success.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Image builder:&lt;/strong&gt; Eventually, we want image builder to operate in a container, even a privileged one, which eliminates the need for the virtualization aspects of this workflow.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Image Registry: &lt;/strong&gt;While the internal OpenShift Container Platform registry does work, using a scalable robust registry makes sense in production. For a primer, check out &lt;a href="https://cloud.redhat.com/blog/the-quintessential-red-hat-quay-quickstart"&gt;this blog post&lt;/a&gt; on getting started with Red Hat Quay.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Links&lt;/h2&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://www.ansible.com/blog/ansible-validated-content-introduction-to-infra.osbuild-collection"&gt;Infra.osbuild validated collection&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://gregsowell.com/?p=7235"&gt;Ansible Controller As Code&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.openshift.com/container-platform/4.12/operators/operator_sdk/ansible/osdk-ansible-k8s-collection.html"&gt;Kubernetes Ansible Collection&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/redhat-manufacturing/device-edge-demos"&gt;Device Edge Demos GitHub&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/redhat-manufacturing/device-edge-demos/tree/main/demos/rhde-pipeline"&gt;Red Hat Device Edge pipeline used in this article&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/05/30/build-edge-manager-single-node-openshift" title="Build an all-in-one edge manager with single-node OpenShift"&gt;Build an all-in-one edge manager with single-node OpenShift&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Benjamin Schmaus, Josh Swanson</dc:creator><dc:date>2023-05-30T07:00:00Z</dc:date></entry></feed>
