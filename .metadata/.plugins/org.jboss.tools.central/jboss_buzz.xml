<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title>How to use RHEL application compatibility guidelines</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/05/18/how-use-rhel-application-compatibility-guidelines" /><author><name>Carlos O'Donell</name></author><id>6da035d9-239a-4926-aeaa-77409c970e44</id><updated>2023-05-18T07:00:00Z</updated><published>2023-05-18T07:00:00Z</published><summary type="html">&lt;p&gt;In this three-part series, we explore the &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; published application compatibility guidelines (ACG), and how developers can use them to ensure their application remains compatible with future releases of RHEL. Building applications can be difficult, and building applications that continue to operate after an in-place distribution upgrade is even harder. How does Red Hat Enterprise Linux make it easier? It provides guidelines and guarantees that you can follow to improve application compatibility.&lt;/p&gt; &lt;p&gt;In this article, we will expand on the concept of application compatibility. In the second part, we will review the topic of compatibility with more examples. In the third article, we will discuss container userspace compatibility with the host kernel services.&lt;/p&gt; &lt;h2&gt;What is application compatibility?&lt;/h2&gt; &lt;p&gt;What we call application compatibility is traditionally referred to as backwards compatibility. It is the ability to run an unmodified application binary on the current or newer version of the distribution and have it operate correctly (i.e., compatible with the release of the distribution).&lt;/p&gt; &lt;h3&gt;Maintaining compatibility&lt;/h3&gt; &lt;p&gt;Application compatibility is maintained by ensuring that the dependencies of the application continue to be provided and that the application continues to function as intended.&lt;/p&gt; &lt;p&gt;When we talk about &lt;a href="https://developers.redhat.com/topics/c"&gt;C or C++&lt;/a&gt; applications, this could mean that the libraries the application needs are still present and providing the expected set of features and behaviors. When we talk about &lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt;, it means continuing to provide the modules the Python script requires or the language features it needs.&lt;/p&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/18/whats-new-red-hat-enterprise-linux-9"&gt;Red Hat Enterprise Linux 9&lt;/a&gt; was released in May of 2022. At the same time, the &lt;a href="https://access.redhat.com/articles/rhel9-abi-compatibility"&gt;Red Hat Enterprise Linux 9: Application Compatibility Guide&lt;/a&gt; (RHEL ACG) was published, and the &lt;a href="https://access.redhat.com/support/policy/rhel-container-compatibility"&gt;Red Hat Enterprise Linux Container Compatibility Matrix&lt;/a&gt; (RHEL CCM) was updated. These two guides are key to helping developers learn about the guidelines they can follow to ensure application compatibility with a given RHEL release.&lt;/p&gt; &lt;p&gt;In the first two parts of this series, we will focus on the RHEL ACG.&lt;/p&gt; &lt;h2&gt;Defining API and ABI&lt;/h2&gt; &lt;p&gt;This is a quick recap of the detailed definitions in the compatibility guide. An API is the application programming interface, and it represents the set of conventions, features, or behaviors at compile time. An ABI is the application binary interface, and it represents the set of conventions, features, or behaviors at run time.&lt;/p&gt; &lt;p&gt;An API can be a source level function call (e.g.,&lt;code&gt;exit(0)&lt;/code&gt;). An ABI can be the actual implementation of the &lt;code&gt;void exit(int status)&lt;/code&gt; function in the C library.&lt;/p&gt; &lt;h2&gt;Components of the application compatibility guide&lt;/h2&gt; &lt;p&gt;The ACG is split into two major sections:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Guidelines for preserving application compatibility across minor and major RHEL versions.&lt;/li&gt; &lt;li aria-level="1"&gt;The binary rpm package list and the compatibility guarantees.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The distribution places the binary rpm packages into one of four compatibility levels. It can be viewed as a narrowing set of compatibility guarantees across minor and major upgrade paths.&lt;/p&gt; &lt;div&gt; &lt;table border="1" cellspacing="0" width="672"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;⇓ Compatibility Level / RHEL Version ⇒&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;RHEL X.Y&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;RHEL X.Y+1&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;RHEL X+1&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;RHEL X+2&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;1&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;API and ABI compatible&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;API and ABI compatible&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;ABI compatible&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;ABI compatible&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;2&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;API and ABI compatible&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;API and ABI compatible&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;3&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;API and ABI defined by life cycle&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;4&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;API and ABI subject to change at any time&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt; &lt;p&gt;Following the guidelines and using only packages that provide the guarantees your application needs will ensure that your application remains as compatible as possible across minor and major RHEL version upgrades.&lt;/p&gt; &lt;p&gt;To be compatible with minor version upgrades, it requires you to use only packages in compatibility level 2 or level 1, with review of packages used in level 3. To be compatible with major version upgrades, it requires you to use only packages in compatibility level 1, with review of packages used in level 3.&lt;/p&gt; &lt;p&gt;The default for packages in RHEL is compatibility level 2, which ensures that applications keep working across minor version upgrades.&lt;/p&gt; &lt;h2&gt;Workloads, services, containers, and packages&lt;/h2&gt; &lt;p&gt;You’ll notice that we talk a lot about packages. We talk about packages because it allows developers to decide which parts of decomposed workloads will be compatible with RHEL for a long time and which parts you might have to recompile, rewrite, or forward port.&lt;/p&gt; &lt;p&gt;At the highest level, you are going to have a workload that you want to support over time. That workload does something useful. Workloads can be managed with &lt;a href="https://developers.redhat.com/topics/automation"&gt;automation&lt;/a&gt; like &lt;a href="https://www.ansible.com/"&gt;Ansible&lt;/a&gt; or &lt;a href="https://backstage.io/"&gt;Backstage&lt;/a&gt;. I am not going to talk about workloads because as an abstraction, they are useful for talking at a very high level. When you think about it, the workload of “transactional request processing system” is too abstract for us to talk about compatibility.&lt;/p&gt; &lt;p&gt;A workload can be decomposed into services, and at this point it starts getting closer to the level at which we are talking about cross-RHEL-release compatibility guidelines. Services have concrete instantiations like an AMQP (Advanced Message Queuing Protocol) server running locally that handles messages (e.g., &lt;a href="https://www.rabbitmq.com/"&gt;RabbitMQ&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;When it comes to the smallest installable unit of something on RHEL, we can talk generally about containers or rpm packages. I’m going to defer the conversation about containers until part 3 of this series since the compatibility of containers is covered in the &lt;a href="https://access.redhat.com/support/policy/rhel-container-compatibility"&gt;Red Hat Enterprise Linux Container Compatibility Matrix&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;So either we are talking about the rpm packages in the container, or we’re talking about rpm packages on the host. As a developer you are still responsible for the decomposition of the workload and services into packages (software collections or modules are still delivered as packages). If packages change between major version upgrades then &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/upgrading_from_rhel_8_to_rhel_9/planning-an-upgrade_upgrading-from-rhel-8-to-rhel-9"&gt;RHEL Leapp&lt;/a&gt; is there to help ensure the same features are present even if the package changes names.&lt;/p&gt; &lt;h2&gt;Example: A RHEL 7 application written in C&lt;/h2&gt; &lt;p&gt;Explaining application compatibility guidelines is easier with an example. Say you are building an application in C that you started developing on RHEL 7, and you are now looking at RHEL 8 and RHEL 9 for eventual deployment.&lt;/p&gt; &lt;p&gt;Let’s dive in by using a simple C “Hello World” example and see what the application compatibility guidelines say about each of the development steps in building the application.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;#include &amp;lt;stdio.h&amp;gt; int main (void) { printf ("Hello World!\n"); return 0; }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Compile, inspect, and run as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ gcc -o helloworld helloworld.c $ ldd helloworld linux-vdso.so.1 =&amp;gt; (0x00007ffd09d75000) libc.so.6 =&amp;gt; /lib64/libc.so.6 (0x00007fe51a633000) /lib64/ld-linux-x86-64.so.2 (0x00007fe51aa01000) $ readelf -W --dyn-syms helloworld Symbol table '.dynsym' contains 4 entries: Num: Value Size Type Bind Vis Ndx Name 0: 0000000000000000 0 NOTYPE LOCAL DEFAULT UND 1: 0000000000000000 0 FUNC GLOBAL DEFAULT UND puts@GLIBC_2.2.5 (2) 2: 0000000000000000 0 FUNC GLOBAL DEFAULT UND __libc_start_main@GLIBC_2.2.5 (2) 3: 0000000000000000 0 NOTYPE WEAK DEFAULT UND __gmon_start__ $./helloworld Hello World!&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;There are several important takeaways from this example such as:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Links only the libraries it needs (ClLibrary: glibc, implicitly). &lt;ul&gt;&lt;li aria-level="2"&gt;Improves compatibility with future RHEL versions by limiting library use.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Uses C library header files for the APIs it uses (i.e., &lt;strong&gt;#include&lt;/strong&gt; directive). &lt;ul&gt;&lt;li aria-level="2"&gt;Ensures the development packages are installed provide those files and ensures any compatibility mechanisms are active.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Application is executed in an environment that is as new as the system it was compiled on. &lt;ul&gt;&lt;li aria-level="2"&gt;Backwards compatibility is guaranteed.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Does not use static linking. &lt;ul&gt;&lt;li aria-level="2"&gt;Robust runtime behavior of statically linked applications requires that the runtime environment match exactly the build time environment.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Avoids explicit dependency on a given Linux kernel version. &lt;ul&gt;&lt;li aria-level="2"&gt;Improves compatibility if the application is later packaged as a container.&lt;/li&gt; &lt;li aria-level="2"&gt;Improves compatibility with future RHEL kernel versions.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The &lt;a href="https://access.redhat.com/articles/rhel-abi-compatibility"&gt;RHEL7 Application Compatibility Guide&lt;/a&gt; recommends all these points and more.&lt;/p&gt; &lt;p&gt;So you might be asking, “Great, but what does that mean for my RHEL 8 and RHEL 9 migration?” Let's look first at the dependencies of the application. In this case, it’s only two dynamic symbols in the rpm package glibc, i.e. &lt;em&gt;&lt;strong&gt;puts@GLIBC_2.2.5&lt;/strong&gt;&lt;/em&gt; and &lt;em&gt;&lt;strong&gt;__libc_start_main@GLIBC_2.2.5&lt;/strong&gt;&lt;/em&gt;. Tracking which package provides these symbols is done by rpm and dynamic shared object names, and symbol set provides. It is a topic for another article, so for now, we'll skip this part.&lt;/p&gt; &lt;p&gt;The glibc binary rpm package is one of a small set of packages that is in compatibility level 1, and these are very important packages that are guaranteed to have a compatible ABI for at least three major RHEL releases. That means that the application we just created in this example should run correctly in RHEL 7, RHEL 8, and RHEL 9. Let's try it out.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;On RHEL 7:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ uname -r 3.10.0-1160.88.1.el7.x86_64 $ sha1sum helloworld beb000e63f609b09583a719ece01ea58b50dd2f8 helloworld $./helloworld Hello World!&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;On RHEL 8:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ uname -r 4.18.0-468.el8.x86_64 $ sha1sum helloworld beb000e63f609b09583a719ece01ea58b50dd2f8 helloworld $./helloworld Hello World!&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;On RHEL 9:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ uname -r 5.14.0-162.21.1.el9_1.x86_64 $ sha1sum helloworld beb000e63f609b09583a719ece01ea58b50dd2f8 helloworld $./helloworld Hello World! &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Having an application that only depends on compatibility level 1 packages allows the binary to run across three major RHEL releases without any issues. By 2024, that will be ten years of runtime compatibility!&lt;/p&gt; &lt;h2&gt;Example: A RHEL 7 application written in C using OpenSSL&lt;/h2&gt; &lt;p&gt;Let us take that example a bit further and try to use a library like OpenSSL that is only compatibility level 2, which means the ABI guarantee is only for the minor version upgrades of RHEL. That means that if you compile on RHEL 7.0, the application should still be working in RHEL 7.9 (last y-stream release), but is not guaranteed to work in RHEL 8.0 without recompilation in that distribution.&lt;/p&gt; &lt;p&gt;The example program here uses OpenSSL’s BIO interface to read from standard input and write the same thing to standard output. While this doesn’t exercise all of OpenSSL’s features, it does help us illustrate application compatibility.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;#include &amp;lt;openssl/ssl.h&amp;gt; #include &amp;lt;openssl/bio.h&amp;gt; #include &amp;lt;stdlib.h&amp;gt; #define BUFSIZE 4096 int main(void) { char buf[BUFSIZE]; int bin, bout; BIO *bio_stdin, *bio_stdout; bio_stdin = BIO_new_fp(stdin, BIO_NOCLOSE); bio_stdout = BIO_new_fp(stdout, BIO_NOCLOSE); if (bio_stdin == NULL || bio_stdout == NULL) exit (1); while ((bin = BIO_read (bio_stdin, buf, BUFSIZE)) &amp;gt; 0) { bout = BIO_write (bio_stdout, buf, bin); if (bin != bout) exit (1); } BIO_free (bio_stdout); BIO_free (bio_stdin); return 0; } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Compiled, inspected, and run like the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ gcc -o bio-cp bio-cp.c -lssl -lcrypto $ ldd./bio-cp linux-vdso.so.1 =&amp;gt; (0x00007ffc4cfa1000) libssl.so.10 =&amp;gt; /lib64/libssl.so.10 (0x00007f1811c65000) libcrypto.so.10 =&amp;gt; /lib64/libcrypto.so.10 (0x00007f1811802000) libc.so.6 =&amp;gt; /lib64/libc.so.6 (0x00007f1811434000) libgssapi_krb5.so.2 =&amp;gt; /lib64/libgssapi_krb5.so.2 (0x00007f18111e7000) libkrb5.so.3 =&amp;gt; /lib64/libkrb5.so.3 (0x00007f1810efe000) libcom_err.so.2 =&amp;gt; /lib64/libcom_err.so.2 (0x00007f1810cfa000) libk5crypto.so.3 =&amp;gt; /lib64/libk5crypto.so.3 (0x00007f1810ac7000) libdl.so.2 =&amp;gt; /lib64/libdl.so.2 (0x00007f18108c3000) libz.so.1 =&amp;gt; /lib64/libz.so.1 (0x00007f18106ad000) /lib64/ld-linux-x86-64.so.2 (0x00007f1811ed7000) libkrb5support.so.0 =&amp;gt; /lib64/libkrb5support.so.0 (0x00007f181049d000) libkeyutils.so.1 =&amp;gt; /lib64/libkeyutils.so.1 (0x00007f1810299000) libresolv.so.2 =&amp;gt; /lib64/libresolv.so.2 (0x00007f181007f000) libpthread.so.0 =&amp;gt; /lib64/libpthread.so.0 (0x00007f180fe63000) libselinux.so.1 =&amp;gt; /lib64/libselinux.so.1 (0x00007f180fc3c000) libpcre.so.1 =&amp;gt; /lib64/libpcre.so.1 (0x00007f180f9da000) $ readelf -W --dyn-syms bio-cp Symbol table '.dynsym' contains 15 entries: Num: Value Size Type Bind Vis Ndx Name 0: 0000000000000000 0 NOTYPE LOCAL DEFAULT UND 1: 0000000000000000 0 FUNC GLOBAL DEFAULT UND BIO_write@libcrypto.so.10 (3) 2: 0000000000000000 0 FUNC GLOBAL DEFAULT UND BIO_read@libcrypto.so.10 (3) 3: 0000000000000000 0 FUNC GLOBAL DEFAULT UND BIO_free@libcrypto.so.10 (3) 4: 0000000000000000 0 FUNC GLOBAL DEFAULT UND exit@GLIBC_2.2.5 (2) 5: 0000000000000000 0 FUNC GLOBAL DEFAULT UND BIO_new_fp@libcrypto.so.10 (3) 6: 0000000000000000 0 FUNC GLOBAL DEFAULT UND __libc_start_main@GLIBC_2.2.5 (2) 7: 0000000000000000 0 NOTYPE WEAK DEFAULT UND __gmon_start__ 8: 0000000000601060 8 OBJECT GLOBAL DEFAULT 25 stdout@GLIBC_2.2.5 (2) 9: 0000000000601054 0 NOTYPE GLOBAL DEFAULT 24 _edata 10: 0000000000601078 0 NOTYPE GLOBAL DEFAULT 25 _end 11: 0000000000601068 8 OBJECT GLOBAL DEFAULT 25 stdin@GLIBC_2.2.5 (2) 12: 0000000000400640 0 FUNC GLOBAL DEFAULT 11 _init 13: 0000000000601054 0 NOTYPE GLOBAL DEFAULT 25 __bss_start 14: 0000000000400914 0 FUNC GLOBAL DEFAULT 14 _fini $./bio-cp &amp;lt; helloworld.c #include &amp;lt;stdio.h&amp;gt; int main (void) { printf ("Hello World!\n"); return 0; } $ echo $? 0&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The application compatibility guidelines for RHEL 7 state that to be compatible, you must recompile at each major release to ensure ABI compatibility. Again, this is because OpenSSL is in compatibility level 2. Let's put this to the test.&lt;/p&gt; &lt;p&gt;On RHEL 8:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$./bio-cp ./bio-cp: error while loading shared libraries: libssl.so.10: cannot open shared object file: No such file or directory&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;On RHEL 9:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$./bio-cp ./bio-cp: error while loading shared libraries: libssl.so.10: cannot open shared object file: No such file or directory&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The default version of OpenSSL in RHEL7 is 1.0. In RHEL8 it is 1.1.1, and in RHEL9 it is 3.0. Each version is unique, and to use them requires the application to be compiled against the specific version in the distribution.&lt;/p&gt; &lt;p&gt;The application compatibility guide lists OpenSSL as compatibility level 2, and it bears repeating that such libraries have guaranteed ABI compatibility only within the major version of RHEL in which they were released.&lt;/p&gt; &lt;p&gt;The recent &lt;a href="https://www.redhat.com/en/blog/experience-bringing-openssl-30-rhel-and-fedora"&gt;migration of RHEL9 to OpenSSL 3.0&lt;/a&gt; was a technically complex migration. Red Hat went above and beyond by providing customers with compatibility packages to facilitate application developers. These compatibility packages provide the libraries required to meet the ABI requirements of OpenSSL using applications. Similar packages were also provided in RHEL 8 to enable the migration from OpenSSL 1.0 to 1.1.1. Lets try out the compatibility packages.&lt;/p&gt; &lt;p&gt;On RHEL 8 with compat-openssl10:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ dnf install compat-openssl10 ... $ ldd bio-cp linux-vdso.so.1 (0x00007fff5df5b000) libssl.so.10 =&amp;gt; /lib64/libssl.so.10 (0x00007fac6d7ed000) libcrypto.so.10 =&amp;gt; /lib64/libcrypto.so.10 (0x00007fac6d38b000) libc.so.6 =&amp;gt; /lib64/libc.so.6 (0x00007fac6cfc6000) libdl.so.2 =&amp;gt; /lib64/libdl.so.2 (0x00007fac6cdc2000) libz.so.1 =&amp;gt; /lib64/libz.so.1 (0x00007fac6cbaa000) /lib64/ld-linux-x86-64.so.2 (0x00007fac6da5c000) $./bio-cp &amp;lt; helloworld.c #include &amp;lt;stdio.h&amp;gt; int main (void) { printf ("Hello World!\n"); return 0; }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;While the ACG says not to rely on such compatibility for compatibility level 2 packages, there are cases like this one with OpenSSL where Red Hat has gone above and beyond to ensure customer success.&lt;/p&gt; &lt;h2&gt;Learn more about RHEL ACG&lt;/h2&gt; &lt;p&gt;We have discussed application compatibility, and how Red Hat provides guidelines and guarantees to help developers create applications that are compatible with minor and major version upgrades for Red Hat Enterprise Linux. We have looked at the first important document that provides those guidelines for RHEL 9, the &lt;a href="https://access.redhat.com/articles/rhel9-abi-compatibility"&gt;Red Hat Enterprise Linux 9: Application Compatibility Guide&lt;/a&gt; (RHEL ACG). We have looked at two examples that showcase application compatibility across major version upgrades.&lt;/p&gt; &lt;p&gt;I encourage all developers out there to read the &lt;a href="https://access.redhat.com/articles/rhel9-abi-compatibility"&gt;Red Hat Enterprise Linux 9: Application Compatibility Guide&lt;/a&gt; (RHEL ACG), and make sure you are following the best practice guidelines to meet the compatibility needs of your application, service, or workload. Stay tuned for part 2 of this series where we will look again at the RHEL ACG, but dive into more complex examples. Part 3 will cover container compatibility and describe the &lt;a href="https://access.redhat.com/support/policy/rhel-container-compatibility"&gt;Red Hat Enterprise Linux Container Compatibility Matrix&lt;/a&gt; (RHEL CCM).&lt;/p&gt; The post &lt;a href="/articles/2023/05/18/how-use-rhel-application-compatibility-guidelines" title="How to use RHEL application compatibility guidelines"&gt;How to use RHEL application compatibility guidelines&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br&gt;&lt;br&gt;</summary><dc:creator>Carlos O'Donell</dc:creator><dc:date>2023-05-18T07:00:00Z</dc:date></entry><entry><title type="html">This Week in JBoss - May, 15 2023</title><link rel="alternate" href="https://www.jboss.org/posts/weekly-2023-05-18.html" /><category term="quarkus" /><category term="java" /><category term="wildfly" /><category term="camel" /><category term="strimzi" /><category term="podman" /><author><name>Francesco Marchioni</name><uri>https://www.jboss.org/people/francesco-marchioni</uri><email>do-not-reply@jboss.com</email></author><id>https://www.jboss.org/posts/weekly-2023-05-18.html</id><updated>2023-05-18T00:00:00Z</updated><content type="html">&lt;article class="" data-tags="quarkus, java, wildfly, camel, strimzi, podman"&gt; &lt;h1&gt;This Week in JBoss - May, 15 2023&lt;/h1&gt; &lt;p class="preamble"&gt;&lt;/p&gt;&lt;p&gt;Happy Friday, everyone!&lt;/p&gt; &lt;p&gt;Here is another edition of the JBoss Editorial with exciting news and updates from your JBoss communities.&lt;/p&gt;&lt;p&gt;&lt;/p&gt; &lt;div class="sect1"&gt; &lt;h2 id="_release_roundup"&gt;Release roundup&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Here are the most recent releases for this edition:&lt;/p&gt; &lt;div class="ulist square"&gt; &lt;ul class="square"&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/quarkus-3-0-3-final-released/"&gt;Quarkus 3.0.3.Final released&lt;/a&gt; - The Quarkus Team released Quarkus 3.0.3.Final, as part of the second maintenance release of our 3.0 release train. This release contains bugfixes and documentation improvements.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://blog.kie.org/2023/05/kogito-1-37-0-released.html"&gt;Kogito 1.37 released&lt;/a&gt; - The new Kogito release features enhancements in the flow actions, workflow definitions logging and service discovery.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://camel.apache.org/releases/release-4.0.0-M3/"&gt;Camel RELEASE 4.0.0-M3 available&lt;/a&gt; - The Camel community announces the availability of Camel 4.0.0-M3, the third milestone towards a new 4.0.0 major release which comes with 155 new features and improvements.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_exactly_once_semantics_with_kafka_transactions"&gt;Exactly-once semantics with Kafka transactions&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="https://strimzi.io/blog/2023/05/03/kafka-transactions/"&gt;Exactly-once semantics with Kafka transactions&lt;/a&gt;, by Federico Valeri.&lt;/p&gt; &lt;p&gt;Kafka transactions play a vital role in guaranteeing the reliability and integrity of data, making them a pivotal component of the Kafka platform. Nevertheless, these benefits are accompanied by a trade-off in terms of decreased throughput and added latency, necessitating potential adjustments. Neglecting to monitor transactions that remain unresolved can adversely affect the availability of the service. This article sheds some light on this topic.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_how_to_use_the_new_openshift_quick_starts_to_deploy_jboss_eap"&gt;How to use the new OpenShift quick starts to deploy JBoss EAP&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2023/05/15/how-use-new-openshift-quick-starts-deploy-jboss-eap"&gt;How to use the new OpenShift quick starts to deploy JBoss EAP&lt;/a&gt;, by Philip Hayes&lt;/p&gt; &lt;p&gt;This article showcases the latest JBoss EAP quick start, specifically created to assist developers who are already familiar with conventional JBoss EAP deployments. Its purpose is to provide a comprehensive walkthrough on constructing and deploying application images on OpenShift. The quick start offers valuable guidance on utilizing Helm to generate the necessary build configs, deployment configs, and external routes for building and deploying JBoss EAP applications on OpenShift.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_integrate_excel_with_drools_on_openshift_with_knative_and_quarkus"&gt;Integrate Excel with Drools on OpenShift with Knative and Quarkus&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="https://blog.kie.org/2023/05/integrate-excel-with-drools-on-openshift-with-knative-and-quarkus.html"&gt;Integrate Excel with Drools on OpenShift with Knative and Quarkus&lt;/a&gt;, by Matteo Mortari&lt;/p&gt; &lt;p&gt;In this blog post, Matteo shares the results of a technical exploration of bringing together different technologies and platforms. At the end of the day, he combined things like regular spreadsheet applications (like Excel), serverless platforms (Knative on OpenShift), and our rule engine Drools to see how they could work together.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_podman_desktop_beginners_guide"&gt;Podman Desktop Beginner’s Guide&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="https://www.mastertheboss.com/soa-cloud/docker/podman-desktop-a-beginners-guide-to-containerization/"&gt;Podman Desktop Guide&lt;/a&gt;, by Francesco Marchioni&lt;/p&gt; &lt;p&gt;In this tutorial, I’m introducing the Podman desktop UI which simplifies the usage and management of container images. As an example, we will learn how to pull, start, and monitor a WildFly Container image with just a few clicks!&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_whats_new_in_red_hats_migration_toolkit_for_applications_6_1"&gt;What’s new in Red Hat’s migration toolkit for applications 6.1&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2023/05/10/whats-new-red-hats-migration-toolkit-applications-61"&gt;What’s new in Red Hat’s migration toolkit for applications 6.1&lt;/a&gt;, by Yashwanth Maheshwaram&lt;/p&gt; &lt;p&gt;Red Hat Migration Toolkit is an essential tool to simplify the upgrade or migration of a large set of enterprise applications. This article gives you an update with the latest news and includes a great demo video.&lt;/p&gt; &lt;p&gt;&lt;em&gt;That’s all folks! Please join us again in two weeks for another round of our JBoss editorial!&lt;/em&gt;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="author"&gt; &lt;pfe-avatar pfe-shape="circle" pfe-pattern="squares" pfe-src="/img/people/francesco-marchioni.png"&gt;&lt;/pfe-avatar&gt; &lt;span&gt;Francesco Marchioni&lt;/span&gt; &lt;/div&gt;&lt;/article&gt;</content><dc:creator>Francesco Marchioni</dc:creator></entry><entry><title type="html">Toward a reliable and fully recoverable Drools stateful session</title><link rel="alternate" href="https://blog.kie.org/2023/05/toward-a-reliable-and-fully-recoverable-drools-stateful-session.html" /><author><name>Mario Fusco</name></author><id>https://blog.kie.org/2023/05/toward-a-reliable-and-fully-recoverable-drools-stateful-session.html</id><updated>2023-05-17T14:09:35Z</updated><content type="html">One of the most important features of Drools is the possibility of evaluating rules in an interactive and conversational way, allowing to use inference to make iterative changes to facts over time and preserving the state of a session among different invocations. Actually this characteristic is so widely used, for instance when performing complex event processing, to be the default behavior of a .  The state of a session is of course made of many different data structures that live on the JVM heap. This implies that its lifespan is bound to the one of the hosting JVM and gets lost if for any reason the JVM itself, or more likely the machine, physical or virtual, on which that JVM is running, experiences a failure and is terminated.  This can be a serious limitation in the use of a stateful session, especially in a cloud environment where also the node running the session has to be considered ephemeral and can be killed at any moment. In this blog post we will introduce the implementation of a Drools reliable session that is capable of automatically persisting its internal state while it is running thus allowing it to recreate a new session from that persisted state when for any reason the original session becomes unavailable. INTRODUCING DROOLS RELIABLE SESSION The version 8.38.0.Final of Drools introduces a containing a first implementation of the Drools reliable session. At the moment it is made of 2 submodules. The first one defines the common API and general implementation of the reliable session, abstracting away the persistence layer in order to make it possible to plug different concrete implementations of this layer. The second provides one of such persistence layer implementations based on , while other modules relying on different technologies for persistence will be made available in future. Through this module it is possible to create a reliable KieSession as it follows KieSessionConfiguration conf = KieServices.get().newKieSessionConfiguration(); conf.setOption(PersistedSessionOption.newSession(PersistedSessionOption.Strategy.STORES_ONLY)); KieSession ksession = kbase.newKieSession(conf, null); and start using it as any other normal Drools stateful session. The only requirement to resume this session after a JVM crash is keeping its identifier on the client side long savedSessionId = session.getIdentifier(); so that it will be possible at a later time to recreate a new stateful session preserving the same state of the old one. KieSessionConfiguration conf = KieServices.get().newKieSessionConfiguration(); conf.PersistedSessionOption.fromSession(savedSessionId, PersistedSessionOption.Strategy.STORES_ONLY)); KieSession ksession = kbase.newKieSession(conf, null); PUTTING THE RELIABLE SESSION AT WORK IN THE CLOUD As anticipated, this new capability for a Drools session of being reliable makes it a perfect fit for a cloud environment where a node hosting a long running stateful computation could suddenly die for many different reasons. When this happens another node can be started without losing any data of the old session and keeping using it as nothing happened. To demonstrate how this works we developed a based on implementing a simple rule service to validate a set of loan applications and get the total amount of approved loans in a reliable stateful session. In this way the state of a session is persisted across different executions, so if the server is shut down and then restarted the state of an old session is not lost. This demo persists the state of a session using Infinispan running in embedded mode. This demo application can be also . While doing so you can try to scale to zero the deployment running this application. If you do so, of course the application won’t respond anymore to any further REST call, but scaling it up to one again the reliable session will automatically restore its state and the computation can continue as nothing happened. A video demonstrating how this works is available here: The post appeared first on .</content><dc:creator>Mario Fusco</dc:creator></entry><entry><title>Build lean Node.js container images with UBI and Podman</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/05/17/build-lean-nodejs-container-images-ubi-and-podman" /><author><name>Evan Shortiss</name></author><id>e02331c3-e1ff-4cd5-b43f-8ebc71e5f23f</id><updated>2023-05-17T07:00:00Z</updated><published>2023-05-17T07:00:00Z</published><summary type="html">&lt;p&gt;Building a &lt;a href="https://developers.redhat.com/topics/containers"&gt;container&lt;/a&gt; image for your &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; application might sound like a trivial task, but there are some pitfalls you’ll want to avoid along the path to containerizing your application. It is all too easy to accidentally include sensitive files, run more than one process in the container, run as root, or ship bloated container images. Because all of these mistakes reflect an image constructed without due care, reducing bloat reduces them all.&lt;/p&gt; &lt;p&gt;This post focuses on the “bloated container images” mistake that’s pretty easy to make when building Node.js application container images. Keep reading to learn about container image layers, how you can slim down a Node.js container image based on &lt;a href="https://developers.redhat.com/products/rhel/ubi"&gt;Red Hat’s Universal Base Images&lt;/a&gt; by over 70% (see Figure 1), and even more &lt;a href="https://developers.redhat.com/articles/2021/08/26/introduction-nodejs-reference-architecture-part-5-building-good-containers"&gt;best practices for building containers&lt;/a&gt;.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="/sites/default/files/image1_16.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="/sites/default/files/styles/article_floated/public/image1_16.png?itok=E9w80EPK" width="600" height="414" alt="A graph comparing the size of container images for a Node.js application depending on the build-strategy and base image used." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: Image sizes resulting from a various build approaches using Red Hat's Universal Base Images for Node.js.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p class="Indent1"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;Note:&lt;/strong&gt; All of the examples and code used in this post can be found in &lt;a href="https://github.com/evanshortiss/nodejs-container-builds-example"&gt;this repository on GitHub&lt;/a&gt;. You’ll need Node.js 18 and either Docker or Podman installed to follow along. &lt;a href="https://podman-desktop.io/"&gt;Podman&lt;/a&gt; is the container engine used for the examples in this post. Substitute &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;code&gt;docker&lt;/code&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt; in place of &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;code&gt;podman&lt;/code&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt; in commands if you’re using Docker instead of Podman. &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;h2&gt;Building a Node.js application container image&lt;/h2&gt; &lt;p&gt;The Node.js documentation provides a great overview of how to &lt;a href="https://nodejs.org/en/docs/guides/nodejs-docker-webapp/"&gt;configure a Containerfile (also known as a Dockerfile)&lt;/a&gt; for a basic Node.js application. Let’s use that as a template to create a container image for a Node.js application. This application will use the &lt;a href="https://fastify.io"&gt;Fastify framework&lt;/a&gt; and TypeScript, but it’s worth noting that this guide is applicable to any Node.js web framework.&lt;/p&gt; &lt;p&gt;To get started, generate a new Fastify project that uses TypeScript using the Fastify CLI:&lt;/p&gt; &lt;pre&gt; npx fastify-cli@5 generate --lang=ts nodejs-ts-basic # Change into the project directory and generate a package-lock.json cd nodejs-ts-basic npm i --package-lock-only&lt;/pre&gt; &lt;p&gt;&lt;br /&gt; Create a &lt;strong&gt;Containerfile&lt;/strong&gt; in the new &lt;code&gt;nodejs-ts-basic&lt;/code&gt; project directory with the following contents:&lt;/p&gt; &lt;pre&gt; FROM registry.access.redhat.com/ubi8/nodejs-18 WORKDIR /usr/src/app # Copy in package.json and package-lock.json COPY --chown=1001:1001 package*.json ./ # Install dependencies and devDependencies RUN npm ci # Copy in source code and other assets COPY --chown=1001:1001 . . # Compile the source TS into JS files RUN npm run build:ts # Configure fastify behaviour, and NODE_ENV ENV NODE_ENV=production ENV FASTIFY_PORT 8080 ENV FASTIFY_ADDRESS 0.0.0.0 EXPOSE 8080 # Set the fastify-cli binary as the entrypoint ENTRYPOINT [ "./node_modules/.bin/fastify" ] # Launch the container by passing these parameters to the entrypoint # These parameters can be overridden if you’d like CMD [ "start", "-l", "info", "dist/app.js" ]&lt;/pre&gt; &lt;p&gt;&lt;br /&gt; Next, create a &lt;code&gt;.dockerignore&lt;/code&gt; file in the root of the repository. This works similar to a &lt;code&gt;.gitignore&lt;/code&gt;, but is respected by tools like Podman and Docker. It’s used to avoid copying the specified files into a container image:&lt;/p&gt; &lt;pre&gt; # Change this as necessary for your own project(s) Containerfile* README.md dist node_modules test *.log .dockerignore .taprc .npmrc .env* &lt;/pre&gt; &lt;p&gt;&lt;br /&gt; With these files in place, build a container image using the Podman (or Docker) CLI:&lt;/p&gt; &lt;pre&gt; podman build . -f Containerfile -t nodejs-ts-basic &lt;/pre&gt; &lt;p&gt;&lt;br /&gt; The resulting container image is approximately 831 MB in size. That's 188 MB larger than the &lt;a href="https://catalog.redhat.com/software/containers/ubi8/nodejs-18/6278e5c078709f5277f26998?container-tabs=gti"&gt;UBI Node.js v18 base image&lt;/a&gt;! Investigate the size of files and folders by running the &lt;code&gt;du&lt;/code&gt; command inside the container:&lt;/p&gt; &lt;pre&gt; podman run --rm nodejs-ts-basic /bin/du -h -d 1 60K ./dist 28K ./src 190M ./node_modules 190M . &lt;/pre&gt; &lt;p&gt;&lt;br /&gt; Clearly the &lt;code&gt;node_modules&lt;/code&gt; folder is causing bloat in the image, because both the &lt;code&gt;dependencies&lt;/code&gt; and the &lt;code&gt;devDependencies&lt;/code&gt; specified in the package.json were installed. &lt;/p&gt; &lt;h2&gt;Attempting to slim down the Node.js container image&lt;/h2&gt; &lt;p&gt;A seemingly obvious solution to this problem is to remove those &lt;code&gt;devDependencies&lt;/code&gt; from the image. Try that by adding &lt;code&gt;npm prune --omit=dev&lt;/code&gt; after the &lt;code&gt;npm run build:ts&lt;/code&gt; command in the &lt;strong&gt;Containerfile&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt; RUN npm run build:ts RUN npm prune --omit=dev &lt;/pre&gt; &lt;p&gt;&lt;br /&gt; And building it:&lt;/p&gt; &lt;pre&gt; podman build . -f Containerfile -t nodejs-ts-basic-prune &lt;/pre&gt; &lt;p&gt;&lt;br /&gt; The container image will be more lightweight, right? Let's check:&lt;/p&gt; &lt;pre&gt; podman images --format '{{.Size}} {{.Repository}}' | grep nodejs 831 MB localhost/nodejs-ts-basic 832 MB localhost/nodejs-ts-basic-prune &lt;/pre&gt; &lt;p&gt;&lt;br /&gt; This new image is larger than the last one! This is because container images are composed of layers. Each layer stores changes compared to the prior layer it's based on. The &lt;code&gt;npm prune&lt;/code&gt; command removed the &lt;code&gt;devDependencies&lt;/code&gt; from the final container image layer (you can confirm using the &lt;code&gt;du&lt;/code&gt; command shown previously), but the layer containing them is still there. Confirm this using the &lt;code&gt;podman history localhost/nodejs-ts-basic-prune&lt;/code&gt; command, noting that the &lt;strong&gt;npm ci&lt;/strong&gt; layer is there, and is over 187 MB in size.&lt;/p&gt; &lt;h2&gt;Multi-stage builds to the rescue&lt;/h2&gt; &lt;p&gt;A great solution to this problem is to use a &lt;a href="https://docs.docker.com/build/building/multi-stage/"&gt;multi-stage build&lt;/a&gt;. Multi-stage builds perform some of the build steps in separate containers, and copy only what‘s needed into the final container image. This reduces the number of layers and overall size of the final container image.&lt;/p&gt; &lt;p&gt;This is an example of a multi-stage &lt;strong&gt;Containerfile&lt;/strong&gt; that can be used to slim down the Node.js container image:&lt;/p&gt; &lt;pre&gt; # First stage of the build is to install dependencies, and build from source FROM registry.access.redhat.com/ubi8/nodejs-18 as build WORKDIR /usr/src/app COPY --chown=1001:1001 package*.json ./ RUN npm ci COPY --chown=1001:1001 tsconfig*.json ./ COPY --chown=1001:1001 src src RUN npm run build:ts # Second stage of the build is to create a lighter container with just enough # required to run the application, i.e production deps and compiled js files FROM registry.access.redhat.com/ubi8/nodejs-18 WORKDIR /usr/src/app COPY --chown=1001:1001 --from=build /usr/src/app/package*.json/ . RUN npm ci --omit=dev COPY --chown=1001:1001 --from=build /usr/src/app/dist/ dist/ ENV NODE_ENV=production ENV FASTIFY_PORT 8080 ENV FASTIFY_ADDRESS 0.0.0.0 EXPOSE 8080 ENTRYPOINT [ "./node_modules/.bin/fastify" ] CMD [ "start", "-l", "info", "dist/app.js" ] &lt;/pre&gt; &lt;p&gt;&lt;br /&gt; A summary of the two stages:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The first stage (&lt;strong&gt;build&lt;/strong&gt;) installs all dependencies, and compiles the TypeScript code.&lt;/li&gt; &lt;li&gt;The second stage copies the compiled code from the &lt;em&gt;build&lt;/em&gt; image and installs only the production dependencies to produce a deployable container image.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The initial build using this multi-stage &lt;strong&gt;Containerfile&lt;/strong&gt; will be slower, but subsequent builds benefit from cached layers and are only a second or two slower than the single stage build. &lt;/p&gt; &lt;p&gt;The multi-stage build results in a container image that's just 661 MB. That’s 20% smaller than the single stage image’s 831 MB. This isn’t bad—but you can do better.&lt;/p&gt; &lt;h2&gt;Going minimal&lt;/h2&gt; &lt;p&gt;There’s one last step you can take to really slim this Node.js container image down; and that’s using a minimal base image. A minimal base image contains as few tools and libraries as possible, which means they have a significantly smaller footprint. &lt;/p&gt; &lt;p&gt;Modifying the second stage of the multi-stage build to use &lt;a href="https://catalog.redhat.com/software/containers/rhel8/nodejs-18-minimal/627d1b055365187064a0c9db?container-tabs=gti"&gt;Red Hat's minimal Node.js v18 Universal Base Image&lt;/a&gt; reduces the final container image size to just 211 MB. All you need to do is change the &lt;code&gt;FROM&lt;/code&gt; statement to use the minimal image:&lt;/p&gt; &lt;pre&gt; FROM registry.access.redhat.com/ubi8/nodejs-18-minimal &lt;/pre&gt; &lt;p&gt;&lt;br /&gt; That 211 MB container image is 75% smaller than the first 831 MB container image you built! Not only is the image smaller, but it also has a lower risk profile since it doesn’t contain tools that could be used for malicious purposes in the event of a security breach.&lt;/p&gt; &lt;h2&gt;Summary and next steps&lt;/h2&gt; &lt;p&gt;Use the following tips to improve your container images for any application runtime:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Use a trusted base image, such as &lt;a href="https://catalog.redhat.com/software/containers/search?q=ubi%20nodejs&amp;amp;p=1"&gt;Red Hat’s Universal Base Image&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Don’t run as root. Using a Red Hat Universal Base Image takes care of this by default.&lt;/li&gt; &lt;li&gt;Use multi-stage builds to minimize container image layers.&lt;/li&gt; &lt;li&gt;Choose a minimal base image for the final stage in a multi-stage build.&lt;/li&gt; &lt;li&gt;Use a &lt;code&gt;.dockerignore&lt;/code&gt; file to keep unwanted files being copied into your container images.&lt;/li&gt; &lt;li&gt;Handle signals such as SIGINT and SIGTERM, or use &lt;a href="https://github.com/krallin/tini"&gt;tini&lt;/a&gt; or &lt;a href="https://github.com/Yelp/dumb-init"&gt;dumb-init&lt;/a&gt; to manage your process(es).&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Take a look at &lt;a href="https://developers.redhat.com/articles/2021/11/08/optimize-nodejs-images-ubi-8-nodejs-minimal-image"&gt;this post by Bethany Griggs&lt;/a&gt; when you’re ready to deploy your lean Node.js container images on the &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt;!&lt;/p&gt; The post &lt;a href="/articles/2023/05/17/build-lean-nodejs-container-images-ubi-and-podman" title="Build lean Node.js container images with UBI and Podman"&gt;Build lean Node.js container images with UBI and Podman&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br&gt;&lt;br&gt;</summary><dc:creator>Evan Shortiss</dc:creator><dc:date>2023-05-17T07:00:00Z</dc:date></entry><entry><title type="html">Kogito 1.37.0 released!</title><link rel="alternate" href="https://blog.kie.org/2023/05/kogito-1-37-0-released.html" /><author><name>Cristiano Nicolai</name></author><id>https://blog.kie.org/2023/05/kogito-1-37-0-released.html</id><updated>2023-05-16T10:42:39Z</updated><content type="html">We are glad to announce that the Kogito 1.37.0 release is now available! This goes hand in hand with , release. From a feature point of view, we have included a series of new features and bug fixes, including: * ForEach state actions now can access the result of the previous action within the same iteration by using $WORKFLOW.prevActionResult. This allows action chaining in SWF loops. * When a task is not found, the GET rest API method now returns 404.  * New APIs to read and write a workflow definition from a yaml/json file * Sysout custom action now uses S4LJ rather than System.out. It optionally allows setting up the log level by adding it to the operation string.  * Service Discovery property expansion now supports the simplified format knative:&lt;namespace&gt;/&lt;serviceName&gt; For more details head to the complete . All artifacts are available now: * Kogito runtime artifacts are available on Maven Central. * Kogito examples can be found . * Kogito images are available on . * Kogito operator is available in the in OpenShift and Kubernetes. * Kogito tooling 0.28.0 artifacts are available at the . A detailed changelog for 1.37.0 can be found in . New to Kogito? Check out our website . Click the "Get Started" button. The post appeared first on .</content><dc:creator>Cristiano Nicolai</dc:creator></entry><entry><title>The benefits of Fedora 38 long double transition in ppc64le</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/05/16/benefits-fedora-38-long-double-transition-ppc64le" /><author><name>Tulio Magno Quites Machado Filho</name></author><id>d06118ad-f5ac-4f38-b98b-6c5dbb3514a6</id><updated>2023-05-16T07:00:00Z</updated><published>2023-05-16T07:00:00Z</published><summary type="html">&lt;p&gt;Fedora 38 will have a new feature for ppc64le. Clang has begun using the IEEE 128-bit long double by default instead of the IBM 128-bit long double format. This allows Clang to behave the same way as GCC, which switched to IEEE 128-bit long double on ppc64le on &lt;a href="https://fedoraproject.org/wiki/Releases/36/ChangeSet#New_128-bit_IEEE_long_double_ABI_for_IBM_64-bit_POWER_LE"&gt;Fedora 36&lt;/a&gt;. This floating point format benefits from the hardware implementation available on IBM® Power9® processor-based servers and IBM® Power10™ processor-based servers.&lt;/p&gt; &lt;h2 id="background"&gt;Background of the floating point format&lt;/h2&gt; &lt;p&gt;The &lt;a href="https://en.wikipedia.org/wiki/IBM_System/360_Model_85"&gt;IBM System/360 Model 85&lt;/a&gt; was released in 1968 and supported a 128-bit extended precision floating point format. A few decades later this format became known in the open source communities as IBM 128-bit long double or IBM double-double.&lt;/p&gt; &lt;p&gt;This floating point format provides a mantissa of 106 bits, 11 bits for the exponent and a signal bit. Meanwhile, its 64-bit floating point format provides a matissa of 53 bits, 11 bits as the exponent and a signal bit. According to the &lt;a href="https://www.ibm.com/docs/en/aix/7.1?topic=sepl-128-bit-long-double-floating-point-data-type"&gt;IBM® AIX® documentation&lt;/a&gt;, this data type can store numbers with more precision than the 64-bit data type, it does not store numbers of greater magnitude.&lt;/p&gt; &lt;p&gt;In 1985, the IEEE 754 Working Group for binary floating-point arithmetic established the &lt;a href="https://en.wikipedia.org/wiki/IEEE_754"&gt;IEEE Standard 754-1985&lt;/a&gt;, defining two binary floating point formats: a 32-bit (&lt;code&gt;binary32&lt;/code&gt;) and a 64-bit (&lt;code&gt;binary64&lt;/code&gt;). The C language was also in the process of standardization, requiring compilers to support at least three different binary floating point types called &lt;code&gt;float&lt;/code&gt;, &lt;code&gt;double&lt;/code&gt;, and &lt;code&gt;long double&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;The ppc64le architecture on Linux adopted the &lt;code&gt;binary32&lt;/code&gt; format as &lt;code&gt;float&lt;/code&gt;, &lt;code&gt;binary64&lt;/code&gt; as &lt;code&gt;double&lt;/code&gt; and &lt;code&gt;ibm128&lt;/code&gt; as &lt;code&gt;long double&lt;/code&gt;, inheriting the same formats for the newer little endian architecture used on the older big endian ppc64.&lt;/p&gt; &lt;p&gt;In 2008, the IEEE Computer Society published the IEEE Std 754-2008, introducing an 128-bit binary floating point format (&lt;code&gt;binary128&lt;/code&gt;).&lt;/p&gt; &lt;table&gt;&lt;thead&gt;&lt;tr class="header"&gt;&lt;th&gt;Format&lt;/th&gt; &lt;th&gt;Signal bits&lt;/th&gt; &lt;th&gt;Exponent bits&lt;/th&gt; &lt;th&gt;Mantissa bits&lt;/th&gt; &lt;th&gt;Size (Bytes)&lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr class="odd"&gt;&lt;td&gt;&lt;code&gt;binary32&lt;/code&gt;&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;8&lt;/td&gt; &lt;td&gt;24&lt;/td&gt; &lt;td&gt;4&lt;/td&gt; &lt;/tr&gt;&lt;tr class="even"&gt;&lt;td&gt;&lt;code&gt;binary64&lt;/code&gt;&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;11&lt;/td&gt; &lt;td&gt;53&lt;/td&gt; &lt;td&gt;8&lt;/td&gt; &lt;/tr&gt;&lt;tr class="odd"&gt;&lt;td&gt;&lt;code&gt;binary128&lt;/code&gt;&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;15&lt;/td&gt; &lt;td&gt;113&lt;/td&gt; &lt;td&gt;16&lt;/td&gt; &lt;/tr&gt;&lt;tr class="even"&gt;&lt;td&gt;&lt;code&gt;ibm128&lt;/code&gt;&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;11&lt;/td&gt; &lt;td&gt;106&lt;/td&gt; &lt;td&gt;16&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;In 2017, IBM introduced the Power9 processor with native hardware support for the &lt;code&gt;binary128&lt;/code&gt; format, leading the way to changing the default &lt;code&gt;long double&lt;/code&gt; type used in C and C++. However, this transition occurred only on ppc64le because it requires an IBM® Power8® processor or newer. The same transition on ppc64 is more complex because it may run on processors that do not support 128-bit registers from VSX or Altivec, requiring an emulation to happen on 64-bit general purpose registers as well as different rules for argument passing.&lt;/p&gt; &lt;h2&gt;How to transition to IEEE 128-bit long double&lt;/h2&gt; &lt;p&gt;In most cases, programs and libraries will not require any modifications. They must be rebuilt with the Clang provided with Fedora 38 to start using the IEEE 128-bit long double. While &lt;a href="https://www.nextplatform.com/2016/08/24/big-blue-aims-sky-power9/"&gt;IBM Power9&lt;/a&gt; introduced native hardware support for &lt;code&gt;binary128&lt;/code&gt;, a &lt;code&gt;long double&lt;/code&gt; based on this format also works on IBM Power8. The only difference is the performance improvement that newer processors provide.&lt;/p&gt; &lt;p&gt;Note that programs built with a previous version of Clang will continue to work using the IBM 128-bit long double.&lt;/p&gt; &lt;h3&gt;Adapting code to IEEE 128-bit long double&lt;/h3&gt; &lt;p&gt;There is a small group of programs that make assumptions about which &lt;code&gt;long double&lt;/code&gt; format ppc64le uses. In those cases, these programs have to be modified.&lt;/p&gt; &lt;p&gt;When rewriting this code, I suggest taking advantage of the features provided by the ISO C standard to write code that will be executed correctly on different processors and operating systems, regardless of the &lt;code&gt;long double&lt;/code&gt; format used by the C Library (e.g., using the &lt;a href="https://www.gnu.org/software/libc/manual/html_node/Floating-Point-Parameters.html#index-LDBL_005fMANT_005fDIG"&gt;macro LDBL_MANT_DIG&lt;/a&gt;) as follows:&lt;/p&gt; &lt;div class="sourceCode" id="cb1"&gt; &lt;pre class="c sourceCode"&gt; &lt;code class="sourceCode c"&gt;&lt;span class="pp"&gt;#include &lt;span class="im"&gt;&amp;lt;float.h&amp;gt; &lt;span class="pp"&gt;#if LDBL_MANT_DIG == 113 &lt;span class="co"&gt;/* Insert code for IEEE binary128 long double. */ &lt;span class="pp"&gt;#elif LDBL_MANT_DIG == 106 &lt;span class="co"&gt;/* Insert code for IBM 128-bit long double. */ &lt;span class="pp"&gt;#elif LDBL_MANT_DIG == 64 &lt;span class="co"&gt;/* Insert code for Intel 80-bit long double. */ &lt;span class="pp"&gt;#elif LDBL_MANT_DIG == 53 &lt;span class="co"&gt;/* Insert code for IEEE binary64 long double. */ &lt;span class="pp"&gt;#endif&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;h2&gt;You can still use IBM 128-bit long double&lt;/h2&gt; &lt;p&gt;It is also possible to continue using the IBM 128-bit long double with Clang on Fedora 38. When building the source code, ensure the parameters &lt;code&gt;-mabi=ibmlongdouble -mlong-double-128&lt;/code&gt; are passed to Clang as follows:&lt;/p&gt; &lt;div class="sourceCode" id="cb2"&gt; &lt;pre class="sh sourceCode"&gt; &lt;code class="sourceCode bash"&gt;$ clang -c -mabi=ibmlongdouble &lt;span class="at"&gt;-mlong-double-128 test.c -o test.o&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;p&gt;The same parameters also work on GCC.&lt;/p&gt; &lt;p&gt;C++ programs built with Clang must also get linked to libstdc++ because other libraries, such as libc++, do not support both &lt;code&gt;long double&lt;/code&gt; formats. The Fedora builds of Clang use libstdc++ by default, but if you would like to enforce the usage of libstdc++, use &lt;code&gt;-stdlib=libstdc++&lt;/code&gt; when calling &lt;code&gt;clang++&lt;/code&gt;, as follows:&lt;/p&gt; &lt;div class="sourceCode" id="cb3"&gt; &lt;pre class="sh sourceCode"&gt; &lt;code class="sourceCode bash"&gt;$ clang++ &lt;span class="at"&gt;-c -mabi=ibmlongdouble &lt;span class="at"&gt;-mlong-double-128 &lt;span class="at"&gt;-stdlib&lt;span class="op"&gt;=libstdc++ test.cc &lt;span class="at"&gt;-o test.o&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;h2&gt;Benefits of transitioning to IEEE 128-bit long double&lt;/h2&gt; &lt;p&gt;The transition to IEEE 128-bit long double on ppc64le will allow programs to compute numbers with greater magnitude and more precision without causing any performance regressions on IBM Power9 and newer processors. This transition is expected to help scientific and engineering programs as well as improve platform compatibility with well-established standards.&lt;/p&gt; &lt;p&gt;Feel free to comment below if you have questions or comments. We welcome your feedback.&lt;/p&gt; The post &lt;a href="/articles/2023/05/16/benefits-fedora-38-long-double-transition-ppc64le" title="The benefits of Fedora 38 long double transition in ppc64le"&gt;The benefits of Fedora 38 long double transition in ppc64le&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br&gt;&lt;br&gt;</summary><dc:creator>Tulio Magno Quites Machado Filho</dc:creator><dc:date>2023-05-16T07:00:00Z</dc:date></entry><entry><title type="html">Q2 2023 RESTEasy Quarterly Releases</title><link rel="alternate" href="https://resteasy.dev/2023/05/16/resteasy-releases/" /><author><name /></author><id>https://resteasy.dev/2023/05/16/resteasy-releases/</id><updated>2023-05-16T04:11:11Z</updated><dc:creator /></entry><entry><title type="html">JQ EXPRESSION AUTO COMPLETIONS</title><link rel="alternate" href="https://blog.kie.org/2023/05/serverless-workflow-editor-new-feature-jq-expression-auto-completions.html" /><author><name>Ajay Jaganathan</name></author><id>https://blog.kie.org/2023/05/serverless-workflow-editor-new-feature-jq-expression-auto-completions.html</id><updated>2023-05-15T16:23:06Z</updated><content type="html">In Serverless Workflow each instance is associated with a data model. These models consist of JSON objects. The data inside these models needs to be accessed and updated throughout the flow execution. jq is a powerful filtering tool that makes working with JSON data seamless. The Serverless Workflow specification supports the use of in multiple places. WHAT’S NEW? In order to improve the authoring experience for the user while working with these expressions, a new auto-completion capability is added to the editor. These features are available in the VS Code extension as well as the Serverless Logic Web Tools and work with both JSON and YAML files. REQUIREMENTS: * (0.28.0) * (0.28.0) * (1.66.0+) The jq auto-completion feature can be categorized into three: * The built-in functions * The workflow variables * The reusable function expressions THE BUILT-IN FUNCTIONS: These are functions provided by jq out of the box. These functions come in handy to do a lot of operations like max, min, length, etc. The completion provides a list of these functions along with a description of what it does so that the user does not have to read the docs every time. More information on the built-in functions can be found . Built-in function completions THE WORKFLOW VARIABLES: Workflow variable completions The serverless workflow works with a lot of variables, which are provided to the project in the form of a JSON schema, an open API file, or an async API file. A Variable in Serverless Workflow always starts with a .(DOT). The auto-completion feature parses these files (if present), extracts the variables, and provides the completion result to the user. Please note that the auto-completion feature is able to parse files in a remote URL as well as a local file system. In the first part of the video above, there is a JSON schema file present in the path: resources/schema/expression.json. The jq expressions can work in certain places and one of them is in the operation of functions which is of type expression. As you can see, the JSON schema has numbers, x, and y in its properties which are listed in the completion items. Please note that remote URLs are also supported for JSON schema. In the second part of the video, we have a function (Check action on RHODS) and this is an OpenAPI specification. We use a remote URL here and we see in the OpenAPI file some parameters available like noOfRunningPods, avgLoad, etc while trying to use the auto-complete we can see these values appearing in the completion item list. Please note that local files (eg: resources/spec/multiplication.yaml) also works. In the third part of the video we have a function which is of type async API and the corresponding file is present in the resources/spec/resume-event.yaml. When trying to auto-complete, we parse the async api file and extract the parameters out of it and show the completion items. Please note that this also works with remote URLs. THE REUSABLE FUNCTION EXPRESSIONS: A reusable is a function definition inside the functions array, which has a type (field) expression and a jq expression as the value for the operation field. These functions can be reused in the specification using the fn: followed by name of the function. The auto-completion feature also provides a list of these functions present in the functions array. Reusable function expression completion Note: The expressions can be used in various places. To know more about the usage please visit . CONCLUSION: The auto-completion feature assists the user in writing the jq expressions and thus improves the authoring experience. In order to learn more about the jq expression in Serverless Workflow, please visit our blog post on  . Stay tuned to get more updates on the upcoming features for the Serverless Workflow. The post appeared first on .</content><dc:creator>Ajay Jaganathan</dc:creator></entry><entry><title>How to use the new OpenShift quick starts to deploy JBoss EAP</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/05/15/how-use-new-openshift-quick-starts-deploy-jboss-eap" /><author><name>Philip Hayes</name></author><id>b003b810-2061-4968-b29f-1864808c85b3</id><updated>2023-05-15T07:00:00Z</updated><published>2023-05-15T07:00:00Z</published><summary type="html">&lt;p&gt;In the articles, &lt;a href="https://developers.redhat.com/articles/2022/01/12/how-migrate-your-java-applications-red-hat-openshift"&gt;How to migrate your Java applications to Red Hat OpenShift&lt;/a&gt; and &lt;a href="https://developers.redhat.com/articles/2022/10/13/how-deploy-jboss-eap-applications-openshift-pipelines"&gt;How to deploy JBoss EAP applications with OpenShift Pipelines&lt;/a&gt;, we covered the technologies utilized to build and deploy &lt;a href="https://www.redhat.com/en/technologies/jboss-middleware/application-platform"&gt;Red Hat JBoss Enterprise Application Platform&lt;/a&gt; images on OpenShift. These technologies include Source to Image (S2I), Helm charts, build configs, deployment configs, tekton pipelines, and ingress routes. While these technologies will be familiar to developers experienced with Kubernetes and containerization, a Jakarta EE developer coming from a traditional JBoss EAP background may be new to these tools.&lt;/p&gt; &lt;p&gt;To help bridge the gap between traditional EAP deployments and OpenShift deployments, Red Hat has created a series of OpenShift quick starts focused on the tools and techniques involved with building and deploying JBoss EAP images on OpenShift. These quick starts provide step-by-step instructions for deploying a JBoss EAP application on OpenShift. They will also explain what is happening at each stage and how to validate each stage you complete.&lt;/p&gt; &lt;p&gt;The first quick start released is a simple JBoss EAP "Hello World" application deployment using Helm charts. This quick start includes the following tasks:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Creating a JBoss EAP 7 application with Helm.&lt;/li&gt; &lt;li aria-level="1"&gt;Viewing the Helm release.&lt;/li&gt; &lt;li aria-level="1"&gt;Viewing the associated code.&lt;/li&gt; &lt;li aria-level="1"&gt;Viewing the build status.&lt;/li&gt; &lt;li aria-level="1"&gt;Viewing the pod status.&lt;/li&gt; &lt;li aria-level="1"&gt;Running the JBoss EAP 7 application.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;We will demonstrate quick start in this article.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;OpenShift CLI tool (optional)&lt;/li&gt; &lt;li aria-level="1"&gt;Openshift 4.x cluster with cluster admin permissions&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The quick start will be available with OpenShift 4.14.  In the mean time, you can add it by following the instructions below.&lt;/p&gt; &lt;h2&gt;Install the quick start using OpenShift CLI&lt;/h2&gt; &lt;p&gt;To download the OpenShift CLI, follow these steps:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Click on the &lt;strong&gt;?&lt;/strong&gt; in the top right hand corner of the OpenShift UI.&lt;/li&gt; &lt;li&gt;Select &lt;strong&gt;Command line tools&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Download and install the command line client for your OS.&lt;/li&gt; &lt;li&gt;Log in as cluster admin.&lt;/li&gt; &lt;li&gt;Run the following command:&lt;/li&gt; &lt;/ol&gt;&lt;pre&gt; &lt;code class="language-bash"&gt;oc apply -f https://raw.githubusercontent.com/jboss-eap-up-and-running/openshift-console-quickstarts/main/jboss-eap7-with-helm.yaml&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Install the quick start using the OpenShift UI&lt;/h2&gt; &lt;p&gt;After logging in to OpenShift as cluster administrator, follow these steps:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;To create the quick start via the OpenShift UI, click on the &lt;strong&gt;+&lt;/strong&gt; icon in the top right hand corner of the UI.&lt;/li&gt; &lt;li&gt;Paste the contents of this link: https://raw.githubusercontent.com/jboss-eap-up-and-running/openshift-console-quickstarts/main/jboss-eap7-with-helm.yaml&lt;/li&gt; &lt;li&gt;Click on &lt;strong&gt;Create&lt;/strong&gt; to create the quick start.&lt;/li&gt; &lt;/ol&gt;&lt;h2&gt;Deploy JBoss EAP 7 using quick start&lt;/h2&gt; &lt;p&gt;In the OpenShift developer UI, click &lt;strong&gt;+Add&lt;/strong&gt; and select &lt;strong&gt;View all quick starts&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;In the search field, enter &lt;strong&gt;eap&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;When you see the &lt;strong&gt;Get started with JBoss EAP 7 using a Helm Chart&lt;/strong&gt; quick start listed (Figure 1), select this quick start to begin.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="/sites/default/files/start_0.jpg" data-featherlight="image"&gt;&lt;img loading="lazy" src="/sites/default/files/styles/article_floated/public/start_0.jpg?itok=xkUK-gmG" width="600" height="350" alt="The get started with JBoss EAP 7 using a Helm Chart option is shown in Red Hat OpenShift UI." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: The JBoss EAP quick start page.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;When a panel opens with instructions, click &lt;strong&gt;Start&lt;/strong&gt; to begin (Figure 2).&lt;/p&gt; &lt;p&gt;In this initial panel, you will follow the steps required to locate and install the JBoss EAP 7.4 Helm chart. You don't need to change any default settings presented by the Helm configuration.&lt;/p&gt; &lt;p&gt;Once you click &lt;strong&gt;Install&lt;/strong&gt;, the user interface will switch to the Topology view and show the &lt;strong&gt;Release notes&lt;/strong&gt; from the Helm chart, as shown in Figure 2.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="/sites/default/files/step2_0.jpg" data-featherlight="image"&gt;&lt;img loading="lazy" src="/sites/default/files/styles/article_floated/public/step2_0.jpg?itok=hxw6ys2v" width="600" height="348" alt="A panel shows instructions to build and deploy a JBoss EAP application on OpenShift using Helm charts." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 2: The steps to build and deploy a JBoss EAP application on OpenShift using Helm charts.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Click &lt;strong&gt;Next&lt;/strong&gt; to open the &lt;strong&gt;Check your work&lt;/strong&gt; panel, as shown in Figure 3. This panel contains questions about the results. It gives you the opportunity to review your work and confirm that you successfully completed the task before advancing to the next task in the quick start.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="/sites/default/files/stage1-check.jpg" data-featherlight="image"&gt;&lt;img loading="lazy" src="/sites/default/files/styles/article_floated/public/stage1-check.jpg?itok=YI5utL8U" width="384" height="319" alt="The quick start &amp;quot;Check your work&amp;quot; panel." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 3: The quick start "Check your work" panel, confirming Helm deployed.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;This is where the quick start helps you understand what was created by the Helm chart. You will be able to identify the Helm release and the eap74 deployment. Click &lt;strong&gt;Yes&lt;/strong&gt; and then click &lt;strong&gt;Next&lt;/strong&gt; to move onto the next section.&lt;/p&gt; &lt;p&gt;This section will guide you through locating the Helm release we just installed. You will be able to view the list of resources (Figure 4).&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="/sites/default/files/step3_0.jpg" data-featherlight="image"&gt;&lt;img loading="lazy" src="/sites/default/files/styles/article_floated/public/step3_0.jpg?itok=1NUVTnI_" width="600" height="349" alt="A list of Helm chart resources." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 4: View a list of Helm chart resources.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;It will prompt you to verify the results again. Validate that you can see the deployed label next to the Helm release and click &lt;strong&gt;Next&lt;/strong&gt; to move to the next section.&lt;/p&gt; &lt;p&gt;You will now be guided through viewing the source code associated with the quick start. There is another verification to confirm that you have successfully completed this step.&lt;/p&gt; &lt;p&gt;Move on to the next section, which describes how the Helm release created two builds. The &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_jboss_enterprise_application_platform/7.4/html/getting_started_with_jboss_eap_for_openshift_container_platform/build_run_java_app_s2i#chained-build-support-eap_default"&gt;build process for JBoss EAP applications utilizes two builds&lt;/a&gt;, an artifact build and a runtime build. The artifact build performs a maven build to create the application artifact. The runtime build deploys the output of this build to an instance of EAP. This results in a much smaller footprint for the runtime image containing the minimum file necessary to run the application.&lt;/p&gt; &lt;p&gt;Once you have completed this check, click &lt;strong&gt;Yes&lt;/strong&gt; to move on to the next check where you will be directed back to view the topology and the running pod status. Follow the instructions to check the status of the pod and complete the check.&lt;/p&gt; &lt;p&gt;Finally, you will be shown how to open the application by clicking on the external URL. Once you have completed this stage, you can complete the final check.&lt;/p&gt; &lt;p&gt;Viewing the external URL will open up a new browser window shown in Figure 5:&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="/sites/default/files/final-page_0.jpg" data-featherlight="image"&gt;&lt;img loading="lazy" src="/sites/default/files/styles/article_floated/public/final-page_0.jpg?itok=ltVqGRhZ" width="600" height="351" alt="The screen shows successful deployment of JBoss EAP application deployment on OpenShift." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 5: This page shows successful deployment of JBoss EAP application deployment on OpenShift.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;You have now completed the quick start. You should see the completed stages of the quick start listed, as shown in Figure 6:&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="/sites/default/files/final-ticks_0.jpg" data-featherlight="image"&gt;&lt;img loading="lazy" src="/sites/default/files/styles/article_floated/public/final-ticks_0.jpg?itok=zyjkUmcC" width="419" height="466" alt="The completed stages of the quick start are listed." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 6: Quick start final checks listed.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;The new quick starts simplify JBoss EAP deployment&lt;/h2&gt; &lt;p&gt;In this article, we demonstrated the new JBoss EAP quick start, designed to guide developers familiar with traditional &lt;a href="https://www.redhat.com/en/technologies/jboss-middleware/application-platform"&gt;JBoss EAP&lt;/a&gt; deployments through the steps to build and deploy application images on OpenShift. The quick start helps developers understand how to use Helm to create the build configs, deployment configs, and external routes required to build and deploy JBoss EAP applications on OpenShift. This quick start uses a sample Git repo with a sample EAP application. Developers will be able to use the same approach to use their own sample applications.&lt;/p&gt; The post &lt;a href="/articles/2023/05/15/how-use-new-openshift-quick-starts-deploy-jboss-eap" title="How to use the new OpenShift quick starts to deploy JBoss EAP"&gt;How to use the new OpenShift quick starts to deploy JBoss EAP&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br&gt;&lt;br&gt;</summary><dc:creator>Philip Hayes</dc:creator><dc:date>2023-05-15T07:00:00Z</dc:date></entry><entry><title type="html">Using Pact and Quarkus to Tame Microservices Testing</title><link rel="alternate" href="https://quarkus.io/blog/pact-and-quarkus-3/" /><author><name>Holly Cummins</name></author><id>https://quarkus.io/blog/pact-and-quarkus-3/</id><updated>2023-05-15T00:00:00Z</updated><content type="html">In a microservices architecture, making sure each microservices works is (relatively) easy. The microservices are usually small, and easy to test. But how do you make sure the microservices work together? How do you know if the system as a whole works? One answer is contract testing. Contract testing gives...</content><dc:creator>Holly Cummins</dc:creator></entry></feed>
