<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title type="html">Getting started with Quarkus 3</title><link rel="alternate" href="http://www.mastertheboss.com/soa-cloud/quarkus/getting-started-with-quarkus-3/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/soa-cloud/quarkus/getting-started-with-quarkus-3/</id><updated>2023-01-19T12:47:00Z</updated><content type="html">This article introduces some of the new features of the upcoming Quarkus 3 release which is, at the time of writing, in Alpha state. We will cover the main highlights and some tools you can use to upgrade existing Quarkus applications. Quarkus 3 highlights Firstly, let’s discuss Quarkus 3 main highlights: An example Quarkus 3 ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">How do I know the WSDL URL of my Web Service ?</title><link rel="alternate" href="http://www.mastertheboss.com/java-ee/jboss-web-services/how-do-i-know-the-wsdl-url-of-my-web-service/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/java-ee/jboss-web-services/how-do-i-know-the-wsdl-url-of-my-web-service/</id><updated>2023-01-19T08:58:55Z</updated><content type="html">In this tutorial we will learn how to find the SOAP Web Service WSDL URL, so that you can quickly test your SOAP Web Service. SOAP Web Service overview Firstly, let’s deploy a sample SOAP Web Service on JBoss EAP or WildFly. If you inspect the server.log file, you will see that the Logger org.jboss.ws.cxf.metadata ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">How to solve java.lang.OutOfMemoryError: unable to create new native thread</title><link rel="alternate" href="http://www.mastertheboss.com/jbossas/monitoring/how-to-solve-javalangoutofmemoryerror-unable-to-create-new-native-thread/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/jbossas/monitoring/how-to-solve-javalangoutofmemoryerror-unable-to-create-new-native-thread/</id><updated>2023-01-19T07:38:27Z</updated><content type="html">In Java you can stumble upon two kinds of Out of Memory errors: The java.lang.OutOfMemoryError Java heap space error : the application tried to allocate more data into the heap space area, but there is not enough room for it.  Although there might be plenty of memory available on your machine, you have hit the ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">How to solve “Too many Open Files” in Java applications</title><link rel="alternate" href="http://www.mastertheboss.com/java/hot-to-solve-the-too-many-open-files-error-in-java-applications/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/java/hot-to-solve-the-too-many-open-files-error-in-java-applications/</id><updated>2023-01-19T07:32:19Z</updated><content type="html">Scenario: Your application log displays the error message “Too many Open Files“. As a result, application requests are failing and you need to restart the application. In some cases, you also need to reboot your machine. Facts: The JVM console (or log file) contains the following error: java.io.FileNotFoundException: filename (Too many open files) at java.io.FileInputStream.open(Native ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>How to distribute workloads using Open Cluster Management</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/01/19/how-distribute-workloads-using-open-cluster-management" /><author><name>Tomer Figenblat</name></author><id>a3ec05c5-fd10-43fd-a4d2-c6295f365931</id><updated>2023-01-19T07:00:00Z</updated><published>2023-01-19T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://open-cluster-management.io/"&gt;Open Cluster Management&lt;/a&gt; (OCM) was accepted to the &lt;a href="https://www.cncf.io/projects/open-cluster-management/"&gt;Cloud Native Computing Foundation&lt;/a&gt; (CNCF) in late 2021 and is currently at the Sandbox project maturity level. OCM is a community-driven project focused on multicluster and multicloud scenarios for &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; applications. This article shows how to bootstrap Open Cluster Management and handle work distribution using &lt;a href="https://open-cluster-management.io/concepts/manifestwork/"&gt;ManifestWork&lt;/a&gt;. We also discuss several ways to select clusters for various tasks using the &lt;a href="https://open-cluster-management.io/concepts/managedclusterset/"&gt;ManagedClusterSet&lt;/a&gt; and &lt;a href="https://open-cluster-management.io/concepts/placement/"&gt;Placement&lt;/a&gt; resources.&lt;/p&gt; &lt;h2&gt;Basic Open Cluster Management&lt;/h2&gt; &lt;p&gt;Open Cluster Management is based on a &lt;a href="https://open-cluster-management.io/concepts/architecture/#hub-spoke-architecture"&gt;hub-spoke&lt;/a&gt; architecture. In this design, a single hub cluster prescribes prescriptions, and one or more spoke clusters act upon these prescriptions. In Open Cluster Management, spoke clusters are called managed clusters. The component running on the hub cluster is the &lt;a href="https://open-cluster-management.io/getting-started/core/cluster-manager/"&gt;cluster manager&lt;/a&gt;. The component running on a managed cluster, is the &lt;a href="https://open-cluster-management.io/getting-started/core/register-cluster/"&gt;klusterlet agent&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;OCM requires one-sided communication. From the managed clusters to the hub cluster. The communication to the hub is handled by two APIs:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Registration: For joining managed clusters to the hub cluster and manage their lifecycle.&lt;/li&gt; &lt;li&gt;Work: For relaying workloads in the form of prescriptions prescribed on the hub cluster and reconciled on the managed clusters.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;OCM resources are easily managed using a command-line interface (CLI) named &lt;a href="https://github.com/open-cluster-management-io/clusteradm"&gt;clusteradm&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;A number of resources help you select the clusters used in your application. We will look at all these resources in this article:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;ManagedClusterSets collect ManagedClusters into groups.&lt;/li&gt; &lt;li&gt;Placements let you select clusters from a ManagedClusterSet.&lt;/li&gt; &lt;li&gt;To select clusters for a placement, you can use labels, &lt;a href="https://open-cluster-management.io/concepts/clusterclaim/"&gt;ClusterClaims&lt;/a&gt;, &lt;a href="https://open-cluster-management.io/concepts/managedcluster/#cluster-taints-and-tolerations"&gt;taints&lt;/a&gt; and &lt;a href="https://open-cluster-management.io/concepts/placement/#taintstolerations"&gt;tolerations&lt;/a&gt;, and &lt;a href="https://open-cluster-management.io/concepts/placement/#prioritizers"&gt;prioritizers&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;As a simple, basic use case for placements, say you have two ManagedClusterSets, one for all the clusters in Israel and one for all the clusters in Canada. You can use placements to cherry-pick from these sets the managed clusters that are suited for testing or production.&lt;/p&gt; &lt;h2&gt;Setting up the example environment&lt;/h2&gt; &lt;p&gt;To get started, you'll need to install &lt;a href="https://github.com/open-cluster-management-io/clusteradm"&gt;clusteradm&lt;/a&gt; and &lt;a href="https://kubernetes.io/docs/tasks/tools/#kubectl"&gt;kubectl&lt;/a&gt; and start up three Kubernetes clusters. To simplify cluster administration, this article starts up three &lt;a href="https://kind.sigs.k8s.io/"&gt;kind&lt;/a&gt; clusters with the following names and purposes:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;&lt;code&gt;kind-fedora1&lt;/code&gt; runs the hub cluster.&lt;/li&gt; &lt;li&gt;&lt;code&gt;kind-rhel1&lt;/code&gt; runs one managed cluster.&lt;/li&gt; &lt;li&gt;&lt;code&gt;kind-qnap1&lt;/code&gt; runs another managed cluster.&lt;/li&gt; &lt;/ol&gt;&lt;h2&gt;Bootstrapping OCM&lt;/h2&gt; &lt;p&gt;The bootstrapping task consists of initializing the hub cluster and joining the managed clusters to it. OCM registration involves a double opt-in handshake initiated by the managed clusters and accepted by the hub cluster. At any point in time, the connection can be ended by either party.&lt;/p&gt; &lt;h3&gt;Initializing the hub cluster&lt;/h3&gt; &lt;p&gt;Run the following command to initialize the hub cluster. Output from the &lt;code&gt;clusteradm&lt;/code&gt; command is filtered by the &lt;code&gt;grep&lt;/code&gt; command and assigned to the &lt;code&gt;joinCommand&lt;/code&gt; variable:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ joinCommand=$(clusteradm init --context kind-fedora1 --wait | grep clusteradm)&lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: This command includes the deployment of the cluster manager, so it might take a couple of minutes.&lt;/p&gt; &lt;p&gt;You can verify that the cluster manager is running by checking for pods in the designated &lt;code&gt;open-cluster-management-hub&lt;/code&gt; and &lt;code&gt;open-cluster-management&lt;/code&gt; namespaces.&lt;/p&gt; &lt;h3&gt;Joining and accepting the managed clusters&lt;/h3&gt; &lt;p&gt;Join each managed cluster to the hub by injecting the cluster name into the stored &lt;code&gt;joinCommand&lt;/code&gt; variable and running the command. This command should be run for every managed cluster. In our example, we have two clusters to manage. As mentioned earlier, registration is a double opt-in handshake, so every join request needs to get accepted by the hub cluster through a &lt;code&gt;clusteradm&lt;/code&gt; command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ eval $(echo "$joinCommand --context kind-rhel1 --insecure-skip-tls-verify --wait" | sed 's/&lt;cluster_name&gt;/kind-rhel1/g' -) $ eval $(echo "$joinCommand --context kind-qnap1 --insecure-skip-tls-verify --wait" | sed 's/&lt;cluster_name&gt;/kind-qnap1/g' -) $ clusteradm --context kind-fedora1 accept --clusters kind-rhel1,kind-qnap1 --wait&lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: These commands deploy the klusterlet agents and initialize the registration, so they might take a couple of minutes.&lt;/p&gt; &lt;p&gt;You can verify that the klusterlet agent is running by checking for pods in the designated &lt;code&gt;open-cluster-management-agent&lt;/code&gt; and &lt;code&gt;open-cluster-management&lt;/code&gt; namespaces for every managed cluster.&lt;/p&gt; &lt;p&gt;For every managed cluster initiating a join request, the registration api creates a cluster-scoped &lt;a href="https://open-cluster-management.io/concepts/managedcluster/"&gt;ManagedCluster&lt;/a&gt; resource on the hub cluster with the specification and status for the associated managed cluster.&lt;/p&gt; &lt;p&gt;Excerpts from the resource for one of the clusters in our example, &lt;code&gt;kind-rhel1&lt;/code&gt;, follow. Note the status object, which holds valuable information about the cluster:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;apiVersion: cluster.open-cluster-management.io/v1 kind: ManagedCluster metadata: name: kind-rhel1 ... spec: hubAcceptsClient: true leaseDurationSeconds: 60 ... status: allocatable: cpu: "4" ephemeral-storage: 71645Mi hugepages-1Gi: "0" hugepages-2Mi: "0" memory: 8012724Ki pods: "110" capacity: cpu: "4" ephemeral-storage: 71645Mi hugepages-1Gi: "0" hugepages-2Mi: "0" memory: 8012724Ki pods: "110" conditions: - lastTransitionTime: "2022-12-21T10:23:47Z" message: Accepted by hub cluster admin reason: HubClusterAdminAccepted status: "True" type: HubAcceptedManagedCluster - lastTransitionTime: "2022-12-21T10:23:47Z" message: Managed cluster joined reason: ManagedClusterJoined status: "True" type: ManagedClusterJoined - lastTransitionTime: "2022-12-21T10:23:47Z" message: Managed cluster is available reason: ManagedClusterAvailable status: "True" type: ManagedClusterConditionAvailable version: kubernetes: v1.25.3 &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Workload distribution across managed clusters&lt;/h2&gt; &lt;p&gt;After a successful registration, the hub cluster creates a designated namespace for every joined managed cluster. These namespaces are called cluster namespaces and form the targets for workload distributions. In our example, we expect two namespaces named &lt;code&gt;kind-qnap1&lt;/code&gt; and &lt;code&gt;kind-rhel1&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;It's important to note that cluster namespaces are used for only workload distributions and certain other activities related to the managed cluster. Anything else, such as placement resources (discussed later in this article) and subscriptions (to be discussed in a future article), go into your application's namespace.&lt;/p&gt; &lt;p&gt;To distribute workloads across managed clusters, apply a &lt;a href="https://open-cluster-management.io/concepts/manifestwork/"&gt;ManifestWork&lt;/a&gt; resource describing your workload in the cluster namespace. The Klusterlet Agents periodically check for ManifestWorks in their designated namespaces, reconcile themselves, and report back with the reconciliation status.&lt;/p&gt; &lt;p&gt;Here's a simple example of a ManifestWork including a namespace and a simple deployment. When applied to a cluster namespace, the associated managed cluster will apply the workload in an orderly fashion:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;apiVersion: work.open-cluster-management.io/v1 kind: ManifestWork metadata: namespace: &lt;target managed cluster&gt; name: hello-work-demo spec: workload: manifests: - apiVersion: v1 kind: Namespace metadata: name: hello-workload - apiVersion: apps/v1 kind: Deployment metadata: name: hello namespace: hello-workload spec: selector: matchLabels: app: hello template: metadata: labels: app: hello spec: containers: - name: hello image: quay.io/asmacdo/busybox command: ["sh", "-c", 'echo "Hello, Kubernetes!" &amp;&amp; sleep 3600']&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can verify that the distribution took place by looking for the &lt;code&gt;hello&lt;/code&gt; deployment in the &lt;code&gt;hello-workload&lt;/code&gt; namespace in each cluster you intended to deploy to.&lt;/p&gt; &lt;p&gt;After deploying this ManifestWork, the klusterlet agent of the associated managed cluster creates the necessary resources and reports back. Excerpts from a status report for the previous ManifestWork follow:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;apiVersion: work.open-cluster-management.io/v1 kind: ManifestWork metadata: ... spec: ... status: conditions: - lastTransitionTime: "2022-12-21T11:15:35Z" message: All resources are available observedGeneration: 1 reason: ResourcesAvailable status: "True" type: Available - lastTransitionTime: "2022-12-21T11:15:35Z" message: Apply manifest work complete observedGeneration: 1 reason: AppliedManifestWorkComplete status: "True" type: Applied resourceStatus: manifests: - conditions: - lastTransitionTime: "2022-12-21T11:15:35Z" message: Apply manifest complete reason: AppliedManifestComplete status: "True" type: Applied - lastTransitionTime: "2022-12-21T11:15:35Z" message: Resource is available reason: ResourceAvailable status: "True" type: Available - lastTransitionTime: "2022-12-21T11:15:35Z" message: "" reason: NoStatusFeedbackSynced status: "True" type: StatusFeedbackSynced resourceMeta: group: "" kind: Namespace name: hello-workload namespace: "" ordinal: 0 resource: namespaces version: v1 statusFeedback: {} - conditions: - lastTransitionTime: "2022-12-21T11:15:35Z" message: Apply manifest complete reason: AppliedManifestComplete status: "True" type: Applied - lastTransitionTime: "2022-12-21T11:15:35Z" message: Resource is available reason: ResourceAvailable status: "True" type: Available - lastTransitionTime: "2022-12-21T11:15:35Z" message: "" reason: NoStatusFeedbackSynced status: "True" type: StatusFeedbackSynced resourceMeta: group: apps kind: Deployment name: hello namespace: hello-workload ordinal: 1 resource: deployments version: v1 statusFeedback: {} &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For every ManifestWork resource identified on the hub cluster, the klusterlet agent creates a cluster-scoped AppliedManifestWork resource on the managed cluster. This resource serves as the owner and the status reporter for the workload. Excerpts from the AppliedManifestWork for the previous ManifestWork follow:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;apiVersion: work.open-cluster-management.io/v1 kind: AppliedManifestWork metadata: ... spec: ... manifestWorkName: hello-work-demo status: appliedResources: - group: "" name: hello-workload namespace: "" resource: namespaces version: v1 .... - group: apps name: hello namespace: hello-workload resource: deployments version: v1 ... &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Grouping managed clusters&lt;/h2&gt; &lt;p&gt;As explained earlier, the hub cluster creates a cluster-scoped &lt;code&gt;ManagedCluster&lt;/code&gt; to represent each managed cluster joined. You can group multiple ManagedClusters using the cluster-scoped &lt;a href="https://open-cluster-management.io/concepts/managedclusterset/"&gt;ManagedClusterSet&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;At the start, there are two pre-existing ManagedClustetSets: The default one, which includes every newly joined ManagedCluster, and the global one that includes all ManagedClusters:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ clusteradm --context kind-fedora1 get clustersets &lt;ManagedClusterSet&gt; └── &lt;default&gt; │ ├── &lt;BoundNamespace&gt; │ ├── &lt;Status&gt; 2 ManagedClusters selected └── &lt;global&gt; └── &lt;Status&gt; 2 ManagedClusters selected └── &lt;BoundNamespace&gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now add your own set:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ clusteradm --context kind-fedora1 create clusterset managed-clusters-region-a&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, configure both of your clusters as members. The following command overwrites the designated label for ManagedClusters custom resources:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ clusteradm --context kind-fedora1 clusterset set managed-clusters-region-a --clusters kind-rhel1,kind-qnap1&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;When you set your clusters as members of your cluster set, they are removed from the pre-existing default set.&lt;/p&gt; &lt;p&gt;As stated earlier, the ManagedClusterSet resource is cluster-scoped. When you write your application using the cluster set, you have to bind it into your application's namespace, which you can do using &lt;code&gt;clusteradm&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;The following command creates a namespace-scoped ManagedClusterSetBinding custom resource in your application namespace:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ clusteradm --context kind-fedora1 clusterset bind managed-clusters-region-a --namespace our-application-ns&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can verify through the following command that your clusters have moved from the &lt;code&gt;default&lt;/code&gt; set to your new set, and that your set is bound to your application namespace:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ clusteradm --context kind-fedora1 get clustersets &lt;ManagedClusterSet&gt; └── &lt;default&gt; │ ├── &lt;BoundNamespace&gt; │ ├── &lt;Status&gt; No ManagedCluster selected └── &lt;global&gt; │ ├── &lt;Status&gt; 2 ManagedClusters selected │ ├── &lt;BoundNamespace&gt; └── &lt;managed-clusters-region-a&gt; └── &lt;BoundNamespace&gt; our-application-ns └── &lt;Status&gt; 2 ManagedClusters selected&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Selecting clusters from the set&lt;/h2&gt; &lt;p&gt;With your ManagedClusterSet bound to your application namespace, you can create a Placement to dynamically select clusters from the set. The following subsections show several ways to select clusters, fetch the selected cluster list, and prioritize managed clusters.&lt;/p&gt; &lt;h3&gt;Using labels to select clusters&lt;/h3&gt; &lt;p&gt;You can select clusters for a placement using labels. The following configuration tells your placement which labels to look for in ManagedClusters within the ManagedClusterSets configured:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;apiVersion: cluster.open-cluster-management.io/v1beta1 kind: Placement metadata: name: our-label-placement namespace: our-application-ns spec: numberOfClusters: 1 clusterSets: - managed-clusters-region-a predicates: - requiredClusterSelector: labelSelector: matchLabels: our-custom-label: "include-me-4-tests"&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Because you haven't yet labeled any of your ManagedClusters yet, your placement will not find any clusters:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kubectl --context kind-fedora1 get placement -n our-application-ns NAME SUCCEEDED REASON SELECTEDCLUSTERS our-label-placement False NoManagedClusterMatched 0&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;So add the appropriate label on one of your ManagedClusters, i.e. &lt;code&gt;kind-rhel1&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kubectl --context kind-fedora1 label managedcluster kind-rhel1 our-custom-label="include-me-4-tests"&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Your placement should now pick this up:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kubectl --context kind-fedora1 get placement -n our-application-ns NAME SUCCEEDED REASON SELECTEDCLUSTERS our-label-placement True AllDecisionsScheduled 1&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Using ClusterClaims to select clusters&lt;/h3&gt; &lt;p&gt;&lt;a href="https://open-cluster-management.io/concepts/clusterclaim/"&gt;ClusterClaims&lt;/a&gt; address two concerns with the use of labels for placement. The first is that, although labels are useful, their overuse makes them error-prone. The second concern is that, in our case, the labels are added for resources on the hub cluster, requiring the cluster administrator or another permitted user with access.&lt;/p&gt; &lt;p&gt;With ClusterClaims, the selection of clusters can be delegated to the managed clusters. ClusterClaims are custom resources applied on the managed cluster. Their content is propagated to the hub as a status for their associated ManagedCluster resources. The cluster administrators for a managed cluster can decide, for instance, which of their clusters are used for tests and which are used for production by simply applying this agreed-upon custom resource.&lt;/p&gt; &lt;p&gt;Clusterclaims can also be used in conjunction with labels to fine-grain your selection.&lt;/p&gt; &lt;p&gt;Apply the following YAML in one of your managed clusters. This example uses &lt;code&gt;kind-qnap1&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;apiVersion: cluster.open-cluster-management.io/v1alpha1 kind: ClusterClaim metadata: name: our-custom-clusterclaim spec: value: include-me-for-tests&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Propagation can be verified on the associated ManagedCluster resource on the hub cluster:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;apiVersion: cluster.open-cluster-management.io/v1 kind: ManagedCluster metadata: name: kind-qnap1 ... spec: hubAcceptsClient: true ... status: allocatable: ... capacity: ... clusterClaims: - name: our-custom-clusterclaim value: include-me-for-tests conditions: ... version: kubernetes: v1.25.3 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now you can create a placement based on your ClusterClaim:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;apiVersion: cluster.open-cluster-management.io/v1beta1 kind: Placement metadata: name: our-clusterclaim-placement namespace: our-application-ns spec: numberOfClusters: 1 clusterSets: - managed-clusters-region-a predicates: - requiredClusterSelector: claimSelector: matchExpressions: - key: our-custom-clusterclaim operator: In values: - include-me-for-tests&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And verify the placement selected the claimed ManagedCluster:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kubectl --context kind-fedora1 get placement -n our-application-ns NAME SUCCEEDED REASON SELECTEDCLUSTERS our-clusterclaim-placement True AllDecisionsScheduled 1 our-label-placement True AllDecisionsScheduled 1&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Using taints and tolerations for the selection&lt;/h3&gt; &lt;p&gt;&lt;a href="https://open-cluster-management.io/concepts/managedcluster/#cluster-taints-and-tolerations"&gt;Taints&lt;/a&gt; and &lt;a href="https://open-cluster-management.io/concepts/placement/#taintstolerations"&gt;tolerations&lt;/a&gt; help you filter out unhealthy or otherwise not-ready clusters.&lt;/p&gt; &lt;p&gt;&lt;a href="https://open-cluster-management.io/concepts/managedcluster/#cluster-taints-and-tolerations"&gt;Taints&lt;/a&gt; are properties of ManagedCluster resources. The following command adds a taint to make your placement deselect your &lt;code&gt;kind-qnap1&lt;/code&gt; cluster. The command also removes existing taints from this ManagedCluster, so be careful when executing such commands:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kubectl --context kind-fedora1 patch managedcluster kind-qnap1 --type='json' -p='[{"op": "add", "path": "/spec/taints", "value": [{"effect": "NoSelect", "key": "our-custom-taint-key", "value": "our-custom-taint-value" }] }]'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To verify that the taint was added, execute:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kubectl --context kind-fedora1 get managedcluster kind-qnap1 -o jsonpath='{.spec.taints[*]'} {"effect":"NoSelect","key":"our-custom-taint-key","timeAdded":"2022-12-22T15:38:47Z","value":"our-custom-taint-value"}&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now verify that the relevant placement has deselected your cluster based on the &lt;code&gt;NoSelect&lt;/code&gt; effect:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kubectl --context kind-fedora1 get placement -n our-application-ns our-clusterclaim-placement NAME SUCCEEDED REASON SELECTEDCLUSTERS our-clusterclaim-placement False NoManagedClusterMatched 0&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;A toleration overrides taints. So your next experiment is to make your placement ignore the previous taint using a toleration. Again, be careful when executing commands like the following because it removes any existing tolerations from the placement:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kubectl --context kind-fedora1 patch placement -n our-application-ns our-clusterclaim-placement --type='json' -p='[{"op": "add", "path": "/spec/tolerations", "value": [{"key": "our-custom-taint-key", "value": "our-custom-taint-value", "operator": Equal }] }]'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Verify that the toleration was added:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kubectl --context kind-fedora1 get placement -n our-application-ns our-clusterclaim-placement -o jsonpath='{.spec.tolerations[*]'} "key":"our-custom-taint-key","operator":"Equal","value":"our-custom-taint-value"}&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And verify that the relevant placement has reselected your cluster:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kubectl --context kind-fedora1 get placement -n our-application-ns our-clusterclaim-placement NAME SUCCEEDED REASON SELECTEDCLUSTERS our-clusterclaim-placement True AllDecisionsScheduled 1&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Two taints are automatically created by the system:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;cluster.open-cluster-management.io/unavailable&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;cluster.open-cluster-management.io/unreachable&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;You can't manually modify these taints, but you can add tolerations to override them. You can even issue temporary tolerations that last a specified number of TolerationSeconds, as described in the placement documentation.&lt;/p&gt; &lt;h3&gt;Fetching the selected cluster list&lt;/h3&gt; &lt;p&gt;As long as a placement has selected at least one cluster, the system creates PlacementDecision resources listing the selected clusters:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kubectl --context kind-fedora1 get placementdecisions -n our-application-ns NAME AGE our-clusterclaim-placement-decision-1 10m our-label-placement-decision-1 22m &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;A PlacementDecision has the same namespace and name as its placement counterpart.&lt;/p&gt; &lt;p&gt;Your PlacementDecision for the placement selected by label should display your labeled ManagedCluster:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;apiVersion: cluster.open-cluster-management.io/v1beta1 kind: PlacementDecision metadata: name: our-label-placement-decision-1 namespace: our-application-ns ownerReferences: - apiVersion: cluster.open-cluster-management.io/v1beta1 kind: Placement name: our-label-placement ... ... status: decisions: - clusterName: kind-rhel1 reason: "" &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Similarly, your PlacementDecision for the placement selected by ClusterClaim should display your claimed ManagedCluster:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;apiVersion: cluster.open-cluster-management.io/v1beta1 kind: PlacementDecision metadata: name: our-clusterclaim-placement-decision-1 namespace: our-application-ns ownerReferences: - apiVersion: cluster.open-cluster-management.io/v1beta1 kind: Placement name: our-clusterclaim-placement ... ... status: decisions: - clusterName: kind-qnap1 reason: "" &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Prioritizing clusters for selection&lt;/h3&gt; &lt;p&gt;&lt;a href="https://open-cluster-management.io/concepts/placement/#prioritizers"&gt;Prioritizers&lt;/a&gt; tell placements to prefer some clusters based on built-in ScoreCoordinates. You can also extend prioritization by using an AddOnPlacementScore.&lt;/p&gt; &lt;p&gt;The following settings configure your placement to use prioritization and sort your clusters based on the allocated memory and CPU capacity (as viewed in the cluster status). Each coordinate is assigned a different weight for the selection:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;apiVersion: cluster.open-cluster-management.io/v1beta1 kind: Placement metadata: name: our-clusterclaim-placement namespace: our-application-ns spec: numberOfClusters: 2 clusterSets: ... prioritizerPolicy: mode: Exact configurations: - scoreCoordinate: builtIn: ResourceAllocatableMemory weight: 2 - scoreCoordinate: builtIn: ResourceAllocatableCPU weight: 3 predicates: - requiredClusterSelector: ...&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To extend the built-in score coordinates, use a designated AddOnPlacementScore in your application namespace:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;apiVersion: cluster.open-cluster-management.io/v1alpha1 kind: AddOnPlacementScore metadata: name: our-addon-placement-score namespace: our-application-ns status: conditions: - lastTransitionTime: "2021-10-28T08:31:39Z" message: AddOnPlacementScore updated successfully reason: AddOnPlacementScoreUpdated status: "True" type: AddOnPlacementScoreUpdated validUntil: "2021-10-29T18:31:39Z" scores: - name: "our-custom-score-a" value: 66 - name: "our-custom-score-b" value: 55&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now, you can modify your placement and add score coordinates referencing your custom addon scores:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;apiVersion: cluster.open-cluster-management.io/v1beta1 kind: Placement metadata: name: our-clusterclaim-placement namespace: our-application-ns spec: numberOfClusters: 2 clusterSets: ... prioritizerPolicy: mode: Exact configurations: - scoreCoordinate: builtIn: ResourceAllocatableMemory weight: 2 - scoreCoordinate: builtIn: ResourceAllocatableCPU weight: 3 - scoreCoordinate: builtIn: AddOn addOn: resourceName: our-addon-placement-score scoreName: our-custom-score-a weight: 1 - scoreCoordinate: builtIn: AddOn addOn: resourceName: our-addon-placement-score scoreName: our-custom-score-b weight: 4 predicates: - requiredClusterSelector: ...&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Summarizing OCM basics&lt;/h2&gt; &lt;p&gt;This article introduced Open Cluster Management and explained how to:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Bootstrap OCM on hub and spoke (managed) clusters.&lt;/li&gt; &lt;li&gt;Deploy a workload across managed clusters.&lt;/li&gt; &lt;li&gt;Group managed clusters into sets.&lt;/li&gt; &lt;li&gt;Select clusters from cluster sets.&lt;/li&gt; &lt;li&gt;Customize placement scheduling.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Upcoming articles will cover various features, addons, frameworks, and integrations related to OCM. These components make use of the infrastructure concerning work distributions and placements described in this article. In the meantime, you can learn about &lt;a href="https://developers.redhat.com/articles/2023/01/16/how-prevent-computer-overload-remote-kind-clusters"&gt;how to prevent computer overload with remote kind clusters&lt;/a&gt;. I want to thank you for taking the time to read this article, and I hope you got something out of it. Feel free to comment below if you have questions. We welcome your feedback.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/01/19/how-distribute-workloads-using-open-cluster-management" title="How to distribute workloads using Open Cluster Management"&gt;How to distribute workloads using Open Cluster Management&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Tomer Figenblat</dc:creator><dc:date>2023-01-19T07:00:00Z</dc:date></entry><entry><title type="html">How to Start, Stop and Restart WildFly</title><link rel="alternate" href="http://www.mastertheboss.com/jbossas/jboss-configuration/how-to-start-stop-and-restart-wildfly/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/jbossas/jboss-configuration/how-to-start-stop-and-restart-wildfly/</id><updated>2023-01-18T17:35:10Z</updated><content type="html">This guide contains some tips to teach you how to start, stop, restart WildFly application server. By the end of this tutorial, you will be able to effectively manage the lifecycle of your Wildfly server as needed. First of all, WildFly can be run in two modes: Standalone mode and Domain mode. Booting WildFly in ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">How to run Keycloak with Docker</title><link rel="alternate" href="http://www.mastertheboss.com/keycloak/keycloak-with-docker/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/keycloak/keycloak-with-docker/</id><updated>2023-01-18T15:27:02Z</updated><content type="html">Keycloak is an Open Source Identity and Access Management solution for modern Applications and Services. Docker Images for Keycloak are available on the quay.io Docker repository. In this tutorial we will learn how to run it with Docker, using some common environment parameters. Available Keycloak distributions The Keycloak Docker Image for is available in this ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">How to configure SSL/HTTPS on WildFly</title><link rel="alternate" href="http://www.mastertheboss.com/jbossas/jboss-security/complete-tutorial-for-configuring-ssl-https-on-wildfly/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/jbossas/jboss-security/complete-tutorial-for-configuring-ssl-https-on-wildfly/</id><updated>2023-01-18T10:51:49Z</updated><content type="html">This is a complete tutorial about configuring SSL/HTTPS support for JBoss EAP / WildFly application server. Generally speaking, to configure SSL/HTTPS you can either use the pure JSSE implementation (and the keytool utility) or a native implementation such as OpenSSL. We will cover at first the JSSE implementation with keytool. Later we will show how ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">How to access WildFly Admin Console</title><link rel="alternate" href="http://www.mastertheboss.com/jbossas/jboss-configuration/how-to-access-wildfly-admin-console/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/jbossas/jboss-configuration/how-to-access-wildfly-admin-console/</id><updated>2023-01-18T10:01:44Z</updated><content type="html">This short tutorial will teach you how to access WildFly Admin Console also known as WildFly Management Console. WildFly Admin Console Default URL The default URL for WildFly Management Console is http://localhost:9990. Upon installation, you cannot connect to WildFly Management Console because there is no default Admin user for the Management Console. To add a ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">Solving java.lang.OutOfMemoryError: Metaspace error</title><link rel="alternate" href="http://www.mastertheboss.com/java/solving-java-lang-outofmemoryerror-metaspace-error/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/java/solving-java-lang-outofmemoryerror-metaspace-error/</id><updated>2023-01-18T08:22:55Z</updated><content type="html">The java.lang.OutOfMemoryError: Metaspace indicates that the amount of native memory allocated for Java class metadata is exausted. Let’s how this issue can be solved in standalone applications and cloud applications. In general terms, the more classes you are loading into the JVM, the more memory will be consumed by the metaspace. In Java 8 and ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry></feed>
